[["index.html", "R für empirische Wissenschaften Vorwort", " R für empirische Wissenschaften Jan Philipp Nolte 2021-04-12 Vorwort Es handelt sich nicht um die finale Version des Buches. Willkommen zum Buch R für empirische Wissenschaften. Hier wirst Du ohne Vorwissen lernen, wie man die Programmiersprache R auf eine moderne Art und Weise im wissenschaftlichen Kontext anwendet. Die Zielgruppen sind im Besonderen Bachelor-, Master- und PhD-Studenten und Studentinnen, die im Rahmen der entsprechenden Qualifikationsarbeit Daten auswerten müssen. Aber auch für Wissenschaftler mit Ambitionen, sich erstmals mit R vertraut zu machen, bietet dieses Buch eine geeignete Einführung. Dabei wird eine kohärente Philosophie verfolgt: Lerne das Prinzip einmal, um es dann auf verschiedenste Kontexte immer wieder anzuwenden. Nach Verstehen der wenigen zentralen Konzepte wirst Du also schnell merken, wie einfach Dir die erfolgreiche Anwendung auf eigene Daten gelingen wird. Das gesamt Buch wird durch das remp Package ergänzt, welches diverse Datensätze, Übungen und die ein oder andere praktische Funktion beinhaltet. R für empirische Wissenschaften ist unter der Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License veröffentlicht. Informiere Dich über die Bedingungen einer Veränderung und Weiterverbreitung des Materials. "],["intro.html", "Kapitel 1 Einleitung 1.1 Für wen ist dieses Buch? 1.2 Aufbau und Bearbeitungsstrategie 1.3 Boxen, Übungen und Datensätze 1.4 Ergänzende Literatur", " Kapitel 1 Einleitung 1.1 Für wen ist dieses Buch? Das Buch ist grundsätzlich für jeden geeignet, der R lernen möchte. Ein ganz besonderer Fokus wird allerdings auf die Herausforderungen und täglichen Aufgaben von (werdenden) empirischen Wissenschaftlern gelegt. Die Zielgruppen sind Bachelor-, Master- und PhD-StudentInnen sowie WissenschaftlerInnen, die R eine Chance als Analysewerkzeug der Wahl geben möchten. Was du lernen wirst. Nach Lesen des Buches wirst du einen Datensatz einlesen und aufbereiten können. du wirst lernen, deskriptive Maße und die üblichsten statistischen Hypothesentests im Rahmen der frequentistischen Statistik zu berechnen. Auch wirst du diese Ergebnisse als publikationsreife Abbildungen oder Tabellen darstellen und in einem direkt verwendbaren Format exportieren können. Was du nicht lernen wirst. du wirst in diesem Buch nichts über die theoretischen Grundlagen der verwendeten statistischen Tests lernen. Dafür gibt es bereits sehr gute Statistiklehrbücher. Auch liegt der Fokus der Inferenzstatistik ausschließlich auf frequentistischen und nicht auf bayesianischen Methoden. Darüber hinaus wird weder auf Vorhersagemodellierungen im Allgemeinen noch auf Machine Learning oder Deep Learning im Speziellen eingegangen. Auf sehr spezielle Anwendungsfelder wie Geoinformationssysteme oder fMRT-Bilder wird ebenfalls verzichtet. 1.2 Aufbau und Bearbeitungsstrategie Tatsächlich sind die Kapitel in der Reihenfolge aufgebaut, wie man normalerweise mit einem frisch erhobenen rohen Datensatz umgehen würde. Nachdem alles richtig eingerichtet und aufgesetzt ist (Teil I), würde man die Daten bereinigen und aufbereiten (Teil II) bis die Daten dann ausgewertet werden können (Teil III). Vertiefende Konzepte runden den Inhalt des Buches dann ab (Teil IV). Teil I: Die ersten Schritte. Die ersten beiden Kapitel bilden den ersten Teil, der auch von Lesern, die bereits mit R gearbeitet haben gründlich gelesen werden sollte. Vor allem die Installation bereitet häufig schon die ersten großen Probleme. Auch Ressourcen für Hilfestellungen werden besprochen. Teil II: Vorbereitung. Die meiste Zeit der Datenauswertung wird von der Datenvorbereitung beansprucht. Die eigentliche Auswertung geht dann meistens schnell. Daher ist die Datenvorbereitung auch eines der ausführlichsten Kapitel dieses Buches. Außerdem werden das moderne Konzept von R Projekten sowie einige Grundlagen wie Variablen und Datentypen erläutert. Auch wird das Einlesen von Datensätzen verschiedener Dateienarten erklärt. Teil III: Auswertung. Wenn der Datensatz endlich fertig aufbereitet ist, können Abbildungen erstellt sowie deskriptive Statistiken und inferenzstatistische Hypothesentest berechnet werden. Auch wird auf einige häufig verwendete latente Variablenmodelle eingegangen. Die Abbildungen und in Tabellen dargestellten Ergebnisse werden dabei publikationsreif gemacht. Teil IV: Vertiefung. Hier werden weiterführende und vertiefende Konzepte vorgestellt, die nicht zwingend für die Datenauswertung benötigt werden. Es wird erklärt, wie man Tabellen oder ganze Berichte exportieren kann. Die verschiedenen Datenstrukturen werden verglichen und abschließend fortgeschrittener Programmiertechniken vorgestellt. Nun ist es so, dass jeder seinen eigenen Lernstil und Anspruch an ein Lehrbuch hat. Die Kapitel des Buches sind zwar grundsätzlich aufeinander aufbauend, allerdings wurde darauf geachtet, die Kapitel möglichst in sich geschlossen zu halten. Wer also nicht das gesamte Buch Schritt für Schritt durcharbeiten möchte, sollte aber zumindest bestimmte Kapitel gelesen haben (auch wenn man nur an einem ganz bestimmten statistischen Test für die Bachelorarbeit interessiert ist). Zeitmangel? Die Kapitel 2, 3, 4, 5 und 6.1 sind essentiell zum Arbeiten mit R. Außerdem sollten auch immer die vorhandenen Einführungskapitel gelesen werden, da folgend immer wieder auf Inhalte daraus zurückgegriffen wird. 1.3 Boxen, Übungen und Datensätze Es gibt drei Arten von Boxen mit jeweils unterschiedlicher Farbe und Symbol. Die mit der Glühbirne markierten Boxen fassen besonders wichtige Konzepte zusammen, die mit dem Warnzeichnen weisen auf häufige Probleme hin und die mit dem Laptop enthalten praktische Übungen. Diese Übungen werden immer auf dieselbe Art und Weise gestartet. Man muss einfach nur den Namen der Übung innerhalb von hands_on() schreiben. hands_on(&quot;ue_datentypen&quot;) Diese Funktion öffnet deinen Browser, indem du dann die Übungen absolvieren kannst. Dafür wird kein Internet benötigt. Der Vorteil dieser Art von Übungspräsentation ist der, dass Dir direkt nach Eingabe des Codes Feedback gegeben werden kann. Alternativ stehen die Übungsaufgaben und Lösungen auch auf der Internetseite zum Buch bereit (r-empirische-wissenschaften.de). Ein Übungsverzeichnis findet du im Appendix. Alle Übungen erkennst du namentlich am Präfix ue. Im Laufe des Buches werden aus didaktischen Gründen verschiedene Datensätze verwendet, die ebenfalls im Appendix erläutert werden. Zum Anwenden auf dem heimischen Computer kannst du entweder händisch den im Buch beschrieben Code abtippen oder du verwendest den Copy to Clipboard Button, der jedes Mal rechts oben im Codeblock erscheint. Dieser macht genau dasselbe, als würdest du den Befehl markieren und mit strg + c oder Rechtsklick und Kopieren, kopieren. 1.4 Ergänzende Literatur Statistical Rethinking: A Bayesian Course With Examples in R and Stan (2nd Edition). Für die Einführung in bayesianische Methoden gibt es wohl kein besseres Buch. Dabei gibt McElreath (2020) viele verständliche Erklärungen mit Beispielen in R und Stan. ggplot2: Elegant Graphics for Data Analysis (3rd Edition). Das für die Visualisierungen verwendete Package namens ggplot2 ist viel zu umfangreich, um es in einem Kapitel (hier Kapitel 8) in Gänze zu erfassen. Daher kann bei Bedarf auf das großartige Buch von Wickham (2021) zurückgegriffen werden(https://ggplot2-book.org/). R Markdown: The Definite Guide. Dieses Buch von Xie, Allaire und Grolemund (2020) erklärt die in Kapitel 11.2 eingeführten Exportierungsmöglichkeiten mithilfe von R Markdown deutlich ausführlicher. Denn mit R Markdown kann man nicht nur kurze Berichte schreiben, sondern auch Bücher, Internetseiten und Abschlussarbeiten erstellen (https://bookdown.org/yihui/rmarkdown/). Advanced R (2nd Edition). Die bereits zweite Version von Advanced R von Wickham (2019) gibt Dir tiefe Einblicke in R als Programmiersprache als solche. Wenn man sich nach R für empirische Wissenschaften noch tiefer mit R befassen möchte, sollte man dieses Buch nicht missen (https://adv-r.hadley.nz/). "],["start.html", "Kapitel 2 Startvoraussetzungen 2.1 Installation von R und RStudio 2.2 Aufbau von RStudio 2.3 RStudio anpassen 2.4 Funktionen und ihre Argumente 2.5 Packages (Erweiterungen) 2.6 Fehler- und Warnmeldungen 2.7 Historische Relikte", " Kapitel 2 Startvoraussetzungen 2.1 Installation von R und RStudio Die Unterscheidung zwischen R und RStudio ist für viele Anfänger verwirrend. Es handelt sich dabei nicht um zwei austauschbare Alternativen. Stattdessen ist R die Programmiersprache und RStudio eine Programmierumgebung. Den Unterschied können wir uns an einem Motor mit seiner Karosserie verdeutlichen, der in Abbildung 2.1 illustriert ist. Man braucht die Programmiersprache R, damit überhaupt etwas voran geht. Grundsätzlich kann man das Auto auch mit einem ganz spartanischen Stahlgerüst fahren. Abbildung 2.1: Illustration des Unterschieds zwischen R und RStudio. Die Programmierumgebung RStudio macht aus diesem minimalistischen Stahlgerüst eine komfortable Luxuslimousine mit Navigationssystem und Sitzheizung. Man kann R folglich auch ohne RStudio benutzen, aber RStudio nicht ohne R. Sonderlich viel Spaß macht das allerdings nicht. RStudio bietet eine Vielzahl von großartigen und praktischen Features, weswegen wir im Laufe des Buches nur innerhalb von RStudio arbeiten werden. Es wird aber trotzdem häufig von R die Rede sein, da RStudio lediglich die verwendete Programmierumgebung ist. In den folgenden Kapiteln wird Schritt für Schritt erklärt, wie du R und RStudio auf den gängigsten Betriebssystemen installierst und wichtige Anpassungen innerhalb von RStudio vornimmst. du solltest Kapitel 2.3 unbedingt auch lesen, wenn sich RStudio bereits auf deinem Computer oder Laptop befindet. 2.1.1 Programmiersprache R Zum Bearbeiten der Übungen aus dem Buch benötigst du die R Version 3.3.3 oder neuer. Falls beim späteren Installieren der Packages (siehe Kapitel 2.5) ein Fehler auftritt, liegt das aller Wahrscheinlichkeit an einer zu alten R Version. Am besten installierst du R wie hier beschrieben neu. Bei der Installation gibt es Unterschiede zwischen den verschiedenen Betriebssystemen Windows, macOS (Apple) und Ubuntu (Linux). In der späteren Benutzung gibt es hingegen keine Unterschiede. du musst dir also nur den jeweiligen Abschnitt anschauen, der die Installation auf deinem Betriebssystem beschreibt. Nach der Installation musst du mit R nichts weiter machen und kannst sofort zur Installation von RStudio hinübergehen. Windows: Geh auf cloud.r-project.org und wähle Download R for Windows Klicke anschließend auf den Link base. Drücke dann auf Download R for Windows und achte darauf, wohin du die Installationsdatei abspeicherst. Führe die Installationsdatei (z.B. R-4.0.4-win.exe) mit einem Doppelklick aus. Folge schließlich den Installationsanweisungen. Hierbei ist das Entfernen des Häkchen bei Message translations zwingend erforderlich (siehe Abbildung 2.2). Ansonsten muss nichts an den Standardeinstellungen der Installation geändert werden. Nach erfolgreicher Installation kannst du die heruntergeladene Installationsdatei wieder löschen. Abbildung 2.2: Richtige Installation von R ohne Sprachsupport macOS (Apple) : Gehe auf cloud.r-project.org und wähle Download R for (Mac) OS X. Je nachdem wie alt dein Mac ist, müssen unterschiedliche Versionen von R heruntergeladen werden. Deine macOS Version darf zur Benutzung der in diesem Buch vorgestellten Methoden nicht älter als 10.9 Mavericks sein. macOS 10.13 High Sierra und neuer: R-4.0.4.pkg (bzw. die gerade aktuellste Version) macOS 10.11 El Capitan und neuer: R-3.6.3.pkg macOS 10.9 Mavericks und neuer: R-3.3.3.pkg Achte auf den Speicherort der Installationsdatei und führe diese aus. Falls deine Einstellungen die Installation externer Programme verhindern, musst du das Installieren von externen Paketen (.pkg) explizit erlauben (siehe unten). Bei der Installation kann, ohne was zu verändern, stehts auf weiter gedrückt werden. Nach fertiger Installation kann die Installationsdatei (z.B. R-4.0.4.pkg) gelöscht werden. Für englische Fehlermeldungen muss schließlich noch folgender Befehl innerhalb von R ausgeführt werden. Danach muss R geschlossen und erneut geöffnet werden. Alternativ kann man auch zuerst RStudio installieren und den Befehl dort entsprechend eingeben. system(&quot;defaults write org.R-project.R force.LANG en_US.UTF-8&quot;) Je nach Einstellungen des Betriebssystems kann die Installation externer Software (nicht von Apple zertifiziert) wie R aus Sicherheitsgründen blockiert werden. Um das zu umgehen, musste du mit dem Finder (nicht Launchpad) nach der Installationsdatei (z.B. R-4.0.3.pkg) suchen. Halte nun ctrl gedrückt und klicke auf die Datei. Wähle schließlich Öffnen aus dem erscheinenden Kontextmenü aus. Ubuntu (Linux): Drücke die Tastenkombination strg + alt + t, um den Terminal zu öffnen. Öffne die Liste deiner Repositorien als Administrator (sudo) mit einem Editor deiner Wahl (z.B. gedit). sudo gedit /etc/apt/sources.list Kopiere nun das Repository in die soeben geöffnete Datei sources.list. Für Ubuntu 20.04 würde man bionic im Namen verwenden. Für andere Versionen müsste bionic zu beispielsweise focal, xenial oder trusty geändert werden. deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Führe anschließend folgende zwei Zeilen hintereinander im Terminal aus. sudo apt-get update sudo apt-get install r-base r-base-dev Falls dies nicht funktionieren sollte, musst du noch den entsprechenden Public Key hinzufügen. Für den Fall eines neuerdings veränderten Keys kann unter cloud.r-project.org &gt; Download R for Linux &gt; Ubuntu &gt; README der derzeit aktuelle eingesehen werden. sudo apt-key adv --keyserver keyserver.ubuntu.com \\ --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 Zum Einstellen englischer Fehlermeldungen muss LANGUAGE=en an beliebiger Stelle in Renviron.site kopiert und gespeichert werden. Öffne diese dazu wie zuvor mit einem Editor wie gedit. sudo gedit /etc/R/Renviron.site 2.1.2 Programmierumgebung RStudio Nachdem R installiert ist, kannst du die Installationsdatei für RStudio unter rstudio.com/products/rstudio/download/ herunterladen. Drücke dazu auf den Download Button für RStudio Desktop (Open Source License) und lade die für dein Betriebssystem richtige Version herunter. Wie beim Installieren von R musst du die heruntergeladene Installationsdatei mit der für dein Betriebssystem richtigen Endung (.exe, .pkg, .deb) ausführen und den anschließend eingeblendeten Anweisungen Folge leisten. Wenn das Programm installiert ist, kannst du diese Installationsdatei wieder löschen. Das eigentliche Programm RStudio findest du nun auf dem Computer. Zum Beispiel beim Drücken der Windows Taste und Eingabe des Wortes RStudio oder bei macOS über den Finder. Von jetzt an solltest du zum Arbeiten mit R immer RStudio öffnen und nicht R. Die extra vorhandene minimalistische Oberfläche von R kannst du getrost ignorieren. Es ist bloß wichtig, es auf dem Computer installiert zu haben. 2.2 Aufbau von RStudio Wir schauen uns nun die einzelnen Komponenten innerhalb von RStudio an. Die Oberfläche von RStudio ist dabei in vier Bereiche unterteilt: Nach frischer Installation ist die Console links unten, das R Skript als Source links oben, Environment &amp; History rechts oben und Plots, Hilfe, Packages und mehr unten rechts. Diese Reihenfolge kann beliebig nach eigener Präferenz verändert werden (siehe Kapitel 2.3). Abbildung 2.3: Aufbau von R Studio mit Skript (oben links), Konsole (unten links), Environment (oben rechts) und u.a. Plots und Help (unten rechts). Console. In der Konsole befindet sich im Prinzip die reine Programmiersprache R. Wir können also jegliche Befehle direkt in die Konsole eingeben, auf Enter drücken und das Ergebnis erhalten. Im Anschluss ist der Befehl weg. Mit der oberen und unteren Pfeiltaste kannst du die in der bisherigen Sitzung bereits ausgeführten Befehle durchgehen. Da das weder sonderlich praktisch noch reproduzierbar ist, sollte man so genannte Skripte verwenden, die in einer gesonderten Datei (mit der Endung .R) gespeichert werden und nach Öffnen immer wieder ausführbar sind. Source. Beim initialen Starten von R, wird kein Skript angezeigt. Die Konsole nimmt also zuerst die gesamte Linke Seite ein. Zum Erstellen eines R Skriptes kann man entweder die Tastenkombination strg/cmd + Shift + N verwenden oder auf das unterhalb des Reiters File gelegene Blatt Papier mit dem Plus Zeichen klicken und dort R Script auswählen. Gespeichert wird das R Skript ganz klassisch mit Strg + S (oder über das Menü). Die Endung des Skriptes muss dabei .R sein, da die darin enthaltenen Befehle sonst nicht ausgeführt werden können. Man kann die Befehle innerhalb des Skriptes nun mit strg/cmd + Enter ausführen. Die Ergebnisse werden in der Konsole angezeigt. Falls das gesamte Skript ausgeführt werden soll, drücke zuerst strg/cmd + A, um alles zu markieren und führe es dann mit strg/cmd + Enter aus. Man kann mit der Tastenkombination strg/cmd + 1 und strg/cmd + 2 mit der Tastatur zwischen Konsole und Skript wechseln. Environment &amp; History. In der Environment werden alle gespeicherten Variablen (siehe Kapitel 4.1) angezeigt. Man kann dort auch auf die eingelesenen Datensätze klicken und sich diese innerhalb von RStudio in einem eigenen Reiter ansehen. Vorsicht sei hier bei großen Datensätzen geboten, da diese Ansicht relativ rechenintensiv ist und RStudio dadurch abstürzen kann. Es empfiehlt sich daher, große Datensätze in ein anderes Format wie CSV oder Excel umzuwandeln, um dort einen guten Überblick über den Datensatz zu gewinnen (siehe Kapitel 5). Die History zeigt alles an, was in der Konsole innerhalb einer R Sitzung ausgeführt wurde. Wenn du vollständig mit Skripten arbeitest, kannst du die History vollständig ignorieren. Tatsächlich solltest du das automatische Speichern und Laden der History wie in Kapitel 2.3 sogar ausschalten. Schließlich haben wir sämtliche durchgeführten Berechnungen bereits im R Skript gespeichert. Plots, Help &amp; Packages. Plots zeigt die erstellten Visualisierungen an (siehe Kapitel 8). Dabei kann man unter Zoom ein eigenes Fenster mit der Abbildung öffnen. Die Abbildung verändert sich, wenn man die Länge oder Breite des Fensters entsprechend verschiebt. Grundsätzlich kann mit Export die Visualisierung direkt hier gespeichert werden. Darauf sollte allerdings aufgrund der Auflösungsunterschiede verzichtet werden (siehe Kapitel 8.14). Unter Help wird die Dokumentation der verschiedenen R Funktionen angezeigt (siehe Kapitel 2.6). Packages ist grundsätzlich auch kein Reiter, der dich interessieren muss. Theoretisch kannst du hier per Mausklick Packages laden und Installieren. Im Sinne der Reproduzierbarkeit solltest du dies jedoch wie in Kapitel 2.5 beschrieben immer mit R Code machen. 2.3 RStudio anpassen Die ersten zwei in diesem Kapitel erklärten Anpassungen sind essentielle und unabdingbare Voraussetzungen, um das erfolgreiche Ausführen des Codes auch auf anderen Computern zu gewährleisten. Diese Einstellungen nicht vorgenommen zu haben, ist eine häufige und schwer zu findende Fehlerquelle. Die anderen Anpassungen stellen Empfehlungen dar, die dir das Programmieren erleichtern sollen. Niemals den Workspace und die History speichern. Um zu gewährleisten, dass dein Code nicht nur bei dir funktioniert, ist es dringend notwendig das ständige Speichern und Laden des Workspaces auszustellen. Auch gewährleistest du dadurch, dass der Code immer funktioniert, wenn du diesen neu durchlaufen lässt. Gehe zu: Tools/Global Options.../General Entferne den Haken bei: Restore .RData into workspace at startup Ändere Save workspace to .RData on exit zu never Entferne den Haken bei: Always save history Keine Sorge, auf das Speichern deines Codes hat das keine Auswirkung. Standard Zeichenencodierung. Damit man den geschriebenen Code fehlerfrei auf anderen Geräten lesen kann, muss dieselbe Zeichencodierung gewählt werden. Die modernste und am weitesten verbreitetste ist UTF-8. Gehe zu: Tools/Global Options.../Code/Saving Ändere: Default text encoding zu UTF-8 Programmierhilfen. Aller Anfang ist schwer und warum sollte man dann nicht jede zur Verfügung stehenden Hilfen nutzen wollen? Hier wird eingestellt, dass Funktionen vom Rest des Codes farblich hervorgehoben werden. Außerdem wirst du darauf hingewiesen, wenn Lehrzeichen im Sinne einer guten Lesbarkeit falsch gesetzt sind. Zum Schluss stellen wir die Vorschläge zur Vervollständigung von Code noch auf eine kürzere Zeit ein. Gehe zu: Tools/Global Options.../Code Wechsel zu Display und mache einen Haken bei Highlight R function calls Wechsel zu Diagnostics und mache einen Haken bei Provide R style diagnostics Wechsel zu Completion und ändere im Abschnitt Completion Delay die Zahlen auf 1 (Character) und 0 (ms) Schickes Aussehen. Es hat einen Grund, weshalb heutzutage viele Internetseiten und Smartphone Apps mit einem dunklen Farbthema angezeigt werden. Das ganze sieht nicht nur besser aus, sondern ist auch deutlich angenehmer für die Augen. Gehe zu: Tools/Global Options.../Appearance Nun kannst du aus verschiedenen Themen wählen und auch die Schriftart- und größe anpassen. Anordnung der vier Layer. Die Anordnung von Console, Skript und Co ist Geschmackssache. Sinnvoll ist eine mit dem Skript auf der linken oberen Seite und der Console auf der rechten oberen Seite. Die Fenster unten links kannst du dann für immer Minimieren und hast so mehr Platz zum Arbeiten im Skript. Gehe zu: Tools/Global Options.../Pane Layout Oben links: Source Oben rechts: Console Unten links: History, Connections Unten rechts: Environment, Files, Plots, Packages, Help, Build, VCS, Viewer Komfortables Arbeiten. Wenig ist nerviger als dauernd die R Datei im Unterordner des Unterordners des Unterordners zu finden, um das Programm zu öffnen. Deshalb kann man einstellen, dass sich immer das zuletzt verwendete R Skript und Projekt öffnet. Über R Projekte erfährt du in Kapitel 3. Gehe zu: Tools/Global Options.../General Mache einen Haken bei Restore most recently openend project at startup Und bei Restore previously open source documents at startup 2.4 Funktionen und ihre Argumente Eine Funktion erkennst du immer daran, dass sie von zwei runden Klammern gefolgt ist. Wenn du den Anweisungen in Kapitel 2.3 gefolgt bist, wird die Funktion auch extra farblich hervorgehoben. Schauen wir uns exemplarisch die Funktion zur Berechnungen des Mittelwertes an. mean(x = c(3, 4, 7, NA, 2), na.rm = TRUE) [1] 4 Wir haben hier sogar gleich zwei Funktionen: einmal mean() und einmal c(). Letztere bindet die Werte zusammen. Nun haben wir auch zwei so genannte Argumente: x und na.rm. Dem ersten Argument geben wir die für die Berechnung des Mittelwerts zu verwenden Werte und na.rm = TRUE entfernt potentielle fehlende Werte. Aus diesem Kapitel sollst du die Bezeichnungen Funktion und Argument mitnehmen. Außerdem ist es wichtig, dass Argumenten Informationen immer mit einem Gleichheitszeichen übergeben werden. Tatsächlich müssten die Namen der Argumente nicht immer ausgeschrieben werden. Wenn die Funktion mean() als erstes Argument die Werte erwartet, könnte man dieses ebenso gut weglassen. Vorausgesetzt man beachtet die entsprechende Reihenfolge der Argumente. mean(c(3, 4, 7, NA, 2), na.rm = TRUE) Das wird im Buch auch immer wieder gemacht, weil es übersichtlicher zum Lernen und schneller für dich später zum Anwenden ist. 2.5 Packages (Erweiterungen) R bietet von Beginn an eine Bandbreite an Funktionen. Über die Jahre haben sich viele fähige Programmierer an die Arbeit gemacht, die Funktionen von base R (dem normalen R) zu erweitern. So genannte Packages sind kostenlose Erweiterungen, die verschiedenste Aufgaben erheblich erleichtern können. Stell dir vor, du kaufst dir ein neues Smartphone. Auf dem Smartphone sind von Anfang an verschiedene Apps installiert. Es gibt aber viele Apps von Drittanbietern, die entweder neue Funktionen bieten oder bestehende Aufgaben einfacher oder komfortabler machen. Genauso sind Packages in R zu verstehen. Auch mit den von Anfang an integrierten Funktionen könnte man die meisten Sachen irgendwie hinbekommen. Nur wäre dies mit deutlich mehr Aufwand verbunden als heutzutage notwendig. Deshalb arbeiten wir im Verlaufe des Buches mit einigen Packages. Die Packages werden unter anderem auf CRAN, Github oder auf Bioconductor geteilt, von wo sie heruntergeladen und perfekt integriert werden können. Es gibt beispielsweise mittlerweile über 11 000 Packages alleine auf CRAN. Nun gibt es eine Funktion zum Installieren und eine zum eigentlichen Laden des Packages innerhalb von R. Abbildung 2.5 versucht den Unterschied der beiden Funktionen zu verdeutlichen. Abbildung 2.4: Vergleich vom (a) Installieren und (b) vom Laden von Packages. Während install.packages() das Package installiert, muss man es mit library() jedes Mal beim Starten von R neu laden. Erstere Funktion kannst du dir wie das Eindrehen einer Glühbirne vorstellen. Es bleibt dunkel im Zimmer, solange du den Lichtschalter nicht betätigst. Wenn du das Zimmer verlässt (R ausschaltest), machst du das Licht wieder aus. Jedes Mal, wenn du das Zimmer erneut betrittst, muss das Licht erneut eingeschaltet werden. Falls bei der Installation Fehler auftreten, kann noch das Zusatzargument dependencies verwendet werden. Damit werden die Packages installiert, von denen unser gewünschtes Packages gegebenenfalls zusätzlich abhängt. Die meisten Packages sind allerdings auch ohne dieses zusätzliche Argument voll funktionsfähig. Beim Starten von R muss jedes Mal aufs Neue der Befehl library() für jedes Package ausgeführt werden, welches du nutzen möchtest. 2.5.1 Installieren und laden Wie bereits erwähnt, muss zur Installation install.packages() benutzt werden. Wichtig ist hierbei, dass der Package Name in Anführungszeichen geschrieben ist. Das Argument dependencies = TRUE installiert hierbei zusätzlich alle Packages, auf denen das gewünschte Package basiert. Meistens ist dies allerdings nicht notwendig. install.packages(&quot;packageName&quot;, dependencies = TRUE) Damit man auf die Funktionen des Packages zugreifen kann, muss das Package jedes Mal  also nach jedem neuen Öffnen von RStudio  aus der Bibliothek mithilfe von library() erneut geladen werden. Hierbei sind keine Anführungszeichen notwendig. library(packageName) Zur besseren Übersichtlichkeit solltest du alle library() Befehle am Anfang des jeweiligen R Skriptes untereinander aufrufen. Wir werden im Rahmen dieses Buches vor allem vier Packages verwenden. Am besten schreibst du beim Üben also direkt oben library(tidyverse) library(here) library(rio) library(remp) in dein R Skript. In diesem Fall ist die Reihenfolge nicht entscheidend. Wenn verschiedene Packages jedoch gleichnamige Funktionen beinhalten, führt dies zu Problemen, auf die in Kapitel 2.5.3 eingegangen wird. 2.5.2 Notwendige Packages Diese vier essentiellen Packages werden wir nun zuerst installieren. Das remotes Package verwenden nur einmalig, um remp von Github installieren zu können. Wofür jedes einzelnen Package davon genau zuständig ist, wirst du im Verlaufe des Buches erfahren. Die Installation kann einige Minuten in Anspruch nehmen. install.packages(c(&quot;tidyverse&quot;, &quot;here&quot;, &quot;rio&quot;, &quot;remotes&quot;)) Um auf diverse Übungsdatensätze und interaktive Übungen zugreifen zu können, musst du das Package remp installieren. Allerdings befindet es sich nicht auf CRAN, sondern auf Github, weswegen der Befehl etwas anders lautet. Während der Installation des Packages kann es sein, dass du gefragt wirst, ob du bestimmte Packages aktualisieren möchtest. du solltest dies an der Stelle ohne weiteres Zutun mit Enter verneinen. library(remotes) install_github(&quot;j3ypi/remp&quot;) Alle anderen, teilweise speziell auf einen Kontext zugeschnittenen Packages, die im Rahmen des Buches vorgestellt werden, kannst du bei Bedarf installieren. Am Anfang jedes Kapitels mit einem neuen Package, wird immer extra darauf hingewiesen. 2.5.3 Namespace Beim Laden mehrerer Packages kann es sein, dass diese Funktionen mit dem selben Namen verwenden (siehe Kapitel 6.1). Während beim tidyverse lediglich zwei selten verwendete base R Funktionen überschrieben werden, kann es beim Arbeiten mit vielen verschiedenen nicht aufeinander abgestimmten Packages durchaus häufiger zur Namensgleichheit kommen. Das ist eine schwierig zu identifizierende Fehlerquelle, weil sich die Funktion plötzlich nicht mehr so verhält wie erwartet. Dabei verwendet man automatisch die Funktion, die aus dem Package stammt, welches als letztes geladen wird. Lädt man beispielsweise zuerst das Package tidyverse und anschließend data.table, erhält man folgende Meldung: library(tidyverse) library(data.table) Attaching package: data.table The following objects are masked from package:dplyr: between, first, last The following object is masked from package:purrr: transpose Drei Funktionen (between(), first(), last()) aus dem Package dplyr und eine Funktionen (transpose()) aus dem Package purrr werden von data.table überschrieben. Es ist also wichtig, diese Meldungen beim Laden eines Packages nicht zu ignorieren. Im Notfall kann dies durch die Verwendung von Doppelpunkten package::funktion() verhindert werden. So teilt man R explizit mit, welche Funktion aus welchem Package man meint. Um trotz des späteren Ladens von data.table auf die between() Funktion von dplyr zuzugreifen, würde man also beim Aufrufen der Funktion folgendes schreiben: dplyr::between(1:12, 7, 9) Da das tidyverse nur andere Packages lädt, kann man nicht tidyverse::funktion() schreiben. Stattdessen muss man das Package, aus dem die Funktion stammt (z.B. dplyr), direkt ansprechen. Für genauere Informationen zum Thema Namespace und die Hintergründe der damit verbundenen Environments sei auf Advanced R von Hadley Wickham verwiesen. 2.6 Fehler- und Warnmeldungen 2.6.1 Der Unterschied Es gibt einen großen Unterschied zwischen Fehler- und Warnmeldungen. Wie der Name bereits suggeriert, stoppen Fehlermeldungen den Code, während Warnmeldungen ein Ergebnis zurückgeben und nur auf mögliche Probleme hinweisen. Es ist also sehr wichtig, die roten Meldungen in der Konsole genau zu lesen, anstatt direkt in Panik zu geraten. Ein Beispiel für eine Fehlermeldung sehen wir, wenn wir die Zahl 1 mit dem Buchstaben c summieren möchten. 1 + &quot;c&quot; Error in 1 + &quot;c&quot;: non-numeric argument to binary operator Hier ist die Fehlermeldung eindeutig. Wir versuchen einen nicht-numerisches Buchstaben (das c) mit einem numerischen zu addieren. Leider sind Fehlermeldungen in R keineswegs immer so eindeutig zu interpretieren. Nicht selten sind sie kryptisch und gerade am Anfang wird man oft im Internet nach einer Lösung suchen müssen. Warnmeldungen haben zwar die selbe erschreckende rote Schrift wie Fehlermeldungen, allerdings starten sie mit Warning Message und stoppen den Code nicht. Warnmeldungen sind für dich als Benutzer gedacht, um dich auf mögliche Probleme bei deiner Eingabe hinzuweisen. Gerade bei der statistischen Auswertungen können Warnungen dir schon Mal häufiger über den Weg laufen und sollten nicht blind ignoriert werden. Die Sprache der Fehler- und Warnmeldungen muss dabei Englisch sein, da sonst nicht vernünftig im Internet danach gesucht werden kann. Bitte beachte die dafür notwendigen Anpassungen bei der Installation 2.1.1. 2.6.2 Wo bekomme ich Hilfe? Gerade bei den ersten Kontakten mit einer Programmiersprache, dauert das Warmwerden möglicherweise eine gewisse Zeit. Damit man nicht gleich die Motivation verliert und aufgibt, sind die richtigen Quellen für eine schnelle Hilfe essentiell. Glücklicherweise ist dies einer der größten Vorteile von R. Es existieren nicht nur ausführliche Dokumentationen der verschiedenen Funktionen mit Anwendungsbeispielen. Darüber hinaus ist die R Community auch besonders hilfsbereit. Falls du verzweifelt vor deinem Computer oder Laptop sitzt und nicht weißt, wieso dein Code schon wieder nicht funktioniert, solltest du besonders an fünf Orten nach Antworten suchen. Package Dokumentation. Das naheliegendste ist die Dokumentation der R Funktionen, die auch ohne Internet direkt in R aufgerufen werden können. Dafür kannst du entweder ein Fragezeichen vor die Funktion schreiben oder nach Anklicken der Funktion F1 auf der Tastatur drücken. Wenn wir zum Beispiel mehr über die Argumente der Funktion install.packages() erhalten möchten, können wir dies so erreichen: ?install.packages Die meisten modernen Packages stellen außerdem so genannte Vignetten zur Verfügung, in denen häufige Probleme ausführlich diskutiert und erklärt werden. Diese können mit vignette(\"nameDerVignette\") auch direkt innerhalb von R aufgerufen werden. Suchmaschine. In die Suchmaschine gibst du einfach r gefolgt von der Fehlermeldung oder dem Problem ein, an dem du gerade festhängst. Es vergeht kaum eine Fehlermeldung bei der das Programmierforum StackOverflow nicht an erster Stelle angezeigt wird. Denn meistens hat jemand anderes schon einmal genau dasselbe Problem gehabt hat. Stackoverflow. Dies ist wohl das mit Abstand größte Forum für Programmierfragen. Allerdings kann es einschüchternd sein, dort als absoluter Anfänger selbst eine Frage zu stellen. Voraussetzung zum Fragestellen ist dort ein kurzes reproduzierbares Beispiel des Fehlers. du erreichst die Seite unter https://stackoverflow.com/. RStudio Community. Eine etwas kleiner Alternative zu StackOverflow ist die RStudio Community. Statte ihr bei Zeiten unter https://community.rstudio.com/ mal einen Besuch ab. Forum zum Buch. Extra zum Buch gibt es auch ein eigenes deutschsprachiges Forum. Hier ist wohl der beste Ort, um eigene Fragen zu Fehlermeldungen oder Problemstellungen zu stellen. du brauchst dich dazu nicht einmal zwingend anmelden. du musst nur auf https://remp.forumotion.com/ gehen und deine Frage stellen. Habe dabei keine falsche Scheu  jeder war mal Anfänger. 2.6.3 Fehler beheben Während deiner ganzen Zeit beim Programmieren wirst du immer wieder falschen Code schreiben, der unweigerlich zu einem Fehler führt. Ob nun ein Package nicht geladen oder eine Klammer zu wenig vorhanden ist. Irgendetwas läuft häufig schief  so mag es zu Beginn erscheinen. Leider sind die dann ausgegebenen Fehlermeldungen in R nicht immer sonderlich aufschlussreich. In Kapitel 2.6.2 wurde bereits beschrieben, wo genau man Hilfe bekommen kann. Es ist zum Beispiel wenig informativ, wenn du jemandem schreibst: RStudio funktioniert nicht. Was funktioniert nicht? Wann tritt der Fehler auf? Hat es vorher mal funktioniert? Alle diese Informationen müssen dann erst einmal mühselig erfragt werden. Nun können die bereitzustellenden Informationen grundsätzlich in drei Abschnitte gegliedert werden. Problembeschreibung Stelle ein minimales, reproduzierbares Beispiel bereit Was wurde bereits probiert? Während Punkt 1 und 3 soweit verständlich sind, bedarf es eine Erklärung für reproduzierbare Beispiel. Ein reproduzierbares Beispiel enthält alle Informationen, die jemand anderes zum Replizieren des Fehlers benötigt. Es müssen also alle geladenen Packages angegeben werden. Außerdem ist es essentiell, den Datensatz in irgendeiner Form zur Verfügung zu stellen. Dabei ist es optimal, eine minimale Version des Problems zu abstrahieren. Angenommen, du stößt beim Rechnen mit einem eigenen Datensatz auf ein Problem. Niemand anderes im Internet hat diesen Datensatz. Nun kann ein möglichst kleiner Datensatz innerhalb von R erstellt werden, mit dem man auch auf dasselbe Problem oder auf denselben Fehler stößt. Da das jedoch gerade am Anfang nicht ganz einfach ist, wird erst in Kapitel 5.3 erläutert, wie man selbst Datensätze erstellen kann. Falls du die Frage im Forum zum Buch stellen magst, kannst du selbstverständlich den Fehler einfach an einem im remp enthaltenen Datensatz nachstellen. Eine weitere Möglichkeit ist die Funktion dput(), die einen bestehenden Datensatz in eine für andere durch Copy &amp; Paste einlesbare Struktur bringt. Gehen wir das Ganze einmal Schritt für Schritt durch. Angenommen, wir haben irgendeine Fehlermeldung bezüglich des big_five Datensatzes. Genauer gesagt möchten wir einfach nur die Spalten Extraversion und Neurotizismus in einem Streudiagramm gemeinsam abbilden. Da immer nur die minimale Struktur zur Verfügung gestellt werden soll (um das Beispiel möglichst einfach und übersichtlich zu halten), wählen wir also nur diese beiden Spalten (select()) der ersten 10 Personen (slice_head(n = 10)) aus. Die Funktion dput() erledigt dann den Rest. big_five %&gt;% select(Extraversion, Geschlecht) %&gt;% slice_head(n = 10) %&gt;% dput() structure(list(Extraversion = c(3, 3.1, 3.4, 3.3, 3, 2.8, 3.5, 3.5, 3, 3.1), Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;)), row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;)) Die Ausgabe dieser Funktion musst du im Detail nicht verstehen. Allerdings kannst du diese genau so kopieren und in R ausführen. Anschließend ist es jedem möglich, auf die Spalten Extraversion und Neurotizismus der ersten 10 Personen zuzugreifen, ohne den big_five Datensatz geladen zu haben. Nun kannst du den fehlerhaften Code auf das Datenbeispiel anwenden und beschreiben, was nicht funktioniert hat. Häufig kommt man von ganz alleine auf die Lösung, während man Schritt für Schritt die durchgeführten Code Zeilen auseinander nimmt, um das Problem zu replizieren. Bevor du allerdings die Frage an jemand anderes richtest, solltest du diese häufigen Fehlerquellen bereits ausgeschlossen haben: Tippfehler Package nicht installiert Package nicht explizit geladen Package ist zu alt Klammer zu viel oder zu wenig RStudio nicht über das R Projekt sondern über das R Skript geöffnet Anführungszeichen vergessen oder an falscher Stelle Alte Ergebnisse, die du nicht mehr auf dem Schirm hast, kommen dir in die Quere Komma statt Punkt bei Dezimalzahlen Numerische Spalten im Datensatz wurden als Character eingelesen (häufig bei Excel Dokumenten) Character Spalten sind fälschlicher Weise als Faktoren dargestellt Das Kapitel soll nicht etwa dazu dienen, dich vorm Fragenstellen abzuschrecken. Stattdessen soll es dafür sorgen, dir eine möglichst positive Erfahrung beim Fragenstellen zu gewährleisten. Auch wirst du merken, wie schnell du lernst, selbst die Probleme zu lösen. Auch wenn es manchmal erst kurz vorm Fragen beim Vorbereiten des reproduzierbaren Beispiels ist. Vor allem das Forum zum Buch steht dir jederzeit für Fragen zur Verfügung (https://remp.forumotion.com/). 2.7 Historische Relikte Dieses Kapitel richtet sich vor allem an diejenigen, die bereits vor diesem Buch Kontakt mit R gehabt haben. Da R seit über 25 Jahren existiert, gibt es dementsprechend auch sehr alte und in die Jahre gekommene Mittel und Wege, die mehr Probleme bereiten, als sie lösen. Keine der hier vorgestellten Funktionen solltest du jemals benutzen müssen: attach() und detach() werden benutzt, um einen Datensatz unsichtbar in die Umgebung zu laden. Die in diesem Buch besprochenen Funktionen sind allerdings alle auf die Arbeit direkt am Datensatz ausgelegt. gc() steht für Garbage Collection (engl. für Müllsammlung) und soll Speicherplatz nach vielen Berechnungen wieder freiräumen. Allerdings führt R im Hintergrund automatisch die Funktion aus, weswegen dieser manuelle Funktionsaufruf redundant ist. rm(list = ls()) verspricht eine saubere neue R Session zu kreieren. Daher findet man es häufig zu Beginn von R Skripten. Tatsächlich löscht dieser Befehl nicht alles, was zu schwierig identifizierbaren Fehlern führen kann. Spätestens seit es R Projekte gibt, ist dieser Befehl überflüssig (siehe Kapitel 3). setwd() teilt R mit, wo genau auf dem Computer sich der einzulesende Datensatz befindet. Auch das ist durch R Projekte und das here Package überflüssig geworden (siehe Kapitel 3.3). Darüber hinaus solltest du es auch in Teil I bis III dieses Buches (bis auf wenige Ausnahmen) dringend vermeiden, einzelne Spalten des Datensatzes separat abzuspeichern. Sämtliche vorgestellte Funktionen arbeiten am besten direkt am Datensatz. Wenn du irgendwann im Rahmen des vierten Teils dieses Buches ein tieferes Verständnis von R gesammelt hast, kannst du dich gerne darin versuchen. Die oben genannten Funktionen solltest du hingegen auch dann auf keinen Fall verwenden. "],["project.html", "Kapitel 3 Projektorientierung 3.1 Das Problem 3.2 R Projekte erstellen 3.3 Das here Package", " Kapitel 3 Projektorientierung 3.1 Das Problem Erinnere dich zurück an das letzte Mal, als du eine Datei irgendwo hochgeladen hast. Normalerweise öffnet sich dann ein kleines Dialogfenster, durch das du bis zu deiner Datei navigieren kannst. Das hat den Hintergrund, dass dein Betriebssystem den genauen Ort der Datei  den so genannten Pfad  wissen muss, um diese hochzuladen. Das gleiche kannst du beobachten, wenn du gefragt wirst, wo genau auf deinem Computer du eine heruntergeladene Datei speichern möchtest. In R war das Einlesen von Datensätzen genau deswegen lange ein Problem. Dabei gibt es vor allem vier zentrale Probleme hervorzuheben: Wenn jemand auf einem anderen Computer die Rechnung deines R Skripts nachvollziehen möchte, ist der Pfad auf dem anderen Computer anders. Der Dateipfad ist oft lang, weil die Zieldatei in vielen verschachtelten Unterordnern liegt. Häufig verschiebt man den Ordner mit der Zeit. Zwischen Betriebssystem ist die Art, den Pfad darzustellen, unterschiedlich. Zum Ersten Punkt kommt hinzu, dass die wenigsten überhaupt wissen, was genau ein Pfad ist und wie man diesen korrekt angeben müsste. Punkt zwei und drei ist vor allem ein Albtraum hinsichtlich Reproduzierbarkeit. du möchtest deine Analysen an einen Kollegen oder Dozenten schicken? Der Code wird ohne Anpassungen nicht funktionieren. du ordnest deine Dateien neu und möchtest Monate später deinen bisher immer funktionierenden Code erneut ausführen? Fehler! Glücklicherweise gibt es mittlerweile R Projekte, so dass du dich niemals mit Pfaden oder fehlender Reproduzierbarkeit auseinander setzten musst. R Projekte sind quasi das Drag and Drop der R Community  nur noch praktischer, da das Finden der Dateien automatisch passiert. So kann die Funktion zum Einlesen des Datensatzes unabhängig vom Ort des Ordners sehen, wo sich die Datei befindet. Die einzigen beiden Voraussetzungen sind, dass auf der einen Seite der Datensatz im selben Ordner oder Unterordner wie die R Projektdatei liegt und dass man das here Package lädt. 3.2 R Projekte erstellen Bleiben wir bei unserem Ordner namens Beispiel, der sich auf dem Desktop befindet. Wie in Abbildung 3.1 ersichtlich, befinden sich in diesem Ordner drei Dateien. Abbildung 3.1: Beispielhafte Ordnerstruktur mit R Skript, Projektdatei und Datensatz. Auswertung.R ist unser R Skript, Beispiel.Rproj unsere Projektdatei und video.xlsx der Datensatz, den wir zur Auswertung einlesen wollen. Die Projektdatei muss vorher erstellt werden. Dafür benötigt man nur ein paar Klicks. Oben rechts befindet sich ein Reiter namens Project: (None), wenn kein Projekt geöffnet ist und ansonsten der Projektname (zum Beispiel das Projekt Beispiel). Öffne zuerst das Dropdown Menu. Abbildung 3.2: Erster Schritt beim Erstellen eines neuen Projektes. Uns interessieren zum einen New Project... und Open Project... und zum anderen stehen weiter unten andere Projekte, die vorher geöffnet wurden (hier 07_Buch und 08_inductive). Man kann mit einem einfachen Klick zwischen diesen Projekten wechseln. Dieses Feature erleichtert die Arbeit ungemein, da man auf dem Computer nicht mehr irgendwo den Ordner mit den richtigen Dateien suchen muss. Zum Erstellen eines neuen Projekts klicke auf New Project.... Es erscheint ein neues Fenster. Abbildung 3.3: Zweiter Schritt beim Erstellen eines neuen Projektes. Wir entscheiden uns exemplarisch für Existing Directory. Das bedeutet, unser Ordner Namens Beispiel existiert bereits auf dem Desktop. Ob dieser leer oder bereits mit Dateien gefüllt ist, ist nicht weiter von Bedeutung. Abbildung 3.4: Letzter Schritt zur Erstelllung eines neuen Projektes. Mit einem Klick auf Create Project wird nun eine Projektdatei mit der Endung .Rproj in den gewählten Ordner gespeichert. Projekte bieten übrigens bezüglich Reproduzierbarkeit einen weiteren Bonus. Jedes Mal beim Starten wird eine neue und in sich abgeschlossene R Umgebung geladen, so dass man garantieren kann, das der Code genau so auch auf anderen Computern ausgeführt werden kann. Beachte, dass du beim Öffnen eines R Projects in dem Ordner nicht auf das R Skript (hier Auswertung.R) sondern auf die Projektdatei klickst und erst im Anschluss das R Skript öffnest. Für alle zukünftigen Öffnungen kannst du dann einfach in dem eingangs beschrieben Dropdown Menü rechts oben das Projekt auswählen. Damit R die Position der Projektdatei auch findet, brauchen wir nun außerdem das here Package. Beim Arbeiten in einer Cloud wie Dropbox kann es zu einer Fehlermeldung kommen, die besagt, dass RStudio nicht auf die Datei zugreifen kann. Um das zu umgehen, muss die Synchronisierung der Cloud für die Dauer des Arbeitens mit R angehalten werden. 3.3 Das here Package Die Magie passiert, wenn du nun das here Package lädst. Das Package findet sofort den relativen Pfad zu deiner Projektdatei heraus. Was bedeutet das? Während man früher beispielsweise auf Windows mit C:\\Users\\J-PhN\\Desktop\\Beispiel den absoluten Pfad zum Ordner eingeben musste, findet das here Package den Ordner Beispiel mit der Projektdatei, unabhängig von der Lage des Ordners. In der Praxis sieht das beim Laden so aus: &gt; library(here) here() starts at C:/Users/J-PhN/Desktop/Beispiel Würden wir den Ordner verschieben, hätte das keine Auswirkungen auf unseren Code. Das Package würde wieder zum Projektordner finden. Der erste Schritt ist also immer das Erstellen eines R Projekts und das Laden des here Packages am Anfang jedes neuen Skripts, mit dem man einen Datensatz einlesen möchte. "],["vars.html", "Kapitel 4 Variablen 4.1 Variablen speichern und verwenden 4.2 Grundlegende Datentypen 4.3 Datentypen konvertieren 4.4 Faktoren 4.5 Zeitdaten", " Kapitel 4 Variablen 4.1 Variablen speichern und verwenden Ein zentrales Konzept in R ist das Speichern von Variablen mithilfe des Zuweisungspfeils. Wenn man das Ergebnis der durchgeführten Operation nicht speichert, ist es sofort weg und muss erneut ausgeführt werden, falls man erneut einen Blick auf das Ergebnis der Operation erhaschen möchte. Würde man nun 2 + 2 [1] 4 rechnen, gibt R zwar 4 zurück, allerdings kann man später nicht mehr auf diese 4 zurückgreifen. Wenn man beispielsweise einen Datensatz einliest, ohne diesen mit dem Zuweisungspfeil zu speichern, ist dieser sofort wieder weg und man kann nicht damit arbeiten. Mithilfe des Zuweisungspfeil wird die Variable in die lokale Environment gespeichert. Wir erinnern uns, die Environment ist in der Standardeinstellung nach Installation im Fenster oben rechts. Möchten wir beispielsweise die Rechenoperation von vorhin namens rechnung speichern, würde man wie folgt vorgehen. rechnung &lt;- 2 + 2 Im Nachfolgenden könnte man nun diese Variable wieder aufrufen. rechnung [1] 4 Genauso gut könnte man mit dieser Variable weiterarbeiten. Zum Beispiel könnte man das Ergebnis der vorherigen Rechnung mit 4 Multiplizieren. rechnung * 4 [1] 16 Variablen kann man grundsätzlich fast so benennen, wie man möchte. Man darf nur nicht mit einer Zahl anfangen oder nach einem Punkt direkt eine Zahl als Namen wählen wie bei .2VariablenName. Auf Umlaute sollte im Zusammenhang mit Programmiersprachen ebenfalls immer verzichtet werden. Das liegt an verschiedenen Zeichenkodierungen, auf die an dieser Stelle nicht weiter eingegangen werden soll. In den Variablen können sämtliche Datenstrukturen (siehe Kapitel 12) verstaut werden. Für den Moment reicht es für uns zu wissen, dass wir Datensätze und Zwischenergebnisse in den Variablen abspeichern müssen, um weiter darauf zugreifen zu können. Variablen können einfach überschrieben werden, indem man der Variablen einen anderen Wert zuweist. Gerade in der Datenvorbereitung kann es schon einmal verlockend sein, die Änderungen unter demselben Variablennamen zu speichern. Wenn man allerdings eine unbeabsichtigte Änderung abspeichert, kann dies nicht rückgängig gemacht werden. Der Datensatz muss dann erneut eingelesen werden. Es sei also gerade am Anfang Vorsicht geboten. Auf der anderen Seite sollte man auch nicht jeden einzelnen Schritt in der Datenvorbereitung mit einem bedeutungslosen Namen versehen. Nicht das am Ende rechnung1, rechnung2, rechnung3 und rechnung4 existieren ohne jegliche Information im Namen, welche Variante nun welche Änderung enthält. Aussagekräftige Namen helfen anderen, deinen Code zu verstehen. Wenn du dir nun denkst, ohnehin nicht mit anderen zusammenarbeiten zu werden, lass dir folgendes gesagt sein: der andere bist in den meisten Fällen du selbst einige Wochen oder Monate später. Ohne Dokumentation und vernünftige Namensgebung sieht dein R Skript Monate später schnell so aus, als hätte es irgendein Fremder geschrieben. dein zukünftiges Ich wird dir dankbar sein. Behalte immer im Hinterkopf, dass der Zuweisungspfeil zwar die Variable speichert, aber keinen direkten Output in der Konsole ausgibt. Oft denkt man dann, es wäre nichts passiert. Es hilft dann, den Namen der Variable wie zuvor gezeigt erneut aufzurufen. Wenn man etwas im Code kommentieren möchte, muss eine führende Raute hinzugefügt werden. Das eignet sich nicht nur für kurze Beschreibungen, sondern auch als Gliederung eines langen R Skripts. Beispielsweise könnte man so den Abschnitt der Berechnung vom Erstellen von Visualisierungen optisch trennen. Die Anzahl der Rauten spielt dabei keine Rolle. ############################### # Berechnung I ############################### # Dieser Code summiert 2 und 2 2 + 2 [1] 4 4.2 Grundlegende Datentypen Grundsätzlich gibt es in R vier verschiedene Grunddatentypen: Integer, Double, Character und Logical (siehe Abbildung 4.1). Dabei lassen sich Integer und Double zu Numeric zusammenfassen, da die Unterscheidung dieser beiden in R selten von Bedeutung ist. Schauen wir uns die Datentypen nun etwas genauer an. Numeric (&lt;num&gt;) beschreibt numerische Werte, also Zahlen. Integer (&lt;int&gt;) sind ganze Zahlen und Doubles (&lt;dbl&gt;) Dezimalzahlen. Beachte dabei, dass Dezimalzahlen in R mit Punkten und nicht mit Kommata ausgedrückt werden. Abbildung 4.1: Schematische Übersicht über die wichtigsten Datentypen in R. Beispiele für Numerics in wären zum Beispiel Alter, Gehalt oder der Intelligenzquotient. Wenn wir später Datentypen aus einem echten Datensatz ansehen, wirst du schnell merken, dass beinahe alle Zahlen als Double deklariert werden. Das liegt an der Eigenheit von R, ein L hinter die Zahl setzen zu müssen, wenn es sich um eine ganze Zahl hat. Dies hat allerdings keinerlei Auswirkung auf die in diesem Buch vorgestellten Funktionen. Double: 3.14 42 Integer: 42L Character (&lt;chr&gt;) ist der Datentyp, der Text enthalten kann  also einzelne Buchstaben, Zeichen, Wörter oder ganze Sätze. Dabei muss der Text immer in Anführungszeichen stehen. Solange die Anführungszeichen verwendet werden, kann mit Ausnahme vom Backslash (\\) alles geschrieben werden. Beispiele für Characters wären zum Beispiel Fernsehserien, das Herkunftsland, die Muttersprache oder Allergien. &quot;Hallo Welt&quot; Logical oder auch logische Datentypen sind etwas abstrakter und kommen in Datensätzen seltener vor. Sie werden dann benötigt, wenn wir aufgrund von Bedingungen manche Operationen durchführen möchten und andere wiederum nicht. Beispiele dafür wären die Auswahl derjenigen Personen, die über 50 Jahre alt sind oder das Erstellen einer neuen Spalte, wenn das Geschlecht weiblich ist. Dabei wird nämlich geprüft, ob die Aussage (zum Beispiel Person A ist älter als 50) wahr oder falsch ist. Das heißt, es gibt dabei zwei Zustände: TRUE oder FALSE. Eine Bedingung kann entweder zutreffen oder eben nicht. Es gibt verschiedene Funktionen, die TRUE oder FALSE zurückgeben, auf die wir im Verlaufe des Buches noch stoßen werden. Grundsätzlich gibt es dabei nur wenige verschiedene Grundoperatoren, die man kennen sollte. Um zu schauen, ob zwei Werte gleich sind, benutzen wir ein doppeltes Gleichheitszeichen (==). 2 == 2 [1] TRUE Gleiches Prinzip gilt für größer gleich (&gt;=) und kleiner gleich (&lt;=). Für größer (&gt;) oder kleiner (&lt;) reicht hingegen das einzelne mathematische Zeichen. Möchten wir nun logische Operationen kombinieren, verwenden wir UND (&amp;) oder ODER (|). Bei UND müssen beide Aussagen wahr sein, 1 &lt; 2 &amp; 1 == 2 [1] FALSE bei ODER hingegen nur mindestens eine. Man würde es wie folgt lesen: Entweder ist 1 kleiner 2 ODER 1 ist gleich 2. Da die erste Aussage wahr ist, wird TRUE zurückgegeben. 1 &lt; 2 | 1 == 2 [1] TRUE So können beliebig viele logischen Operationen miteinander kombiniert werden. Ein besonderer Fall logischer Datentypen ist NA (Akronym für Not Available), also die Bezeichnung für einen fehlenden Wert. Wir können einen Wert oder eine Variable auf seinen Datentyp überprüfen. Zurückgegeben wird uns nur TRUE oder FALSE. Die für uns interessanten Funktionen hierfür heißen is.numeric(), is.character(), is.logical() und is.na(). Natürlich gibt es auch is.double() und is.integer(), allerdings genügt uns für die Anwendungen in diesem Buch is.numeric(). Möchte man generell herausfinden, mit welchem Datentyp man es zu tun hat, verwendet man typeof(). typeof(&quot;R ist toll.&quot;) [1] &quot;character&quot; Da es unpraktisch ist, bei diversen Spalten eines Datensatzes einzeln den Typ abzufragen, wird dieser in tibbles (siehe Kapitel 12.3)  dem Datensatzformat, welches wir innerhalb von R konsistent im gesamten Buch verwenden  direkt unter dem Spaltennamen angezeigt. big5 # A tibble: 200 x 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # ... with 196 more rows Da dort nur die Spalten angezeigt werden, die auf den Bildschirm passen, kann glimpse() eine übersichtlichere Möglichkeit bieten, um einen schnellen Überblick über sämtliche Datentypen zu erhalten. glimpse(big5) Rows: 200 Columns: 7 $ Alter &lt;dbl&gt; 36, 30, 23, 54, 24, 14, 32, 20, 29, 17, 30, 15, 14, 23, 27, 15, 1964... $ Geschlecht &lt;chr&gt; &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;... $ Extraversion &lt;dbl&gt; 3.0, 3.1, 3.4, 3.3, 3.0, 2.8, 3.5, 3.5, 3.0, 3.1, 3.2, 3.5, 3.0, 3.2... $ Neurotizismus &lt;dbl&gt; 1.9, 3.4, 2.4, 4.2, 2.8, 3.5, 3.1, 2.6, 3.7, 3.6, 3.6, 2.8, 3.8, 2.0... $ O1 &lt;dbl&gt; 5, 5, 3, 2, 5, 5, 3, 2, 4, 4, 5, 4, 2, 5, 4, 2, 5, 3, 5, 5, 3, 4, 2,... $ O2 &lt;dbl&gt; 1, 3, 3, 5, 1, 1, 1, 1, 1, 3, 2, 3, 3, 1, 2, 3, 1, 2, 4, 1, 1, 1, 4,... $ O3 &lt;dbl&gt; 5, 5, 5, 3, 5, 5, 5, 3, 5, 4, 5, 4, 3, 5, 4, 5, 5, 5, 4, 4, 1, 5, 3,... Möchten wir nun mehrere Werte eines Datentypens aneinanderreihen, um sie zum Beispiel in eine Spalte eines Datensatzes zu schreiben, können wir c() (Abkürzung für Combine) verwenden. Möchtest du beispielsweise die Werte 1, 4, 5, und 10 kombinieren, erstellt dir c() einen Vektor (siehe Kapitel 12.1). Die Feinheiten und Merkmale von Vektoren brauchen dich an dieser Stelle nicht zu interessieren. Allerdings brauchen wir die Funktion c() des Öfteren, um Werte aneinander zu reihen. vec &lt;- c(1, 4, 5, 10) Wenn du verschiedene Datentypen innerhalb von c() miteinander kombinierst, werden die Datentypen ineinander umgewandelt. Dabei gilt Character &gt; Integer &gt; Logical. Wenn Zahlen mit Buchstaben kombiniert werden, wird also alles zu Buchstaben, auch wenn eigentlich Zahlen gemeint sind. Es kann passieren, dass Zahlen als Buchstaben interpretiert werden. Das kommt vor allem beim Einlesen von schlecht formatierten Datensätzen vor. Wenn eine Berechnung nicht so funktioniert, wie sie sollte, lohnt es sich, die Datentypen der jeweiligen Spalten zu überprüfen. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 4.3 Datentypen konvertieren Datentypen können auch umgewandelt werden. Die Namen sind ähnlich wie oben bereits beschrieben, nur dass der Präfix hier nicht is, sondern as ist. Um den vorhin erstellten Vektor vec in den Typ Character umzuwandeln, würde man dementsprechend as.character(vec) [1] &quot;1&quot; &quot;4&quot; &quot;5&quot; &quot;10&quot; schreiben. Wie man sieht, stehen nun sämtliche Zahlen in Anführungszeichen. Es gibt diverse Datentypen, die auf diesen vier Grundtypen aufbauen. Zwei wichtige, Faktoren und Zeitdaten, werden wir uns im Folgenden noch anschauen. 4.4 Faktoren Wenn wir wissen, wie viele Ausprägungen Characters (Zeichenketten) annehmen können, verwenden wir Faktoren. Ein illustratives Beispiel hierfür wäre die Aufteilung in Experimental- und Kontrollgruppen. Bereits bei der Versuchsplanung weißt du, wie viele Gruppen du haben möchtest (beispielsweise eine Experimental- und zwei Kontrollgruppen), damit du eine entsprechende Stichprobenplanung durchführen kannst. Schauen wir uns das ganze mal etwas konkreter an. Angenommen die Information über die Bedingung (Experimental, Kontrolle) befindet sich in der Variable namens Bedingung als Characters. bedingung &lt;- c(&quot;exp&quot;, &quot;kont1&quot;, &quot;exp&quot;, &quot;exp&quot;, &quot;kont2&quot;, &quot;kont1&quot;) Mit der Funktion factor() können nun Faktoren daraus gemacht werden. Zusätzlich sollte man das optionale levels Argument verwenden, um die Reihenfolge der Faktorstufen festzulegen. factor(bedingung, levels = c(&quot;exp&quot;, &quot;kont1&quot;, &quot;kont2&quot;)) [1] exp kont1 exp exp kont2 kont1 Levels: exp kont1 kont2 Wenn die als Faktor zu kodierende Spalte numerisch ist, können mit dem Argument labels die Namen der verschiedenen Ausprägungsgrade spezifiziert werden. Ein häufiges Beispiel hierfür wäre die Variable Geschlecht mit drei Ausprägungsgeraden. geschlecht &lt;- c(1, 1, 2, 3, 1, 3) Die Umkodierung geht mit dem labels Argument intuitiv. factor(geschlecht, labels = c(&quot;m&quot;, &quot;f&quot;, &quot;d&quot;)) [1] m m f d m d Levels: m f d Grundsätzlich sollte man Faktoren nur bei Bedarf erstellen (zum Beispiel unmittelbar vor der ANOVA oder vor Erstellen einer Abbildung), da Faktoren nicht mit allen Funktionen erwartungsgemäß harmonieren. Tatsächlich behandelt R Faktoren als Integer, was zu überraschenden Outputs führen kann. Verwende Faktoren also am besten nur dann, wenn du sie wirklich brauchst. Beispielsweise zum Rechnen inferenzstatistischer Verfahren oder unmittelbar vor dem Erstellen von Visualisierungen. Wie man mit Faktoren konkret umgehen kann, wird in Kapitel 6.11 erklärt. 4.5 Zeitdaten Ähnlich wie Faktoren bauen auch Zeitdaten auf den Grunddatentypen auf. Im Rahmen dieses Buches wird kaum mit Zeitdaten gearbeitet. Trotzdem wird auch dieser Datentyp kurz vorgestellt und im Rahmen der Datenvorbereitung auch kurz auf verschiedene Umwandlungen eingegangen. Schauen wir uns dafür die Spalte Watchdate des Datensatzes video an. Das Format ist dabei POSIXct, was für Portable Operating System Inferface calendar time steht (in der Ausgabe mit dttm für Datetime abgekürzt). In dem Fall ist es offensichtlich das falsche Format, da wir keine Informationen über die genaue Uhrzeit, sondern ausschließlich das Datum haben. Mit dem Package lubridate kann man dieses intuitiv in das zweite wichtige Zeitformat bringen, dem sogenannten Date. Die mutate() Funktion wird erst in Kapitel 6.4 genau vorgestellt. Zur Illustration des Prinzips sollte es trotzdem enthalten sein. Beachte, dass auch das Package tidyverse geladen werden muss. Dabei steht ymd() für year-month-date, da in diesem Fall das Datum in diesem Format vorliegt. Weitere Funktionen werden unter Kapitel 6.12 vorgestellt. Es gibt außerdem den difftime Datentyp, der die Differenz zwischen Tagen, Wochen und Monaten berechnen kann, auf den hier aber nicht weiter eingegangen werden soll. "],["io.html", "Kapitel 5 Datensätze 5.1 Einlesen externer Dateien 5.2 Datensätze aus Packages laden 5.3 Datensätze in R erstellen 5.4 Speichern und Konvertieren", " Kapitel 5 Datensätze 5.1 Einlesen externer Dateien Im Regelfall möchte man einen erhobenen Datensatz zur Auswertung in R einlesen. Datensätze können dabei in verschiedenen Formaten vorliegen. Dies ist vor allem abhängig davon, mit welchen Programmen Unternehmen, Universitäten oder Kollegen zur Datenerhebung arbeiten. Einige Beispiele für Dateientypen, in denen Daten häufig gespeichert werden, sind R (.RData | .rda | .rds) Excel (.xlsx | .xls) SPSS (.sav) Stata (.dta) Comma separated values (.csv) Tabular separated values (.tsv) Eingelesen werden können sämtliche Dateitypen mithilfe von einer einzigen Funktion namens import() aus dem rio Package. Dabei erkennt die Funktion die Dateiendung und übernimmt hinter den Kulissen alles Weitere. Damit der Datensatz als tibble (ref Datenstrukturen) eingelesen wird, solltest du setclass = \"tbl\" setzen. Für den Moment musst du Dir beim setclass Argument allerdings noch nichts denken (aber trotzdem verwenden!). Wir laden also zuerst das Package mit library(rio) und lesen anschließend die Datei (hier namens video) mit import() ein. Dabei achten wir natürlich darauf, den Datensatz direkt mit dem Zuweisungspfeil zu speichern (siehe Kapitel 4.1). daten &lt;- import(&quot;video.xlsx&quot;, setclass = &quot;tbl&quot;) Dabei muss sich das test.xlsx Excel Dokument innerhalb des selben Ordners wie die Projektdatei befinden. Wenn die Datei in einem Unterordner ist, Abbildung 5.1: Beispielshafte Ordnerstruktur mit Unterordner für Datensätze, R Skript und Projektdatei. muss man den relativen Pfad  also den Weg bis zur Datei innerhalb der Unterordner (hier Daten) des Ordners (hier Beispiel)  zusätzlich der R Funktion mitteilen. Dabei schreibt man den Namen des Unterordners oder den mehrerer Unterordner vor den Dateinamen und trennt diese mit einem Slash (/). daten &lt;- import( file = here(&quot;Daten&quot;, &quot;video.xlsx&quot;), setclass = &quot;tbl&quot; ) Manchmal liegen Daten jedoch nicht in einer Datei sondern in vielen verschiedenen vor. Das ist vor allem häufig bei biophysiologischen Messung wie Eye Tracking der Fall. Dort werden die erhobenen Daten pro Person abgespeichert. Da wir nicht 20 Mal import() kopieren möchten (da Copy &amp; Paste sehr fehleranfällig ist), gibt es die Funktion import_list(). Angenommen im Ordner Daten wären unsere 20 Excel Dokumente, in denen jeweils die Daten pro Person liegen, könnte man zuerst mit dir() die Dateinamen herausfinden, um diese dann mit import_list() einzulesen. Mit dem Zusatzargument rbind = TRUE können wir die Datensätze direkt zusammenfügen. Voraussetzung dafür ist, dass die Datensätze die gleichen Spalten haben. Da die Daten in einem Unterordner liegt, müssen wir dies zusätzlich import_list() mit der here() Funktion sagen, wo sich die Datensätze befinden. dateien &lt;- dir(&quot;Daten&quot;, pattern = &quot;.xlsx$&quot;) daten &lt;- import_list( file = here(&quot;Daten&quot;, dateien), setclass = &quot;tbl&quot;, rbind = TRUE ) Das Dollar-Zeichen signalisiert in diesem Fall nur, dass wir am Ende des Dateinamens, die Endung .xlsx (also ein Excel Dokument) erwarten. Das selbe funktioniert übrigens für den Fall, verschiedene Excel Sheets innerhalb eines Excel Workbooks zu haben. Mit import_list() können alle auf einen Schlag eingelesen werden. 5.2 Datensätze aus Packages laden Zum Bearbeiten dieses Buches brauchst du jedoch keine externen Datensätze, sondern nur jene, die im remp Package enthalten sind. Nach laden des Packages hast du grundsätzlich erst Zugriff auf alle enthaltenden Datensätze, wenn du extra die data() Funktion aufrufst. Zum Einlesen des video Datensatzes, muss demnach library(remp) data(video) geschrieben werden. So kannst du das Gelernte sofort in deiner RStudio Session nachvollziehen und ausprobieren. Sollte das Package vorher nicht geladen sein, kann alternativ auch nur data(video, package = &quot;remp&quot;) verwendet werden. Da die Syntax länger ist und beim Üben das remp Package sowieso geladen werden sollte, ist jedoch der zuvor dargestellte Weg empfohlen. 5.3 Datensätze in R erstellen Direkt innerhalb von R Datensätze zu erstellen, macht nur in sehr wenigen Anwendungsfällen tatsächlich Sinn. Der wohl Wichtigste ist das Erstellen eines minimalen reproduzierbaren Beispiels, falls man auf einen Fehler stößt, den man selbst nicht lösen kann. Für größere Datensätze sollte man die Daten jedoch besser in Datenformaten wie .csv oder .xlsx kreieren, die genau für so etwas erfunden wurden. Möchte man einen Datensatz erstellen, muss man lediglich der Funktion tibble() Werte übergeben. In diesem Beispiel speichern wir den neuen Datensatz mit den zwei Spalten Extraversion und Geschlecht als my_tbl. Die Funktion c() (für combine) kombiniert die einzelnen Werte eines Datentyps und kettet selbige aneinander. my_tbl &lt;- tibble( Extraversion = c(1.2, 2.7, 1.5, 4.8), Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;) ) my_tbl # A tibble: 4 x 2 Extraversion Geschlecht &lt;dbl&gt; &lt;chr&gt; 1 1.2 m 2 2.7 f 3 1.5 f 4 4.8 m Wenn wir nun auf eine Spalte zugreifen möchten, geht das beispielsweise nach dem Schema Datensatz$Spalte oder pull(). Der Dollar-Operator bei erster Methode wird an dieser Stelle nur erwähnt, da es noch nicht vollständig möglich ist, auf diese Funktionalität zu verzichten. my_tbl$Extraversion [1] 1.2 2.7 1.5 4.8 Grundsätzlich werden wir im Verlaufe des Buches allerdings der Konsistenz halber die pull() Funktion aus dem tidyverse bevorzugen. pull(my_tbl, Geschlecht) [1] &quot;m&quot; &quot;f&quot; &quot;f&quot; &quot;m&quot; 5.4 Speichern und Konvertieren Das Speichern von Datensätzen funktioniert durch das rio Package ähnlich intuitiv wie das Importieren von Dateien. Anstelle von import() benutzen wir dafür stattdessen export(). Das erste Argument der Funktion ist der Datensatzname und das zweite Argument ist der gewünschte Dateiname. Der Dateientyp wird durch die gewählte Endung festgelegt. Möchte man beispielsweise den fertig aufbereiteten Datensatz video_clean als csv Datei abspeichern, würde man export(video_clean, &quot;video_bereinigt.csv&quot;) schreiben. Manchmal ist es nützlich, den Datensatz unabhängig vom Einlesen umzuwandeln. Das kann zum Beispiel der Fall sein, wenn deine Kollegen Dir eine SPSS Datei schicken (.sav) und du selbst kein SPSS hast, aber trotzdem einen Blick in die Daten werfen möchtest. Die Entwickler des rio Packages haben auch daran gedacht und die Funktion convert() geschrieben. Als erstes Argument übergibst du der Funktion den ursprünglichen Dateinamen (mit Endung) und als zweites den selben oder einen anderen Dateinamen mit der Endung des gewünschten Dateientypen. Sinnvoll wäre in diesem Kontext das Umwandeln in ein Excel Dokument, da dieses mit MS oder Libre Office problemlos geöffnet werden kann. convert(&quot;video.sav&quot;, &quot;video.xlsx&quot;) Die neue Datei wird sowohl bei export() als auch bei convert() in deinem Projektverzeichnis gespeichert. "],["datenvorbereitung.html", "Kapitel 6 Datenvorbereitung 6.1 Einführung 6.2 select(), rename() und relocate() 6.3 filter() und arrange() 6.4 mutate() und across() 6.5 Eigene Funktionen 6.6 rowwise() und c_across() 6.7 Breites und langes Datenformat 6.8 Spalten trennen 6.9 Datensätze zusammenführen 6.10 Character Manipulation 6.11 Faktoren verändern 6.12 Mit Zeitdaten arbeiten 6.13 Binäre Antwortmatrix erstellen", " Kapitel 6 Datenvorbereitung 6.1 Einführung Die Datenvorbereitung oder auch Datenaufbereitung ist im Regelfall der mit Abstand aufwendigste Teil. Selten hat man nach der Datenerhebung bereits einen perfekt formatierten Datensatz, den man statistisch auswerten kann. Manche gehen sogar so weit, der Datenvorbereitung einen Anteil von über 90% der gesamten Bearbeitungszeit zuzuschreiben. Mit den Funktionen des tidyverse ist dies heutzutage glücklicherweise leicht zu bewerkstelligen. Das tidyverse ist ein Sammelsurium an Packages, die die gleiche Philosophie teilen. Dabei steht der Name tidy universe, also eine Art aufgeräumtes Universum. Beim Laden des tidyverse werden die acht in Abbildung 6.1 illustrierten Packages gemeinsam bereitgestellt. Damit spart man sich im Prinzip nur das einzelne Aufrufen der acht Packages. Man könnte stattdessen auch jedes Package einzeln laden. Abbildung 6.1: Übersicht über die Packages im tidyverse. Ausgeführt in R sieht das wie folgt aus. Unter Conflicts werden Funktionen genannt, die den selben Namen wie base R Funktionen haben und die von hier an überschrieben werden. library(tidyverse) -- Attaching packages ------------------ tidyverse 1.3.0 -- v ggplot2 3.3.2 v purrr 0.3.4 v tibble 3.0.4 v dplyr 1.0.2 v tidyr 1.1.2 v stringr 1.4.0 v readr 1.4.0 v forcats 0.5.0 -- Conflicts --------------------- tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() ggplot2 bietet ein mächtiges Framework zum Erstellen von Visualisierungen (siehe Kapitel 8). tibble erweitert das klassische Format eines Datensatzes (früher data.frame) (siehe Kapitel 12.3). tidyr stellt vor allem zwei zentrale Funktionen zum Wechsel zwischen langem und breiten Dateiformat dar (siehe Kapitel 6.7). readr bietet verschiedene Funktionen zum Einlesen. Wir werden im Rahmen dieses Buches nur indirekt durch das rio Package darauf zurückgreifen (siehe Kapitel 5). purrr für verschiedene Funktionen für iterative Prozesse (siehe Kapitel 13). dplyr stellt diverse Funktionen zur Datenvorbereitung vor und stellt den Hauptteil dieses Kapitel dar. stringr zur Veränderung von Charactern, also sogenannten Buchstabenfolgen (siehe Kapitel 6.10). Dabei iststring ein anderes Wort für Characters. forcats zur Manipulation von Faktoren (siehe Kapitel 6.11). Der Name kommt von for categoricals und bietet folglich Funktionen für kategorische Variablen. Nicht mit dem tidyverse geladen, aber dennoch gut kombinierbar sind außerdem lubridate zur Manipulation von Zeitdaten (siehe Kapitel 6.12). inductive für die gängigsten inferenzstatistischen Verfahren (siehe Kapitel 9). Die in diesem Kapitel eingeführten Funktionen zur Datenaufbereitung sind in sich konsistent. Man muss das Prinzip also nur einmal verstehen, um sämtliche Funktionen anwenden zu können. Dabei sind diese durch die ausdrucksstarke Namensgebung beinahe schon selbsterklärend. Schauen wir uns mal eine typische Aneinanderreihung von Befehlen an: big5 %&gt;% select(Geschlecht, Extraversion) %&gt;% filter(Geschlecht == &quot;m&quot;) %&gt;% mutate(Extraversion_lg = log(Extraversion)) # A tibble: 82 x 3 Geschlecht Extraversion Extraversion_lg &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 m 3 1.10 2 m 3.4 1.22 3 m 3.3 1.19 4 m 3.5 1.25 # ... with 78 more rows Gelesen würde es wie folgt: Man nehme den Datensatz big5 UND DANN wähle die Spalten Geschlecht, Extraversion UND DANN filtere die Spalten, in denen Geschlecht gleich m (für männlich) ist UND DANN mutiere oder verändere die (neue) Spalte Extraversion_lg durch die logarithmierten Werte der Extraversion Die anderen Funktionen sind ähnlich intuitiv und nahe an der englischen Sprache benannt. Besonders ist an dieser Stelle das kryptische Symbol Prozent-Größer als-Prozent (%&gt;%). Dieses Symbol können wir nur nach Laden des tidyverse (oder genauer magrittr) verwenden. Es macht dabei nichts anderes als die Übergabe oder das Weiterreichen des modifizierten Datensatzes an die nächste Funktion. Dies ist nur möglich, da das erste Argument der hier behandelten Funktionen immer der Datensatzname ist. Daher können wir uns hier den Namen des Datensatzes sparen. In dem obigen Beispiel werden zuerst zwei der Spalten ausgewählt. Dann wird das Ergebnis dieses Befehls  also der Datensatz mit den zwei Spalten  im nächsten Schritt der Funktion filter() übergeben. Dieses Verbindungssymbol %&gt;% wird Pipe genannt. Es kann mit dem Shortcut strg + shift + M beziehungsweise cmd + shift + M direkt erstellt werden. Die Verwendung der Pipe hat zwei große Vorteile: Die Verschachtelung mehrerer Funktionen ineinander wird verhindert. Wir müssen nicht jedes Ergebnis der verschiedenen Funktionen einzeln zwischenspeichern. Trotzdem müssen wir das Ergebnis dieser aneinandergeketteten Funktion natürlich irgendwann mit dem Zuweisungspfeil speichern. daten &lt;- big5 %&gt;% select(Geschlecht, Extraversion) %&gt;% filter(Geschlecht == &quot;m&quot;) %&gt;% mutate(Extraversion_lg = log(Extraversion)) Beachte, dass sämtliche Änderungen, die du am Datensatz vollziehst, erst gespeichert werden, wenn du sie mit dem Zuweisungspfeil (siehe Kapitel 4.1) einer Variable zuweist. Wenn dieser Variablenname bereits vergeben ist (z.B. der bisherige Datensatzname) wird dieser überschrieben. Um das rückgängig zu machen, muss der Datensatz dann einfach wieder neu eingelesen werden. Es empfiehlt sich bei einschneidenden Änderungen einen neuen Variablennamen zu verwenden. Ein zentrales Konzept ist die sogenannte Pipe (%&gt;%), die verschiedenste Funktionsaufrufe aneinanderbinden kann. Dabei wird der Datensatz an die nächste Funktion weitergeben. 6.2 select(), rename() und relocate() Die Funktionen in diesem Kapitel beschäftigen sich mit der Auswahl, Umbenennung und Umordnung von Spalten. Wir haben die Funktion select() bereits im vorherigen Kapitel kennengelernt. Es können also beliebig viele Spalten ausgewählt werden. Dies ist vor allem sehr nützlich, wenn der Datensatz sehr groß ist und man übersichtlich nur die Spalten haben möchte, die zur Auswertung verwendet werden. Zur Auswahl einer Spalte muss nur der Name (ohne Anführungszeichen) übergeben werden. Man kann auch direkt in der Funktion die Spalte umbenennen. Dabei muss auf der linken Seite des Gleichheitszeichen der neue Name stehen. big5 %&gt;% select(Extraversion, Neuro = Neurotizismus) # A tibble: 200 x 2 Extraversion Neuro &lt;dbl&gt; &lt;dbl&gt; 1 3 1.9 2 3.1 3.4 3 3.4 2.4 4 3.3 4.2 # ... with 196 more rows schreiben. Zur Auswahl der Spalten von Extraversion bis O2 verwendet man einen Doppelpunkt. big5 %&gt;% select(Extraversion:O2) Möchte man nur die Spalte Geschlecht entfernen und den Rest ausgeben lassen, erreicht man dies mit einem Minus vor dem Spaltennamen. Bei mehreren zu entfernenden Spalten müsste man diese in Klammern rahmen (z.B. -(Extraversion:O2)). big5 %&gt;% select(-Geschlecht) Darüber hinaus können wir so genannte Helferfunktionen verwenden. Diese können nur in Kombination mit einer anderen Funktion verwendet werden. Ein nützliches Beispiel ist where(). So können beispielsweise alle numerischen Spalten ausgewählt werden. big5 %&gt;% select(where(is.numeric)) Eine weitere nützliche Helferfunktion ist starts_with(). So könnte man in diesem Fall beispielsweise alle Fragen zum Persönlichkeitsfaktor Offenheit auswählen, da diese alle mit dem Buchstaben O beginnen. big5 %&gt;% select(starts_with(&quot;O&quot;)) Außerdem nützlich sind ends_with() und contains(). Wenn man hingegen die Spalten nur umbenennen und dabei den gesamten Datensatz behalten möchte, muss man rename() verwenden. Die Syntax des Umbenennens bleibt dabei gleich. big5 %&gt;% rename(Sex = Geschlecht) # A tibble: 200 x 7 Alter Sex Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # ... with 196 more rows Während beide Funktionen Spalten umbenennen können, gibt select() nur die ausgewählten Spalten und rename() hingegen alle Spalten zurück. Außerdem können Funktionen zur Umbenennung von Spalten verwendet werden. Dafür müssen wir rename_with() einfach nur die Funktion (ohne Klammern) übergeben. In diesem Beispiel werden alle Buchstaben der Spaltennamen in Großbuchstaben verändert. big5 %&gt;% rename_with(toupper) # A tibble: 200 x 7 ALTER GESCHLECHT EXTRAVERSION NEUROTIZISMUS O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # ... with 196 more rows Gerade bei sehr großen Datensätzen mit vielen Spalten ist die Funktion relocate() äußerst nützlich. Eine neue Spalte wird zum Beispiel immer ans Ende des Datensatzes angefügt. Um diese trotzdem betrachten zu können, übergeben wir den Spaltennamen einfach unserer Funktion. big5 %&gt;% relocate(O1) # A tibble: 200 x 7 O1 Alter Geschlecht Extraversion Neurotizismus O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 36 m 3 1.9 1 5 2 5 30 f 3.1 3.4 3 5 3 3 23 m 3.4 2.4 3 5 4 2 54 m 3.3 4.2 5 3 # ... with 196 more rows Wenn die Spalte nicht direkt am Anfang, sondern nach einer bestimmten anderen Spalte eingeordnet werden soll, können wir dies mit dem .after Argument festlegen. Hier würde die Spalte O1 hinter der Spalte Alter ausgegeben werden. big5 %&gt;% relocate(O1, .after = Alter) Auch hier können wir wieder Helferfunktionen wie where() verwenden. Man könnte beispielsweise alle numerischen Spalten hinter alle Character Spalten anfügen. big5 %&gt;% relocate(where(is.numeric), .after = where(is.character)) Eine weitere nützliche Funktion bei sehr breiten Datensätzen mit vielen Spalten ist names(). So können wir auf einem Blick alle Spaltennamen ausgegeben bekommen. big5 %&gt;% names() [1] &quot;Alter&quot; &quot;Geschlecht&quot; &quot;Extraversion&quot; &quot;Neurotizismus&quot; &quot;O1&quot; [6] &quot;O2&quot; &quot;O3&quot; Von Zeilennamen (rownames()) sollte hingegen grundsätzlich Abstand genommen werden. Falls der Datensatz Zeilennamen enthält, die tatsächlich von Bedeutung sind, sollte man diese mit der Funktion rownames_to_column(\"Spaltenname\") aus dem tibble Package in eine eigene Spalte befördern. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.3 filter() und arrange() Anders als im vorherigen Kapitel beschäftigen sich diese beiden Funktionen mit der Auswahl und Umordnung von Zeilen. Der Funktion filter() muss dabei ein logischer Ausdruck übergeben werden. Das Ergebnis der Abfrage muss also immer TRUE oder FALSE zurückgeben können (siehe Kapitel 4.2). Zur Auswahl aller männlichen Probanden würde man Geschlecht == \"m\" schreiben. Beachte an dieser Stelle das doppelte Gleichheitszeichen. big5 %&gt;% filter(Geschlecht == &quot;m&quot;) # A tibble: 82 x 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 23 m 3.4 2.4 3 3 5 3 54 m 3.3 4.2 2 5 3 4 32 m 3.5 3.1 3 1 5 # ... with 78 more rows Um Zeilen neu anzuordnen, benutzt man arrange(). Wenn die Zeilen nach aufsteigendem Alter sortiert werden sollen, muss man lediglich den Spaltennamen übergeben. big5 %&gt;% arrange(Alter) Für eine absteigende Anordnung kann man sich die Funktion desc() zur Hilfe nehmen. big5 %&gt;% arrange(desc(Alter)) # A tibble: 200 x 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1995 f 2.5 3.7 4 1 4 2 1964 f 3.2 2.3 5 1 5 3 60 f 3 2.7 5 1 5 4 59 m 2.7 2.3 5 1 5 # ... with 196 more rows So sehen wir hier beispielsweise zwei falsch eingetragene Alterswerte. Hier haben zwei Probanden nicht das Alter sondern das jeweilige Geburtsjahr in den Datensatz eingetragen. Das müsste man vor einer Auswertung natürlich noch einsprechend korrigieren. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.4 mutate() und across() Die Funktion mutate() ist eine sehr divers einsetzbare Funktion zum Verändern bestehender oder Hinzufügen neuer Spalten. Dabei wird der neue oder bereits bestehende Spaltennamen auf die linke Seite des Gleichheitszeichens geschrieben. Auf der rechten Seite kann so ziemlich alles stehen, solange die Funktion eine Spalte zurückgibt, die genauso lang ist wie der Datensatz. Man könnte hier zum Beispiel nicht den Mittelwert berechnen, da dabei nur ein Wert zurückgeben werden würde. Mit der Funktion log() logarithmieren wir hingegen jeden einzelnen der Extraversionswerte, sodass wir 200 Werte erhalten  also genau so viele, wie wir Zeilen haben. big5 %&gt;% mutate(Extraversion_lg = log(Extraversion)) Innerhalb eines mutate() Aufrufes können auch gleich mehrere Spalten neu erstellt oder verändert werden. Die verschiedenen Spalten müssen dabei lediglich mit einem Komma getrennt werden. Hier berechnen wir beispielsweise die logarithmische mittlere Ausprägung von Extraversion und Neurotizismus. Der Abstand der öffnenden Klammer oben und schließenden Klammer unten ist aus funktioneller Sicht nicht relevant. Es ist allerdings im Sinne der Lesbarkeit bei vielen Argumenten sinnvoller, die Befehle auf mehrere Zeilen aufzuteilen. Um unser Ergebnis zu betrachten, ordnen wir noch unsere neuen mit lg endenden Spalten nach vorne an. big5 %&gt;% mutate( Extraversion_lg = log(Extraversion), Neurotizismus_lg = log(Neurotizismus) ) %&gt;% relocate(ends_with(&quot;lg&quot;)) # A tibble: 200 x 9 Extraversion_lg Neurotizismus_lg Alter Geschlecht Extraversion Neurotizismus O1 O2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.10 0.642 36 m 3 1.9 5 1 2 1.13 1.22 30 f 3.1 3.4 5 3 3 1.22 0.875 23 m 3.4 2.4 3 3 4 1.19 1.44 54 m 3.3 4.2 2 5 # ... with 196 more rows, and 1 more variable: O3 &lt;dbl&gt; Bei zwei Spalten ist das obige Beispiel eine praktische Möglichkeit zum Logarithmieren. Wenn du eine Funktion über mehrere Spalten anwenden möchten, solltest du hingegen zusätzlich auf across() (engl. für herüber) zurückgreifen. Wir wollen schließlich eine Funktion über mehrere Spalten anwenden. Die Auswahl der Spalten erfolgt dabei innerhalb von across() genau wie bei select() (siehe Kapitel 6.2). Man kann beispielsweise den Doppelpunkt zur Auswahl eines Bereichs verwenden. An dieser Stelle müssen wir einmal genau hingucken, wo welche Klammer aufhört und endet. Denn die Funktion log() ist noch immer innerhalb von across(). big5 %&gt;% mutate(across(Extraversion:Neurotizismus, log)) Dies wird noch etwas klarer, wenn wir einmal exemplarisch die Namen der jeweiligen Argumente von across() auftragen. Mit .cols wählen wir die Spalten aus und dem .fns Argument übergibt man die anzuwendende Funktion. Außerdem können wir an dieser Stelle noch das .names Argument verwenden. Wie Dir vielleicht bereits aufgefallen ist, werden sonst die bisherigen Werte der ausgewählten Spalten nur überschrieben und keine neuen erstellt. Mit .names können wir die neuen Spaltennamen festlegen. Beachte dabei die geschweiften Klammern um .cols innerhalb des .names Argumentes, die notwendig sind, um auf den jeweiligen Namen der Spalte zurückzugreifen. Diesem Spaltennamen wird dann der Suffix _lg angehangen. big5 %&gt;% mutate(across( .cols = Extraversion:Neurotizismus, .fns = log, .names = &quot;{.col}_lg&quot;) ) Die auf mehrere Spalten anzuwendende Funktion muss innerhalb von across() übergeben werden. Falls ein Fehler auftritt, ist dieser in der Regel auf falsche Positionierung der Klammern zurückzuführen. Der übersichtshalber lassen wir diese Argumente allerdings für einfachere Anwendungsfälle im Verlaufe des Buches weg. Ein weiterer Unterschied besteht in der manuellen Auswahl einzelner Spalten. Hier müssen wir die einzelnen Spalten im Gegensatz zur Anwendung bei select() innerhalb von c() schreiben. big5 %&gt;% mutate(across(c(Extraversion, Neurotizismus), log)) Auch hier können wir die bereits in Kapitel 6.2 besprochenen Helferfunktionen wie where() verwenden. big5 %&gt;% mutate(across(where(is.numeric), log)) # A tibble: 200 x 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.58 m 1.10 0.642 1.61 0 1.61 2 3.40 f 1.13 1.22 1.61 1.10 1.61 3 3.14 m 1.22 0.875 1.10 1.10 1.61 4 3.99 m 1.19 1.44 0.693 1.61 1.10 # ... with 196 more rows Wenn wir beispielsweise eine bestehende Spalte auf bestimmte Art und Weise verändern wollen, wenn eine Bedingung zutrifft, erreichen wir dies mit der Funktion if_else(). Wir haben in Kapitel 6.3 gesehen, dass zwei Probanden ihr Geburtsjahr anstelle des Alters in Jahren angegeben haben. Wenn unsere Bedingung (condition) zutrifft, also das Alter in Jahren größer als 120 ist, soll das Jahr der Erhebung (2020) Minus das Alter gerechnet werden. Ansonsten (else beziehungsweise false) wird nur das unveränderte Alter zurückgegeben. Anschließend überprüfen wir noch unsere Berechnung, indem wir das Alter wieder absteigend anordnen. big5 %&gt;% mutate(Alter = if_else( condition = Alter &gt; 120, true = 2020 - Alter, false = Alter) ) %&gt;% arrange(desc(Alter)) # A tibble: 200 x 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 60 f 3 2.7 5 1 5 2 59 m 2.7 2.3 5 1 5 3 59 m 2.8 1.6 4 2 5 4 58 m 2.3 2.9 5 1 4 # ... with 196 more rows Bei mehr als zwei Bedingungen können wir stattdessen die Funktion case_when() verwenden. Auf der linken Seite der Tilde (~) ist dabei immer die Bedingung angegeben. Auf der rechten Seite hingegen ist die Ausgabe, wenn die zugehörige Bedingung auf der linken Seite zutreffen sollte. Allen Werten, auf die keine der explizit genannten Bedingungen zutrifft, wird NA (Akronym für Not Available, engl. für nicht vorhanden) zugewiesen. Dies kann man anpassen, indem man am Ende noch TRUE als Bedingung hinzufügt. In diesem Beispiel müssen alle, die bisher keiner Bedingung zugeordnet sind, der ältesten Altersgruppe angehören. Am Ende ordnen wir unsere neu erstellte Spalte zum Kontrollieren unserer Berechnung noch nach vorne. big5 %&gt;% mutate(Gruppe = case_when( Alter &lt;= 25 ~ &quot;Jungspund&quot;, Alter &gt; 25 &amp; Alter &lt;= 45 ~ &quot;Mittel&quot;, between(Alter, 46, 65) ~ &quot;Erfahren&quot;, TRUE ~ &quot;Weise&quot;) ) %&gt;% relocate(Gruppe) # A tibble: 200 x 8 Gruppe Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Mittel 36 m 3 1.9 5 1 5 2 Mittel 30 f 3.1 3.4 5 3 5 3 Jungspund 23 m 3.4 2.4 3 3 5 4 Erfahren 54 m 3.3 4.2 2 5 3 # ... with 196 more rows Die Helferfunktion between() ist eine übersichtliche Alternative zum kombinierten logischen Begriff eine Zeile darüber. Wichtig ist an dieser Stelle, dass der Datentyp auf der linken Seite immer logisch sein muss. Auf der rechten Seite der Tilde muss es immer der gleiche Datentyp sein. Wenn wir also wie hier den Datentyp Character haben, muss bei allen diesen Zuweisungen auf der rechten Seite der Datentyp übereinstimmen. Ein weiterer praktischer Anwendungsfall ist die Umkodierung von von einzelnen Fragen. Angenommen, wir messen auf einer Skala von 1 (trifft gar nicht zu) bis 5 (trifft vollkommen zu) die Ausprägung der Offenheit für neue Erfahrungen. Um Verzerrungen zu vermeiden, sind in einem derartigen Fragebogen immer einige Items verneint gestellt. Normalerweise würde beispielsweise fragen, ob man gerne neue Sportarten ausprobiert. Würden wir allerdings fragen, ob man nicht gerne neue Sportarten ausprobiert, trifft unsere Skala natürlich nicht mehr zu. Jetzt wäre 5 (trifft gar nicht zu) und 1 (trifft vollkommen zu). Stellen wir uns vor, dies würde für das Item O1 zutreffen. Zum Vergleich erstellen wir eine neue Spalte namens O1_new, welche die umkodierten Werte enthält. big5 %&gt;% select(O1) %&gt;% mutate(O1_new = case_when( O1 == 1 ~ 5, O1 == 2 ~ 4, O1 == 3 ~ 3, O1 == 4 ~ 2, O1 == 5 ~ 1) ) # A tibble: 200 x 2 O1 O1_new &lt;dbl&gt; &lt;dbl&gt; 1 5 1 2 5 1 3 3 3 4 2 4 # ... with 196 more rows Immer wenn die Spalte O1 den Werte 1 hat, wird eine 5 daraus gemacht und immer wenn eine 2 angekreuzt wurde, diese mit einer 4 ersetzt. Die 3 können wir so belassen und die 4 und 5 wandeln wir auf die gleiche Art und Weise um. Beachte auch hier, dass wir auf der linken Seite immer eine logische Abfrage und rechts den selben Datentyp (hier Double) vorliegen haben. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.5 Eigene Funktionen Obwohl direkt in R oder in zusätzlichen Packages bereits eine Vielzahl von Funktionen enthalten sind, braucht man doch immer wieder eigene Funktionen für spezifische Anwendungsfälle. Dies versuchen wir anhand einer Funktion zu illustrieren, die den Logarithmus einer der Funktion übergebenen Zahl mit zwei summiert. Diese Funktion sei new_log() genannt. Eine Funktion wird mit function() erstellt. Innerhalb der runden Klammern können wir mit einem Komma getrennt beliebig viele Argumente festlegen. An dieser Stelle nehmen wir nur x. Der Name dieses Arguments ist grundsätzlich egal, solange er wie hier in dem Beispiel sowohl innerhalb von function() als auch in log() miteinander übereinstimmt. Die eigentliche Berechnung findet innerhalb der geschweiften Klammern statt. Es ist wichtig, dass wir einmal vor Benutzung diese Funktion durch Ausführen (strg + enter) lokal speichern. new_log &lt;- function(x) { log(x) + 2 } Eigene Funktionen müssen genau wie Packages nach Neustart von R immer wieder neu geladen werden. Dies erreicht man beispielsweise durch einfaches Ausführen des obigen Befehls. Es gibt in der Hinsicht also keinen Unterschied zum Speichern gewöhnlicher Variablen. Eine Funktion mit zwei Argumenten, wenn wir beispielsweise zusätzlich noch die Höhe der Zahl innerhalb der Funktion anpassen möchten, könnte wie folgt aussehen. new_log &lt;- function(x, zahl = 2) { log(x) + zahl } Nun könnte man als zweites Argument die zu addierende Zahl modifizieren. Innerhalb der runden Klammern steht zahl = 2, um den Standardwert von zahl zu definieren. Würden wir das Argument dann beim Anwenden weglassen, würde 2 addiert werden. Nur wenn man die Zahl verändern wollen würde, müsste man das der Funktion new_log() das Argument zahl übergeben. Erst einmal erstellt, können die eigenen Funktionen nun wie im vorherigen Kapitel bereits gelernt direkt in mutate() angewendet werden. big5 %&gt;% mutate(across(Extraversion:Neurotizismus, new_log)) # A tibble: 200 x 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3.10 2.64 5 1 5 2 30 f 3.13 3.22 5 3 5 3 23 m 3.22 2.88 3 3 5 4 54 m 3.19 3.44 2 5 3 # ... with 196 more rows Eine besondere Art, diese eigenen Funktionen zu definieren, sind sogenannte Lambda Funktionen. Diese sind anonyme Funktionen, die keinen Funktionsnamen erhalten und daher auch nur an der definierten Stelle verwendet werden können. Dabei sind zwei Sachen hervorzuheben. Auf der einen Seite muss man immer eine Tilde (~) führend hinzufügen. Auf der anderen Seite ändert sich auch der Name des Arguments innerhalb von log(). Schließlich haben wir hier keine Namen der Argumente (im Beispiel oben waren das x und zahl) festgelegt. Das zu übergebende Argument ist an dieser Stelle die jeweilige Spalte des Datensatzes. Diese wird unabhängig vom Kontext mit .x festgelegt (beachte den führenden Punkt). big5 %&gt;% mutate(across(Extraversion:Neurotizismus, ~ log(.x) + 2)) Lambda Funktionen sind eine praktische Möglichkeit, schnell eigene wenig komplexe Funktionen zu erstellen, die man nur an einer Stelle benötigt. So spart man sich das eigenständige Erstellen einer neuen Funktion. Für komplexere Anwendungen ist jedoch das Erstellen einer eigenen Funktion mit function() {} der übersichtlichere und damit empfohlene Weg. Mithilfe der Lambda Funktionen könnten wir jetzt auf einen Schlag nicht nur ein Item umkodieren, sondern so viele wie wir wollen. Wir erinnern uns, ein Item könnten wir mithilfe von case_when() umkodieren. big5 %&gt;% select(O1) %&gt;% mutate(O1_new = case_when( O1 == 1 ~ 5, O1 == 2 ~ 4, O1 == 3 ~ 3, O1 == 4 ~ 2, O1 == 5 ~ 1) ) Möchten wir nun auf einen Schlag die Spalten o1, O4 und O6 umkodieren, könnten wir dies wie gewohnt mit across() erreichen. Was sich nun durch die Lambda Funktion ändern, ist zum einen die Tilde und zum anderen ersetzt unser Platzhalter Argument .x den Spaltennamen aus dem vorherigen Beispiel. big5 %&gt;% mutate(across(c(O1, O3), ~ case_when( .x == 1 ~ 5, .x == 2 ~ 4, .x == 3 ~ 3, .x == 4 ~ 2, .x == 5 ~ 1) )) # A tibble: 200 x 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 1 1 1 2 30 f 3.1 3.4 1 3 1 3 23 m 3.4 2.4 3 3 1 4 54 m 3.3 4.2 4 5 3 # ... with 196 more rows Beachte an dieser Stelle, dass die schließende Klammer von across() erst hinter dem vollständigen Funktionsaufruf von case_when() geschrieben werden muss. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.6 rowwise() und c_across() In Kapitel 6.4 haben wir bereits kennengelernt, wie man durch das Anwenden von Funktionen auf eine oder mehre Spalten eben diese verändert. Allerdings kann man mit dem bisherigen Wissen noch keine Berechnung in Abhängigkeit von den jeweiligen Zeilen durchführen. Dies ist allerdings gar kein seltener Anwendungsfall. Zum Beispiel möchte man die Mittelwerte bestimmter Spalten pro Person ausrechnen. Dafür gibt es die Funktionen rowwise() und c_across(), die miteinander kombiniert werden müssen. Wir möchten an dieser Stelle den Mittelwert pro Person für Offenheit berechnen. Dieser ergibt sich aus drei einzelnen Fragen zur Offenheit (O1, O2, O3). Zuerst müssen wir die Funktion rowwise() aufrufen. Schließlich müssen wir R erst einmal signalisieren, dass wir die folgende Funktion nun pro Zeile (oder zeilenweise) anwenden möchten. Innerhalb von mutate() müssen unsere drei Fragen zur Offenheit nun der Funktion c_across() übergeben werden. Beachte das Präfix c_ an dieser Stelle. Zur Kontrolle holen wir uns die neu erstellte Spalte namens Offenheit wieder an den Anfang des Datensatzes. big5 %&gt;% rowwise() %&gt;% mutate(Offenheit = mean(c_across(O1:O3))) %&gt;% relocate(Offenheit) # A tibble: 200 x 8 # Rowwise: Offenheit Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.67 36 m 3 1.9 5 1 5 2 4.33 30 f 3.1 3.4 5 3 5 3 3.67 23 m 3.4 2.4 3 3 5 4 3.33 54 m 3.3 4.2 2 5 3 # ... with 196 more rows Hinter die Spaltenauswahl mithilfe von c_across() können wir mit einem Komma getrennt wie gewohnt weitere Argumente der jeweiligen Funktion übergeben. Hier sei exemplarisch die Entfernung fehlender Werte mit na.rm = TRUE illustriert. big5 %&gt;% rowwise() %&gt;% mutate(Offenheit = mean(c_across(O1:O3), na.rm = TRUE)) Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.7 Breites und langes Datenformat Grundsätzlich kann man ein breites von einem sogenannten langen Datenformat unterscheiden. Im breiten Datensatz ist jede Spalte eine Variable, jede Zeile eine Beobachtung und jede Zelle ein Wert. Für die meisten Fälle ist das unser gewünschtes Datenformat. In Abbildung 6.2 ist ein einfaches Beispiel für einen breiten Datensatz mit drei Personen und zwei Variablen. Abbildung 6.2: Breites Datenformat mit drei Personen und drei Variablen. Für das Erstellen mehrfaktorieller Abbildungen und hierarchischer statistischer Modellierung benötigen wir allerdings das lange Datenformat. In Abbildung 6.3 ist das lange Äquivalent zum gerade besprochenen breiten Datenbeispiel. Abbildung 6.3: Langes Datenformat mit Persönlichkeitsfaktor als Innersubjektfaktor. Schauen wir uns nun an, wie wir dies intuitiv mithilfe des tidyr Packages erreichen können. Mit pivot_longer() (engl. für Drehpunkt länger) können wir einen breiten ins lange Datenformat bringen. Auf der anderen Seite nutzen wir pivot_wider() für die Transformation vom langen ins breite Datenformat. Erster Funktion findet deutlich häufiger Anwendung. Für das Umformatieren ins lange Datenformat ist es essentiell wichtig, einen eindeutigen Personenidentifikator im breiten Datensatz zu haben. Ansonsten wird es nicht funktionieren. Hier entscheiden wir uns einfach für die Zeilennummer, die wir mit der Funktion row_number() in die Spalte VPN (Akronym für Versuchspersonennummer) schreiben. wide_big5 &lt;- big5 %&gt;% mutate(VPN = row_number()) %&gt;% select(VPN, Geschlecht, Extraversion, Neurotizismus) wide_big5 # A tibble: 200 x 4 VPN Geschlecht Extraversion Neurotizismus &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 m 3 1.9 2 2 f 3.1 3.4 3 3 m 3.4 2.4 4 4 m 3.3 4.2 # ... with 196 more rows Nun müssen wir in der Funktion pivot_longer() nur noch die gewünschten Spalten auswählen. Im obigen Beispiel wären das Extraversion und Neurotizismus. Beachte, dass wie bei across() auch hier die Spalten bei einzelner Auswahl der Funktion innerhalb von c() übergeben werden müssen. Nun müssen wir noch definieren, wie unsere beiden neuen Spalten heißen sollen. Schließlich kommen in eine Spalte unsere Werte (values_to) und in eine andere Spalte die Spaltennamen (names_to), welche hier die Persönlichkeitsfaktoren sind. Hier entscheiden wir uns für die Namen \"Auspraegung\" und \"Faktor\". Die Namen müssen hier unbedingt in Anführungszeichen geschrieben werden, da die Spalten noch nicht existieren. Das Ergebnis speichern wir an dieser Stelle als long_big5 ab. long_big5 &lt;- wide_big5 %&gt;% pivot_longer( cols = c(Extraversion, Neurotizismus), values_to = &quot;Auspraegung&quot;, names_to = &quot;Faktor&quot; ) long_big5 # A tibble: 400 x 4 VPN Geschlecht Faktor Auspraegung &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 m Extraversion 3 2 1 m Neurotizismus 1.9 3 2 f Extraversion 3.1 4 2 f Neurotizismus 3.4 # ... with 396 more rows Zur Auswahl der Spalten können die selben Helferfunktionen verwendet werden wie in Kapitel 6.2 beschrieben (z.B. starts_with(), ends_with() oder everything()). Umgekehrt können wir mithilfe von pivot_wider() den Datensatz long_big5 wieder ins breite Datenformat bringen. Dafür müssen wir hier nur festlegen, aus welcher Spalte die Werte (values_from) und woher die Spaltennamen (names_from) kommen sollen. Hier benötigen wir keine Anführungszeichen, da die Spalten bereits in unserem Datensatz enthalten sind. long_big5 %&gt;% pivot_wider( values_from = Auspraegung, names_from = Faktor ) # A tibble: 200 x 4 VPN Geschlecht Extraversion Neurotizismus &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 m 3 1.9 2 2 f 3.1 3.4 3 3 m 3.4 2.4 4 4 m 3.3 4.2 # ... with 196 more rows Als grobe Daumenregel kann man sich merken, dass man nicht vorhandene Spalten mit Anführungszeichen übergeben muss. Auf bereits im Datensatz vorhandene Spalten kann man hingegen im Regelfall ohne Anführungszeichen zugreifen. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.8 Spalten trennen Mit der Funktion separate() können Spalten getrennt werden. Dies ist vor allem praktisch, wenn der Datensatz im langen Format vorliegt. Exemplarisch sei hier der im remp Package enthaltene Datensatz big5_zeit enthalten. big5_zeit # A tibble: 5 x 5 VPN Extrav_T1 Extrav_T2 NeurotFA NeurotFB &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 3.2 3.3 2.8 3.2 2 2 1.7 1.5 4.1 3.2 3 3 2.8 2.7 3.2 2.8 4 4 4.7 4.2 1.7 2.4 # ... with 1 more row Um das hier bestehende Problem klarer zu machen, wandeln wir diesen erst einmal in ein langes Datenformat um. zeit1 &lt;- big5_zeit %&gt;% select(-NeurotFA, -NeurotFB) %&gt;% pivot_longer( cols = Extrav_T1:Extrav_T2, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) zeit1 # A tibble: 10 x 3 VPN Faktor Auspraegung &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Extrav_T1 3.2 2 1 Extrav_T2 3.3 3 2 Extrav_T1 1.7 4 2 Extrav_T2 1.5 # ... with 6 more rows Nun sehen wir, dass die Spalte Faktor zwei Informationen enthält. Einmal den Persönlichkeitsfaktor (Extrav) und einmal den entsprechenden Messzeitpunkt (T1, T2). Nun können wir der Funktion separate() die zu trennende Spalte übergeben. Außerdem müssen wir noch spezifizieren, in welche verschiedene Spalten wir hier trennen müssen (hier Faktor und Zeitpunkt). Getrennt sind die beiden Informationen durch einen Unterstrich (_). Dies können wir durch das sep (Akronym für Separator) festlegen. Hätten wir in der Spalte Faktor mehr als zwei Informationen durch mehrere Unterstriche getrennt, müssten wir dem Argument into entsprechend drei Spaltennamen übergeben. zeit1 %&gt;% separate( col = Faktor, into = c(&quot;Faktor&quot;, &quot;Zeitpunkt&quot;), sep = &quot;_&quot; ) # A tibble: 10 x 4 VPN Faktor Zeitpunkt Auspraegung &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Extrav T1 3.2 2 1 Extrav T2 3.3 3 2 Extrav T1 1.7 4 2 Extrav T2 1.5 # ... with 6 more rows Falls die Spalten nicht eindeutig durch einen Unterstrich, sondern beispielsweise durch Großschreibung voneinander getrennt werden sollen, kann man mit sep auch die Trennstelle (hier 6) festlegen. big5_zeit %&gt;% select(-Extrav_T1, -Extrav_T2) %&gt;% pivot_longer( cols = NeurotFA:NeurotFB, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) %&gt;% separate( col = Faktor, into = c(&quot;Faktor&quot;, &quot;Zeitpunkt&quot;), sep = 6 ) 6.9 Datensätze zusammenführen Eine ziemlich häufige und dabei leider oft schwierige Herausforderung ist das Zusammenführen von Datensätzen. Dies hat man beispielsweise bei den meisten biologischen Messungen, in denen man pro Person viele Messwerte erhält. Meist müssten dann die einzelnen Datensätze der verschiedenen Personen aneinander gebunden werden. Eine Möglichkeit zur Lösung dieses speziellen Szenarios haben wir bereits in Kapitel 5.1 kennengelernt. Dort haben wir mehrere Datensätze direkt mit der Funktion zum Einlesen zeilenweise (row bind) aneinander gebunden. import_list(dateien, rbind = TRUE) Ein weiteres sehr häufiges Szenario ist das Zusammenfügen beim Erheben zu verschiedenen Messzeitpunkten. Mit potentiellen Problemen, die einem dabei begegnen können und deren Lösungen werden wir uns im restlichen Teil dieses Kapitels detailliert auseinandersetzen. Natürlich gibt es noch diverse andere Anwendungsfälle, in denen ein Zusammenführen mehrerer Datensätze gewünscht ist. Dieses Kapitel sollte das grundlegende Verständnis vermitteln, mit jeder zukünftigen Problematik dieses Kontextes fertig zu werden. Zum Verstehen der verschiedenen Funktionen verwenden wir die im remp Package enthaltenden Datensätze namens A, B, C, D und E. In A bis D wurde bei drei Personen zu zwei Messzeitpunkten (T1, T2) jeweils die Konzentration von Low Density Lipoprotein (LDL) gemessen. Dieses Lipoprotein ist hauptsächlich für den Transport von Cholesterin zur Leber verantwortlich. Ein erhöhter Spiegel erhöht maßgeblich das Risiko eine Herz-Kreislauf Erkrankungen. Zwischen den beiden Messzeitpunkten wurden Maßnahmen wie Sport, Ernährungsumstellung und Statine (Medikamente) angewandt, sodass sich die Konzentration am zweiten Messzeitpunkt verringert hat. Die Spaltennamen von A und B sind dieselben, während sich die von C und D unterscheiden. Datensatz E enthält Daten von Person Nummer 3 sowie zwei weiteren Personen mit der ID 4 und 6. Schauen wir uns kurz Datensatz A A # A tibble: 3 x 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 und Datensatz B an. B # A tibble: 3 x 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 156 T2 2 2 102 T2 3 3 111 T2 Wie zu Beginn des Kapitels beschrieben, könnten wir aufgrund derselben Spaltennamen die beiden Datensätze zeilenweise zusammenbinden. Wir verwenden hier allerdings nicht die in R enthaltene rbind() Funktion sondern bind_rows() aus dem dplyr Package (enthalten im tidyverse). Zum Aneinanderbinden müssen wir diese beiden Datensätze einfach nacheinander der Funktion übergeben. Es können pro Aufruf immer nur zwei Datensätze aneinander gebunden werden. bind_rows(A, B) # A tibble: 6 x 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 4 1 156 T2 # ... with 2 more rows Wir sehen, dass unser neue Datensatz nun 6 anstelle der vorherigen 3 Zeilen hat. Jede Person ist also jetzt doppelt enthalten. Mit der im Kapitel 6.7 kennengelernten Funktion pivot_wider() können wir dann den Datensatz wieder in ein breites Datenformat bringen. bind_rows(A, B) %&gt;% pivot_wider( values_from = LDL, names_from = Zeitpunkt, names_glue = &quot;LDL_{.name}&quot; ) # A tibble: 3 x 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 156 2 2 118 102 3 3 128 111 Das ist grundsätzlich eine sehr einfache und elegante Lösung. Wichtig hierbei ist die zwingende Notwendigkeit einer Spalte mit eindeutiger Zuordnung der Beobachtungen (zum Beispiel ID). Ansonsten können wir pivot_wider() nicht verwenden. Bei komplexeren Datensätzen stößt dieser Ansatz allerdings schnell an seine Grenzen. Sobald eine Zeile doppelt vorkommt, würde es beispielsweise schon nicht mehr funktionieren. Wenn die Spaltennamen nicht dieselben sind, können wir nicht mehr zeilenweise binden. Man könnte natürlich die Spalten in diesem trivialen Beispiel einfach umbenennen. Da man selten nur drei Spalten im gesamten Datensatz hat, nutzen wir diese Gelegenheit, um uns das spaltenweise Zusammenfügen anzuschauen. Zuerst werfen wir aber noch einen Blick in den Datensatz C C # A tibble: 3 x 3 ID LDL_T1 Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 und dann in D D # A tibble: 3 x 3 ID LDL_T2 Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 156 T2 2 2 102 T2 3 3 111 T2 Syntaktisch ändert sich im Vergleich zu vorher nichts. Nur das Suffix heißt nun anders. Wir verwenden also bind_cols(), indem wir einfach die beiden Datensätze als Argumente übergeben. df &lt;- bind_cols(C, D) New names: * ID -&gt; ID...1 * Zeitpunkt -&gt; Zeitpunkt...3 * ID -&gt; ID...4 * Zeitpunkt -&gt; Zeitpunkt...6 df # A tibble: 3 x 6 ID...1 LDL_T1 Zeitpunkt...3 ID...4 LDL_T2 Zeitpunkt...6 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 1 156 T2 2 2 118 T1 2 102 T2 3 3 128 T1 3 111 T2 Hier sehen wir einen Grund für die Benutzen der dplyr anstelle der Basisfunktion (z.B. cbind()) in Aktion. Die Funktion bind_cols() ändert automatisch doppelte Spaltennamen um. Schließlich liegen in beiden Datensätzen eine Spalte namens ID und Zeitpunkt vor. Jetzt kann man natürlich die Spalten der ID und beiden LDL Konzentrationen manuell auswählen (siehe Kapitel 6.2). df %&gt;% select(ID = `ID...1`, LDL_T1, LDL_T2) # A tibble: 3 x 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 156 2 2 118 102 3 3 128 111 Beachte an dieser Stelle die Notwendigkeit die Backticks vor und nach dem Spaltennamen ID1, da wir keinen normalen Namen vorliegen haben. Bei komplexeren Datensätzen ist dieses spaltenweise Zusammenfügen jedoch sehr fehleranfällig. Daher sind in dplyr auch noch sogenannte Joining Funktionen enthalten. Diese kommen aus der Welt der Datenbanken, beziehungsweise aus der dort verbreiteten Sprache SQL. Um das Prinzip des Joinings etwas genauer zu betrachten, nehmen wir uns an dieser Stelle noch den E Datensatz zur Hand. E # A tibble: 3 x 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 111 T2 2 4 88 T2 3 6 93 T2 Wenn wir diesen mit Datensatz A zusammenführen möchte, könnten wir dies wieder eine Kombination aus bind_rows() und pivot_wider() erreichen. bind_rows(A, E) %&gt;% pivot_wider( values_from = LDL, names_from = Zeitpunkt, names_glue = &quot;LDL_{.name}&quot; ) # A tibble: 5 x 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 NA 2 2 118 NA 3 3 128 111 4 4 NA 88 # ... with 1 more row Dies ist wie gesagt für komplexere Datensätze selten fehlerfrei machbar. Eine erste Alternative bietet daher die Funktion full_join(). Diese integriert alle Werte aus beiden Datensätze. full_join(A, E) Joining, by = c(&quot;ID&quot;, &quot;LDL&quot;, &quot;Zeitpunkt&quot;) # A tibble: 6 x 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 4 3 111 T2 # ... with 2 more rows Wir erhalten die Nachricht Joining, by = c(\"ID\", \"LDL\", \"Zeitpunkt\"), welche uns angibt, nach welchen Variablen gejoint wurde (hier alle). Wir können dies auch explizit mit dem by Argument spezifizieren. So kann man full_join() zwingen, nur die ID in die Zeilen zu schreiben und die restlichen Werte mit NAs (fehlenden Werten) aufzufüllen. full_join(A, E, by = &quot;ID&quot;) # A tibble: 5 x 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 NA &lt;NA&gt; 2 2 118 T1 NA &lt;NA&gt; 3 3 128 T1 111 T2 4 4 NA &lt;NA&gt; 88 T2 # ... with 1 more row Diese Ausgabe können wir noch etwas verschönern, indem wir das Suffix T1 und T2 anstelle von x und y verwenden. Außerdem wählen wir nur die ID und LDL Spalten aus und lassen uns alle fünf Zeilen ausgeben. full_join(A, E, by = &quot;ID&quot;, suffix = c(&quot;_T1&quot;, &quot;_T2&quot;)) %&gt;% select(ID, starts_with(&quot;LDL&quot;)) %&gt;% print(n = 5) # A tibble: 5 x 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 NA 2 2 118 NA 3 3 128 111 4 4 NA 88 5 6 NA 93 Wenn hingegen wie bei Datensatz A und B nur dieselben Personen zwei mal gefragt wurden, beide Datensätze also in dem Sinne vollständig sind, entstehen keine fehlenden Werte in Form von NA. full_join(A, B, by = &quot;ID&quot;) # A tibble: 3 x 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 156 T2 2 2 118 T1 102 T2 3 3 128 T1 111 T2 Neben dem full_join() gibt es noch andere Joins, die je nach Anwendungsfall angemessener sind. Der left_join() integriert nur die Werte vom zweiten (rechten) Datensatz in den ersten (linken) Datensatz, die im ersten (hier A) enthalten sind. left_join(A, E, by = &quot;ID&quot;) # A tibble: 3 x 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 NA &lt;NA&gt; 2 2 118 T1 NA &lt;NA&gt; 3 3 128 T1 111 T2 Möchte man nur die Werte aus dem ersten (linken) Datensatz A in den zweiten (rechten) Datensatz E integrieren, die im rechten Datensatz enthalten sind, kann die right_join() Funktion verwendet werden. Beachte, dass bei beiden Funktion hier explizit nach der ID zusammengeführt wird. Wenn wir also über das Vorhandensein von etwas in einem Datensatz reden, ist in Falle unserer Beispiele damit immer ID gemeint. right_join(A, E, by = &quot;ID&quot;) # A tibble: 3 x 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 128 T1 111 T2 2 4 NA &lt;NA&gt; 88 T2 3 6 NA &lt;NA&gt; 93 T2 Wenn wir nur diejenigen Werte integrieren möchten, die in beiden Datensätzen enthalten sind, verwenden wir inner_join(). Da hier nur die Person 3 in beiden Datensätzen vorhanden ist, wird auch nur diese vollständig zusammengeführt und ausgegeben. inner_join(A, E, by = &quot;ID&quot;) # A tibble: 1 x 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 128 T1 111 T2 Abschließend gibt es noch zwei Funktion, die nicht direkt zusammenführen, sondern nur eine Bedingung prüfen und davon abhängig den ersten (linken) Datensatz zurückgeben. Die Funktion semi_join() gibt nur jene Werte aus dem ersten Datensatz zurück, welche im ersten (linken) und zweiten (rechten) vorkommen. semi_join(A, E, by = &quot;ID&quot;) # A tibble: 1 x 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 128 T1 Die Funktion anti_join() hingegen gibt nur die Werte aus dem ersten (linken) Datensatz zurück, die nicht im zweiten (rechten) Datensatz enthalten sind. anti_join(A, E, by = &quot;ID&quot;) # A tibble: 2 x 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 Bei sehr einfachen Szenarien kann man auf die Kombination aus bind_rows() und pivot_wider() zurückgreifen. Die Datensätze spaltenweise mithilfe von bind_cols() zusammenzuführen ist hingegen immer eine fehleranfällige und daher riskante Idee. Die Lösung ist in den meisten komplexeren Fällen ein sogenannter Join. Welche Art von Join man im eigenen Anwendungsfall letzten Endes verwendet, hängt immer vom Kontext ab und kann nicht generell beantwortet werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.10 Character Manipulation Oft muss man entweder die Spaltennamen oder die Inhalte verschiedener Spalten, die Characters enthalten in irgendeiner Form anpassen. In diesem Kapitel schauen wir uns an, wie man mit Funktionen aus stringr (enthalten im tidyverse) Character ersetzt (str_replace()), extrahiert (str_extract()) und entdeckt (str_detect()). Das Präfix str steht dabei für String  einem anderen Wort für Character. Ein häufiges Ärgernis im Kontext von Programmiersprachen sind Umlaute, da verschiedene Zeichenkodierungen diese intern verschieden übersetzen, so dass auf anderen Betriebssystemen zu Kauderwelsch kommen kann. Schauen wir uns im Folgenden an, wie man Umlaute ersetzt. Es sei der Satz char &lt;- &quot;Österreich hat 28610 schräge Berge&quot; gegeben. Möchte man nun das eine ä mit ae ersetzen, verwendet man str_replace(). str_replace( string = char, pattern = &quot;ä&quot;, replacement = &quot;ae&quot; ) [1] &quot;Österreich hat 28610 schraege Berge&quot; Dabei muss der zu ersetzende Buchstabe immer zuerst eingegeben werden. Nun haben wir aber immer noch ein ö im Satz enthalten. Für mehr als eine Anpassung verwendet man str_replace_all(). Die Syntax ist leider etwas kontra intuitiv, wenn man bereits select() und rename() kennengelernt hat. Hier ist der alte Name auf der linken Seite der jeweiligen Gleichung. str_replace_all( string = char, c(&quot;ä&quot; = &quot;ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;) ) [1] &quot;Oesterreich hat 28610 schraege Berge&quot; Aber wie geht man vor, wenn Spaltennamen Umlaute enthalten? Um dem auf den Grund zu gehen, erstellen wir uns einen neuen tibble (12.3), der die Preise für drei verschiedene Sägen in Österreich enthält. umlaut &lt;- tibble( Säge = c(&quot;Häxler&quot;, &quot;Sünde3000&quot;, &quot;Lölf4&quot;), Österreich = c(10.45, 4.60, 9.70) ) umlaut # A tibble: 3 x 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 2 Sünde3000 4.6 3 Lölf4 9.7 Die Namen enthalten jeweils einen Umlaut. Beachte, dass ein Umlaut groß geschrieben ist und die Funktionen case sensitive sind. Das bedeutet, dass wir mit den Befehlen oben nur kleine Buchstaben ersetzen und nicht ihre großen Äquivalente. An dieser Stelle verwenden wir eine im Kapitel 6.5 bereits eingeführte so genannte Lambda Funktion. umlaut %&gt;% mutate(across(where(is.character), ~ str_replace_all( string = .x, c(&quot;ä&quot; = &quot;ae&quot;, &quot;ö&quot; = &quot;oe&quot;, &quot;ü&quot; = &quot;ue&quot;, &quot;Ä&quot; = &quot;Ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;, &quot;Ü&quot; = &quot;Ue&quot;) ))) # A tibble: 3 x 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Haexler 10.4 2 Suende3000 4.6 3 Loelf4 9.7 Möchten wir alle Spaltennamen von Umlauten befreien, könnten wir dies mithilfe von rename_with() und str_replace_all() erreichen. umlaut %&gt;% rename_with(~ str_replace_all( string = .x, c(&quot;ä&quot; = &quot;ae&quot;, &quot;ö&quot; = &quot;oe&quot;, &quot;ü&quot; = &quot;ue&quot;, &quot;Ä&quot; = &quot;Ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;, &quot;Ü&quot; = &quot;Ue&quot;) )) # A tibble: 3 x 2 Saege Oesterreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 2 Sünde3000 4.6 3 Lölf4 9.7 Für das Extrahieren von Buchstaben oder Zahlen können wir str_extract() verwenden. Wir nehmen wieder unseren Beispielsatz von oben, der als char gespeichert ist. Wir könnten beispielsweise die Zahl herausfiltern, indem wir einen sogenannten Regex verwenden (Akronym für Regular Expression). Diese sind grundsätzlich sehr komplex und benötigen zur vernünftigen Einführung ein eigenes Buch. In der Praxis muss man in der Regel nur Online nach dem gewünschten Regex suchen, ohne selbst jeden Kniff und jedes Detail zu kennen. Um eine Zahl mit mehr als einer Ziffer herauszuholen, könnte man nach \"\\\\d+\" suchen. Der erste Backslash ist nur nötig, weil wir mit R arbeiten. Danach folgt ein weiterer Backslash sowie ein d (für digit, engl. für Ziffer) und ein Plus zur Signalisierung, dass es sich hier auch um eine Zahl mit mehreren Ziffern handeln kann. str_extract(char, &quot;\\\\d+&quot;) [1] &quot;28610&quot; Die Funktion str_detect() entdeckt bestimmte Buchstaben, Wörter oder ganze Regex. Dabei gibt die Funktion einen logischen Wert aus (TRUE, FALSE), wenn das Gesuchte gefunden oder nicht gefunden wurde. Das ist daher praktisch, da man diese Funktion für logische Bedingungen innerhalb von if_else() oder case_when() verwenden kann. str_detect(char, &quot;\\\\d+&quot;) [1] TRUE In der Praxis müssen die Funktionen aus dem stringr Package in der Regel in Kombination mit mutate() verwendet werden. Einen weiteren Anwendungsfall stellt der Umbruch langer Achsenbeschriftung bei der Erstellung von Visualisierungen mithilfe von str_wrap() dar. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.11 Faktoren verändern Falls du nicht mehr genau im Kopf hast, was Faktoren sind, schaue Dir noch einmal Kapitel 4.4 an. Faktoren sind vor allem zur automatischen Erstellung von Dummy Variablen für Regressionsmodelle (siehe Kapitel 9.6.1), für das richtige Abbilden und das Ändern der Reihenfolge für das Erstellen von Visualisierungen (siehe Kapitel 8.6) sowie für andere inferenzstatistische Verfahren nützlich. Im Folgenden schauen wir uns Beispiele an, wie man mit einer Funktion aus dem forcats Package (mit dem tidyverse geladen) Faktoren umbenennen (fct_recode()) und deren Reihenfolge ändern (fct_relevel(), fct_reorder()) kann. Dafür verwenden wir die Spalte Gruppe aus dem big5_mod Datensatz mit den Faktorstufen (oder Leveln) Jungspund, Mittel, und Weise. big5_mod %&gt;% relocate(Gruppe) # A tibble: 200 x 6 Gruppe Alter Geschlecht Extraversion Neurotizismus ID &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 Mittel 36 m 3 1.9 1 2 Jung 30 f 3.1 3.4 2 3 Jung 23 m 3.4 2.4 3 4 Weise 54 m 3.3 4.2 4 # ... with 196 more rows Um die Veränderungen der Faktorstufen besser darstellen zu können, ziehen wir uns die Spalte Gruppe aus dem Datensatz heraus (siehe Kapitel 5.3). Zuvor müssen wir die Spalte allerdings noch zum Datentyp Faktor umwandeln. faktoren &lt;- big5_mod %&gt;% mutate(Gruppe = as.factor(Gruppe)) %&gt;% pull(Gruppe) Zum Anzeigen der Faktorstufen, verwenden wir levels faktoren %&gt;% levels() [1] &quot;Jung&quot; &quot;Mittel&quot; &quot;Weise&quot; Dass die Reihenfolge schon der Altersreihenfolge entspricht, liegt nur daran, dass wir die Faktorstufen oben genau spezifiziert haben. Ansonsten können durchaus unerwartete Reihenfolgen der Faktorstufen auftreten. Es lohnt sich also in jedem Fall ein Blick in die Faktorstufen, bevor man sie verwendet. Möchte man einzelne Faktoren umbenennen, verwendet man fct_recode(). faktoren %&gt;% fct_recode(Alt = &quot;Weise&quot;) %&gt;% levels() [1] &quot;Jung&quot; &quot;Mittel&quot; &quot;Alt&quot; Möchten wir eine bestimmte Faktorstufe an erster Stelle haben, verwenden wir fct_relevel() mit der gewünschten Stufe als Character. faktoren %&gt;% fct_relevel(&quot;Mittel&quot;) %&gt;% levels() [1] &quot;Mittel&quot; &quot;Jung&quot; &quot;Weise&quot; Zum Ändern der gesamten Reihenfolge kann man beliebig viele weitere Faktorstufen der Funktion übergeben. faktoren %&gt;% fct_relevel(&quot;Weise&quot;, &quot;Mittel&quot;, &quot;Jung&quot;) %&gt;% levels() [1] &quot;Weise&quot; &quot;Mittel&quot; &quot;Jung&quot; Wenn die Reihenfolge der Faktoren in absteigender (.desc = TRUE) oder aufsteigender (.desc = FALSE) Reihenfolge z.B. in Abhängigkeit des Mittelwertes einer anderen Spalte (wie dem Ausmaß an Extraversion) sortiert werden soll, verwendet man fct_reorder(). extraversion &lt;- big5_mod %&gt;% pull(Extraversion) faktoren %&gt;% fct_reorder(extraversion, .fun = mean, .desc = TRUE) %&gt;% levels() [1] &quot;Mittel&quot; &quot;Jung&quot; &quot;Weise&quot; Dabei kann man mit dem Argument .fun die gewünschte Funktion zur Auswertung der zweiten Variable (hier Extraversion) verwenden. In den am Anfang des Kapitels beschriebenen Anwendungsfällen wird es noch klarer werden, weshalb die Notwendigkeit von Faktoren besteht und inwiefern Dir die Funktionen aus dem forcats Package konkret das Leben erleichtern. big5_mod %&gt;% mutate(Gruppe = fct_relevel(Gruppe, &quot;Jung&quot;)) # A tibble: 200 x 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # ... with 196 more rows big5_mod %&gt;% mutate(Gruppe = fct_reorder( Gruppe, Alter, .fun = mean, .desc = FALSE) ) # A tibble: 200 x 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # ... with 196 more rows Faktoren sollten erst unmittelbar vor Verwendung erstellt und verändert werden. Es kann im Umgang von Faktoren zu seltsamen Fehlermeldungen kommen, da diese innerhalb von R als Integer und nicht als Character behandelt werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.12 Mit Zeitdaten arbeiten In Kapitel 4.5 wurden zwei verschiedene Zeitdatentypen bereits eingeführt. Wir schauen uns an dieser Stelle noch einige nützliche Funktionen aus dem lubridate Package an. Möchte man ein Datum auseinander nehmen, kann man die Funktionen year(), month() und day() verwenden. video %&gt;% mutate( Jahr = year(Watchdate), Monat = month(Watchdate), Tag = day(Watchdate) ) %&gt;% select(Watchdate, Tag, Monat, Jahr) # A tibble: 1,493 x 4 Watchdate Tag Monat Jahr &lt;dttm&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2019-07-22 00:00:00 22 7 2019 2 2019-07-22 00:00:00 22 7 2019 3 2019-07-21 00:00:00 21 7 2019 4 2019-07-21 00:00:00 21 7 2019 # ... with 1,489 more rows Neben der bereits vorgestellten ymd() Funktion, die aus dem Format Jahr-Monat-Tag einen Eintrag vom Typ Date erstellt date1 &lt;- &quot;02.08.2019&quot; dmy(date1) [1] &quot;2019-08-02&quot; gibt es auf selbe Art und Weise die Funktion dmy(), die mit Daten nach dem Format Tag-Monat-Jahr arbeiten kann. date2 &lt;- &quot;2019-08-02&quot; ymd(date2) [1] &quot;2019-08-02&quot; 6.13 Binäre Antwortmatrix erstellen Wenn man Daten im Rahmen von Fragebögen erhebt, kriegt man meist nicht zurück, ob ein Item (eine Aufgabe oder Frage) richtig oder falsch beantwortet wurde. Stattdessen werden nur die abgegeben Antworten im Datensatz gespeichert. Mit data_binary() aus dem remp Package kann man den Datensatz in die gewünschte binäre Antwortmatrix umwandeln. Sprich, man erhält die Information für jedes Item pro Person, ob das Item richtig (1) oder falsch (0) beantwortet wurde. Wenn wir einen Beispielsdatensatz mit drei Items und vier Antwortoptionen haben, die vier Personen beantwortet haben, df2 &lt;- big5 %&gt;% select(O1:O3) df2 # A tibble: 200 x 3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 1 5 2 5 3 5 3 3 3 5 4 2 5 3 # ... with 196 more rows bei Item 1 die Antwort 3 richtig ist, bei Item 2 die Antwort 2 und bei Item 3 die Antwort 4, würde man die Antworten in einer Variable speichern. df2 %&gt;% data_binary(answers = c(4, 1, 5)) # A tibble: 200 x 3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 1 1 2 0 0 1 3 0 0 1 4 0 0 0 # ... with 196 more rows Nun benutzen wir die Funktion konsistent zu den bisher gelernten in Kombination mit der Pipe. big5 %&gt;% select(Geschlecht, O1:O3) %&gt;% data_binary(answers = tibble(&quot;f&quot;, 4, 1, 5)) # A tibble: 200 x 4 Geschlecht O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 0 1 1 2 1 0 0 1 3 0 0 0 1 4 0 0 0 0 # ... with 196 more rows "],["descr.html", "Kapitel 7 Deskriptive Statistik 7.1 Verschiedene Lagemaße 7.2 Umgang mit fehlenden Werten 7.3 Häufigkeiten und Kontingenztafeln", " Kapitel 7 Deskriptive Statistik 7.1 Verschiedene Lagemaße Deskriptive Statistiken sind wichtig, um einen ersten Überblick über die Daten zu erhalten. Wir werden an dieser Stelle den big5_mod Datensatz verwenden. big5_mod # A tibble: 200 x 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # ... with 196 more rows Für die Berechnung von Mittelwert (mean()) und Standardabweichung (sd()) verwenden wir die im tidyverse enthaltene Funktion summarise(). Genauer gesagt stammt die Funktion, wie die meisten aus Kapitel 6, aus dem dplyr Package. Daher ist die Anwendung im Prinzip die selbe. Auf der linken Seite des Gleichheitszeichens stehen auch hier wieder die Namen der neu erstellten Spalten. big5_mod %&gt;% summarise( mean = mean(Extraversion), sd = sd(Extraversion) ) # A tibble: 1 x 2 mean sd &lt;dbl&gt; &lt;dbl&gt; 1 3.08 0.347 Zum Gruppieren der Variablen wird group_by() verwendet. big5_mod %&gt;% group_by(Geschlecht) %&gt;% summarise( mean = mean(Extraversion), sd = sd(Extraversion) ) # A tibble: 2 x 3 Geschlecht mean sd * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 f 3.05 0.358 2 m 3.11 0.328 Dabei können beliebig viele Spalten der Funktion übergeben werden. So könnte man beispielsweise nicht nur nach Geschlecht, sondern auch nach der Altersgruppe gruppieren. big5_mod %&gt;% group_by(Geschlecht, Gruppe) %&gt;% summarise( mean = mean(Extraversion), sd = sd(Extraversion) ) # A tibble: 6 x 4 # Groups: Geschlecht [2] Geschlecht Gruppe mean sd &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 f Jung 3.07 0.373 2 f Mittel 3.07 0.299 3 f Weise 2.83 0.269 4 m Jung 3.12 0.324 # ... with 2 more rows Neben dem Mittelwert und der Standardabweichung gibt es noch diverse weitere Funktionen: n() für die Anzahl an Beobachtungen, min() und max() für Minimum und Maximum, var() für die Varianz, sqrt() für die Quadratwurzel, median() für den Median und quantile() zur Berechnung der jeweiligen Quantile. Für die Berechnung des Standardfehlers teilen wir direkt innerhalb des Funktionsaufrufes die Standardabweichung durch die Wurzel aus der Anzahl der Personen. big5_mod %&gt;% summarise( N = n(), Min = min(Alter), Mean = mean(Alter), Median = median(Alter), Max = max(Alter), SD = sd(Alter), SE = SD / sqrt(N) ) Grundsätzlich kann jede Funktion summarise() übergeben werden, die einen einzelnen Wert berechnet. Somit unterscheidet sich die Anwendung maßgeblich vom bereits kennengelernten mutate(). Dort musste die Ausgabe immer eine Reihe von Werten umfassen, die der Anzahl der Zeilen im Datensatz entspricht. Auch hier können wir mehrere Spalten gleichzeitig mithilfe von across() auswerten (siehe Kapitel 6.4). Die Syntax ändert sich in dem Fall im Vergleich zu vorher. Hier müssen wir die verschiedenen Funktionen mit entsprechendem Namen innerhalb einer Liste übergeben. Diese werden erst später eingeführt und müssen uns an dieser Stelle nicht weiter interessieren. big5_mod %&gt;% summarise(across( .cols = Extraversion:Neurotizismus, .fns = list(mean = mean, sd = sd)) ) # A tibble: 1 x 4 Extraversion_mean Extraversion_sd Neurotizismus_mean Neurotizismus_sd &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.08 0.347 3.13 0.682 Bei vielen Lagemaßen kann es schon einmal recht viel zu schreiben sein. Wenn man keine Lust hat, dies jedes Mal manuell abzutippen, kann man auch direkt vereinfachte Funktionen zur deskriptiven Statistik verwenden. Das remp Package bietet die Funktion descriptive() an. Dieser muss man kein weiteres Argument übergeben. Es wird dann die Anzahl, das Minimum, das erste Quartil, der Mittelwert, der Median, das zweite Quartil, die Standardabweichung und der Standardfehler für sämtliche numerische Spalten zurückgegeben. Alle anderen Datentypen werden von dieser Funktion ignoriert. big5_mod %&gt;% descriptive() # A tibble: 4 x 10 Variable N Min Q1 Mean Median Q3 Max SD SE * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Alter 200 13 18.8 26.5 23 31.2 60 11.4 0.8 2 Extraversion 200 2.3 2.8 3.08 3 3.3 4.3 0.35 0.02 3 ID 200 1 50.8 100. 100. 150. 200 57.9 4.09 4 Neurotizismus 200 1.4 2.7 3.13 3.1 3.6 4.6 0.68 0.05 Auch hier können wir die Berechnungen auf die selbe Art und Weise gruppieren. big5_mod %&gt;% group_by(Geschlecht) %&gt;% descriptive() Alternativen für einen schnellen Überblick bieten beispielsweise auch das skimr Package mit der Funktion skim() oder describe() aus dem psych Package. Beide können auch nicht-numerische Spalten auswerten und erstere gibt zu jeder Spalte sogar ein kleines Histogramm aus. Wie Dir bereits vielleicht aufgefallen ist, sieht die Ausgabe von descriptive() anders aus als die von summarise(). Während erstere die Variablen untereinander in unterschiedliche Zeilen übersichtlich auflistet, fügt summarise() die Ergebnisse spaltenweise hinzu. Wenn wir den selben Output wie in descriptive() erreichen möchten, müssen wir zuerst den Datensatz in ein langes Format bringen (siehe Kapitel 6.7). Nun gruppieren wir nach der neuen Spalte namens Variable. Anschließend können wir wie gewohnt mit summarise() die deskriptiven Statistiken berechnen. Nichts anderes macht die Funktion descriptive() intern. big5_mod %&gt;% pivot_longer( cols = c(Alter, Extraversion, Neurotizismus), names_to = &quot;Variable&quot;, values_to = &quot;Wert&quot; ) %&gt;% group_by(Variable) %&gt;% summarise( Q1 = quantile(Wert, 0.25), Mean = mean(Wert), Q3 = quantile(Wert, 0.75), Schiefe = skewness(Wert), Kurtosis = kurtosis(Wert) ) # A tibble: 3 x 6 Variable Q1 Mean Q3 Schiefe Kurtosis * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Alter 18.8 26.5 31.2 1.34 3.97 2 Extraversion 2.8 3.08 3.3 0.761 3.95 3 Neurotizismus 2.7 3.13 3.6 -0.132 2.56 Beachte an dieser Stelle, dass für die externen Funktionen skewness() und kurtosis() das moments Package installiert und geladen sein muss. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 7.2 Umgang mit fehlenden Werten Zur besseren Illustration der verschiedenen Möglichkeiten verwenden wir an dieser Stelle einen kleinen selbst erstellten Datensatz namens df. df &lt;- tibble( Alter = c(34, NA, 45, 999), Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, NA), Extraversion = c(4, 3, 999, 2) ) Enthalten sind zum einen fehlende Werte als NA und zum anderen als 999 kodiert. NA ist dabei ein besonderer Datentyp, der für Not Available (engl. für nicht verfügbar) steht. Die Kodierung als 999 ist typisch für SPSS Nutzer, da dort kein extra Datentyp für fehlende Werte vorhanden ist. Wir sind also daran interessiert, diese 999 oder andere nicht passende Werte in NAs sowie umgekehrt NAs in bestimmte Zahlen umzuwandeln. Zum Umwandeln von Werten in NA können wir die Funktion na_if() aus dem dplyr (tidyverse) Package verwenden. Die Syntax ist dabei denkbar intuitiv. Wenn beispielsweise in der Spalte Alter die Zahl 999 vorkommt, soll stattdessen NA geschrieben werden. Das ganze müssen wir natürlich innerhalb von mutate() verwenden (siehe Kapitel 6.4). df %&gt;% mutate(Alter = na_if(Alter, 999)) # A tibble: 4 x 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f 999 4 NA &lt;NA&gt; 2 Das selbe können wir natürlich auch gleich auf mehrere Spalten anwenden. df %&gt;% mutate(across(Alter:Geschlecht, ~ na_if(.x, 999))) Alternativ kann mit der Helferfunktion everything() auch der ganze Datensatz ausgewählt werden. df %&gt;% mutate(across(everything(), ~ na_if(.x, 999))) # A tibble: 4 x 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f NA 4 NA &lt;NA&gt; 2 Zur Umwandlung von NAs in beispielsweise die Zahl 999, können wir replace_na() aus selbigen Package benutzen. Wenn in der Spalte Alter ein NA steht, soll dieses mit 999 ersetzt werden. df %&gt;% mutate(Alter = replace_na(Alter, 999)) # A tibble: 4 x 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 999 f 3 3 45 f 999 4 999 &lt;NA&gt; 2 Auch hier können wir selbstverständlich die Funktion auf alle Spalten anwenden. df %&gt;% mutate(across(everything(), ~ replace_na(.x, 999))) # A tibble: 4 x 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 999 f 3 3 45 f 999 4 999 999 2 Eine Besonderheit stellt die Zuweisung von NAs innerhalb von if_else() oder case_when() dar. So können NAs erstellt werden, wenn eine bestimmte Bedingung zutrifft. Wir haben bereits in Kapitel 6.4 gelernt, dass die Datentypen auf der rechten Seite des Gleichheitszeichens beziehungsweise auf der rechten Seite der Tilde übereinstimmen müssen. Für die unterschiedlichen Datentypen gibt es jeweils einen eigenen NA Typ. Numerisch: NA_real_ Character: NA_char_ Logisch: NA df %&gt;% mutate(Alter = if_else( condition = Alter &gt; 120, true = NA_real_, false = Alter) ) # A tibble: 4 x 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f 999 4 NA &lt;NA&gt; 2 Im vorherigen Kapitel haben wir bereits summarise() mit den verschiedenen Funktionen der deskriptiven Lagemaße kennengelernt. Diese haben im Regelfall ein Argument namens na.rm (Akronym für not available remove), welches die fehlenden Werte der entsprechenden Spalte direkt entfernt. Genauer gesagt verwenden diese Funktionen an dieser Stelle die sogenannten Pairwise complete observations. big5_mod %&gt;% summarise( Min = min(Alter, na.rm = TRUE), Mean = mean(Alter, na.rm = TRUE) ) Eine weitere Möglichkeit ist das Entfernen von Zeilen, die fehlende Werte enthalten. Dies erreichen wir mit drop_na() aus dem dplyr Package. Allerdings entfernt diese Funktion alle Zeilen mit NAs. Wenn du also zwei Spalten auswerten möchtest und in einer dritten für den Moment irrelevanten Spalte ist ein fehlender Wert, würde die entsprechende Zeile trotzdem entfernt werden. Hier ist also Vorsicht geboten, um keine Informationen zu verlieren. df %&gt;% drop_na() # A tibble: 2 x 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 45 f 999 Alternativ können wir mithilfe von filter() und der logischen Abfrage is.na() das Entfernen von NAs auch auf eine bestimmte Spalte begrenzen (hier Geschlecht). df %&gt;% filter(!is.na(Geschlecht)) # A tibble: 3 x 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f 999 Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 7.3 Häufigkeiten und Kontingenztafeln Eine Möglichkeit an Häufigkeiten zu kommen, haben wir mit n() innerhalb von summarise() bereits kennengelernt. Eine alternative Möglichkeit ist die count() Funktion aus selbigem Package. Auch diese können wir einfach mit group_by() kombinieren. big5_mod %&gt;% group_by(Geschlecht, Gruppe) %&gt;% count() # A tibble: 6 x 3 # Groups: Geschlecht, Gruppe [6] Geschlecht Gruppe n &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 f Jung 89 2 f Mittel 20 3 f Weise 9 4 m Jung 58 # ... with 2 more rows Für die Erstellung von Kontingenztafeln benötigen, die wir mit statistischen Tests auswerten können, benötigen wir allerdings eine andere Funktion namens table(). Dieser müssen wir für eine klassischen Vierfeldertafel zwei Argumente in Form von einzelnen Spalten (oder Vektoren) übergeben. Wir müssen die Spalten also wie in Kapitel 5.3 bereits besprochen aus dem Datensatz extrahieren. extraversion &lt;- big5_mod %&gt;% pull(Extraversion) sex &lt;- big5_mod %&gt;% pull(Geschlecht) Nun können wir uns beispielsweise ausgeben lassen, wie häufig eine mittlere Ausprägung von größer 3 bei Männern und Frauen vorkommt. table(extraversion &gt; 3, sex) sex f m FALSE 66 43 TRUE 52 39 Natürlich könnte man auch die alternative Schreibweise mit dem Dollar-Operator verwenden. table(big5_mod$Extraversion &gt; 3, big5_mod$Geschlecht) Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). "],["visual.html", "Kapitel 8 Visualisierungen 8.1 Einführung 8.2 Histogramm und Dichte 8.3 Streudiagramm 8.4 Boxplot 8.5 Violin Plot 8.6 Balkendiagramm 8.7 Liniendiagramm 8.8 Odds Ratios 8.9 Q-Q Plot 8.10 Ellipse 8.11 Mehrfaktorielle Abbildungen 8.12 Anordnen mehrerer Graphen 8.13 Anpassen des Aussehens 8.14 Speichern von Graphen 8.15 Anwendungsbeispiel 8.16 Exemplarische Erweiterungen", " Kapitel 8 Visualisierungen 8.1 Einführung Für die sämtliche Visualisierungen werden wir das ggplot Package aus dem tidyverse verwenden. ggplot steht dabei für grammar of graphics. Genau wie in der Datenvorbereitung geht es also auch hier darum, das zugrundeliegende Prinzip einmal zu verstehen, um es dann auf verschiedenste Abbildungstypen anzuwenden. Wir werden wie bei der deskriptiven Statistik (siehe Kapitel 7) auch hier mit der leicht modifizierten Variante des Big Five Datensatzes arbeiten. big5_mod # A tibble: 200 x 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # ... with 196 more rows In Abbildung 8.1 sind die grundsätzlichen Komponenten links und je ein entsprechendes Beispiel rechts abgebildet. Jeder ggplot besteht aus verschiedenen Layern, die untereinander gelegt werden. Um eine Abbildung zu erstellen, muss man auf jeden Fall Data, Aestetics und Geometries verwenden. Die Layer namens Scales und Theme sind optional und passen lediglich das Erscheinungsbild an. Wir werden uns in Kapitel 8.2 bis 8.10 die jeweiligen Data, Aestetics und Geometries anschauen. Erst in Kapitel 8.13 werden die Anpassungsmöglichkeiten mithilfe der Scales und Theme Layer umfangreich eingeführt. Abbildung 8.1: Vereinfachte Anordnung der Layer im Rahmen der Grammar of Graphics mit Beispielen. Es ist wichtig zu verstehen, dass die Layer untereinander gelegt werden und man am Ende von unten auf die kreierte Abbildung schaut. Wenn man beispielsweise mehrere Geometries hintereinander in einen Plot einbaut, kann der zuletzt hinzugefügte den zuvor hinzugefügten (teilweise) überdecken. Bleiben wir bei dem Beispiel der Erstellung eines Histogramms aus Abbildung 8.1. ggplot(data = big5_mod, mapping = aes(x = Extraversion)) + geom_histogram() Innerhalb der Funktion ggplot() wird dem data Argument der Datensatz big5_mod übergeben. Aus diesem Datensatz möchten wir die Spalte Extraversion auf der x-Achse abgebildet haben. Die ersten beiden Layer sind somit bereits implementiert. Der Aestetics Layer wird durch das mapping Argument hinzugefügt. Allerdings können wir die Spalte nicht einfach mit x = Extraversion hinzufügen, sondern benötigen die zusätzliche Funktion aes(), der wir unsere Spalte übergeben. Dabei steht aes() für aesthetics (engl. für Ästhetik). Neben der Spalte, die auf der x-Achse abgebildet werden soll, kann auf die selbe Art und Weise die y-Achse definiert werden. Auch Argumente zur Veränderung des Aussehens wie color (Außenfarbe) oder fill (Füllfarbe) können hier innerhalb von aes() der Funktion ggplot übergeben werden. Nun haben wir oben bereits gesehen, dass das Histogramm erst mit der Funktion geom_histogram() hinzugefügt wird. Das Präfix geom ist dabei für jeden Geometry Layer der selbe. Für ein einfaches Histogramm muss an dieser Stelle nichts weiteres getan werden. Wichtig ist an dieser Stelle noch das +, welches sämtliche Layer zusammenbindet und untereinander hinzufügt. Anders als in Kapitel 6 wird hier also nicht die Pipe (%&gt;%), sondern ein Plus-Zeichen verwendet. Anders als sonst können Funktionen aus ggplot2 nicht mit einer Pipe aneinander gebunden werden. Das hat ausschließlich historische Gründe, da zu der Zeit der Erstellung von ggplot2 die Pipe noch nicht existiert hat. Dies wird sich in Zukunft auch nicht mehr ändern. Schauen wir uns die einzelnen Befehle einmal genauer an. Die erste Zeile kreiert erst einmal nur den Plot an sich und kein Histogramm. Die beiden Argumente data und mapping schreiben wir von nun an nicht mehr extra dazu, da wir die Reihenfolge dieser Argumente im Verlaufe des Buches ändern. ggplot(big5_mod, aes(x = Extraversion)) Wie zuvor kurz erwähnt, beginnt die Funktion zur eigentlichen Visualisierung der Daten mit geom_ und endet mit dem Namen der Abbildungsart (hier Histogramm). ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram() Das standardmäßige Aussehen mit dem grauen Hintergrund ist ein ggplot in der Form nicht publikationsreif. In Kapitel 8.13 werden diverse Anpassungsmöglichkeiten dieses flexiblen und sehr umfangreichen Packages erläutert werden. Zuvor werden wir allerdings erst einmal die üblichsten Abbildungsarten nacheinander Schritt für Schritt durchgehen. 8.2 Histogramm und Dichte Das Histogramm wurde exemplarisch bereits in der Einführung beschrieben. Wir erinnern uns, in dem Fall bedarf es nur der Zuweisung der Variable für die x-Achse, da auf der y-Achse die Häufigkeitsverteilung dargestellt wird. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram() Für ein schlichteres Aussehen fügen wir in Abbildung 8.2 (b) noch eine schwarze Rahmenfarbe mit dem color Argument und eine weiße Füllungsfarbe mit fill hinzu. Histogramme sind maßgeblich von der gewählten Breite der Balken abhängig. Bei zu wenigen Balken können Informationen der Verteilung verloren gehen, bei zu vielen hingegen irrelevante Trends erscheinen. Dieses kann entweder direkt mit der Anzahl der Balken (bins Argument) oder mit der Breite (binwidth Argument) verändert werden. Wir werden hier binwidth verwenden, da man dieser auch eine Funktion übergeben kann. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = 0.2 ) Abbildung 8.2: Histogramme Es gibt verschiedene Arten, eine möglichst optimale binwidth herauszufinden. Exemplarisch sei hier die Freedman-Diaconis Regel gezeigt. Wie bereits in Kapitel 6.5 eingeführt, erstellen wir dafür eine Funktion namens opt_bin(). opt_bin &lt;- function(x) { (max(x) - min(x)) / nclass.FD(x) } Diese neu erstellte Funktion muss man erst einmal speichern, in dem man den obigen Befehl ausführt. Nun können wir opt_bin direkt in geom_histogram() einfügen (siehe Abbildung 8.3 (a)). ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = opt_bin ) Eine weitere Möglichkeit, die Verteilung der Extraversion in unserer Population darzustellen, ist die Wahrscheinlichkeitsdichte. Dazu müssen wir lediglich den Suffix des Geometry Layers zu density (engl. für Dichte) verändern (siehe Abbildung 8.3 (b)). ggplot(big5_mod, aes(x = Extraversion)) + geom_density() Möchten wir nun das Histogramm gemeinsam mit der Wahrscheinlichkeitsdichte abbilden, müssen wir erst einmal beide auf die selbe Skala bringen. Hier möchten wir exemplarisch die Häufigkeiten des Histogramms ebenfalls als Dichte ausgedrückt haben. Dafür bedarf es eines kleinen Tricks. Dazu muss der Histogrammfunktion ebenfalls mit aes() ein Wert für die y-Achse übergeben werden. Mit ..density.. wird die Dichte berechnet. Die beiden Punkte vor und hinter der Dichte signalisieren der Funktion, dass etwas berechnet werden soll. Anschließend muss lediglich mit einem weiteren Plus-Zeichen die Dichtefunktion hinzugefügt werden. Für eine ansprechendere optische Darstellung fügen wir noch eine graue Füllfarbe hinzu und machen diese mit alpha leicht durchsichtig. Das Ergebnis kann in Abbildung 8.3 (c) betrachtet werden. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( mapping = aes(y = ..density..), binwidth = 0.2, color = &quot;black&quot;, fill = &quot;white&quot; ) + geom_density( fill = &quot;grey&quot;, alpha = 0.7 ) Abbildung 8.3: Histogramm und Dichte Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.3 Streudiagramm Um ein Streudiagramm zu erstellen, müssen wir neben der x-Achse nun ebenfalls die y-Achse festlegen. Ansonsten ändert sich nur der Geometry Layer zu geom_point(). Exemplarisch sei hier die mittlere Extraversion gegen das Lebensalter aufgetragen (siehe Abbildung 8.4 (a)). Auf weitere Parameter wie das Ändern der Farbe (color) oder Form (shape) verzichten wir an dieser Stelle. ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point() Nun sieht dieses Streudiagramm etwas seltsam aus, weil die zugrunde liegenden Daten ordinal und nicht metrisch skaliert waren. Die Extraversion wurde mit einem Fragebogen ermittelt und daraus dann der Mittelwert gebildet. Dadurch ist natürlich kein richtiges metrisches Skalenniveau gegeben, weswegen die Punkte hier in Reih und Glied erscheinen. Bei größeren Datensätzen kann es passieren, dass die Punkte sich in einem derartigen Szenario überlappen. Um das zu verhindern, kann die Position zu jitter verändert werden. Dies bewirkt eine leichte zufällige Variation jedes Datenpunktes (siehe Abbildung 8.4 (b)). ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) Mit geom_smooth() kann man eine am besten passende Linie durch die Punkte ziehen. Wir entscheiden uns an der Stelle für eine lineare Regressionsgrade (method = lm). Für alternative Methoden sei auf die Dokumentation der Funktion verwiesen. Außerdem färben wir die Grade schwarz und fügen ein 95% Konfidenzintervall mit se = TRUE hinzu. Damit der Regressionsgrade nicht nur in dem Bereich, in dem Daten beobachtet wurden, abgebildet wird, kann zusätzlich das fullrange Argument auf TRUE gesetzt werden. Um den Effekt dieser Funktion zu illustrieren, greifen wir an dieser Stelle etwas voraus und definieren mit xlim(c(2, 5)) die untere Grenze der x-Achse mit 2 und die obere mit 5 (siehe Abbildung 8.4 (c)). ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) + geom_smooth( color = &quot;black&quot;, method = lm, se = TRUE, fullrange = TRUE ) + xlim(c(2, 5)) Abbildung 8.4: Streudiagramm und Regressionsgrade Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.4 Boxplot Würde man nur einen Boxplot für die mittlere Ausprägung von Extraversion erstellen wollen, könnte man dies genau wie in den beiden zuvor besprochenen Kapiteln nur durch das Auswechseln des Geomitry Layers mit geom_boxplot() erreichen. Dies ist allerdings eine seltene Situation. Meistens möchte man auf der x-Achse mehrere Variablen miteinander vergleichen. Wir möchten an dieser Stelle zum Beispiel Extraversion und Neurotizismus miteinander vergleichen. Um dies zu erreichen, müssen wir den Datensatz vom breiten ins lange Datenformat transformieren. Wenn Dir das lange Datenformat kein Begriff ist, schaue Dir noch einmal das Kapitel 6.7 genauer an. Dort wird die hier verwendete Funktion pivot_longer() ausführlich eingeführt. big5_long &lt;- big5_mod %&gt;% pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) big5_long # A tibble: 400 x 6 Alter Geschlecht Gruppe ID Faktor Auspraegung &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 36 m Mittel 1 Extraversion 3 2 36 m Mittel 1 Neurotizismus 1.9 3 30 f Jung 2 Extraversion 3.1 4 30 f Jung 2 Neurotizismus 3.4 # ... with 396 more rows Nun sind unsere Persönlichkeitsfaktoren in der Spalte Faktor und die Werte aus den Spalten in Auspraegung. Einen Boxplot erstellt man mit geom_boxplot(). Auf der x-Achse möchten wir die Persönlichkeitsfaktoren und auf der y-Achse die mittleren Ausprägungen darstellen (siehe Abbildung 8.5 (a)). ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_boxplot() Um zusätzlich Fehlerbalken zu erhalten müssen wir diese mit stat_boxplot() berechnen und ausgeben lassen. Als Argument muss das geom Argument auf \"errorbar\" (engl. für Fehlerbalken) gesetzt werden. Die Breite des Fehlerbalkens kann durch das optionale Argument width kontrolliert werden. Zum Ausblenden der Ausreißer setzt man innerhalb von geom_boxplot() die outlier.shape auf NA (Akronym für Not Available). Das Ergebnis ist in Abbildung 8.5 (b) illustriert. ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + stat_boxplot(geom = &quot;errorbar&quot;, width = 0.4) + geom_boxplot(outlier.shape = NA) Abbildung 8.5: Boxplots Wichtig ist dabei die Reihenfolge der Funktionsaufrufe. Wie bereits erwähnt, werden die verschiedenen Layer untereinander gezeichnet. Würden wir also zunächst geom_boxplot() und erst anschließend stat_boxplot() zum ggplot hinzufügen, würde die Linie des Fehlerbalkens über den Boxplot gezeichnet werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.5 Violin Plot Für die Violin Plots nutzen wir, genau wie zuvor bei den Boxplots (siehe Kapitel 8.4), den Datensatz im langen Datenformat. Dieser heißt big5_long und wurde mithilfe von pivot_longer() erstellt. Es ändert sich nichts außer das Geometry Layer, welches nun geom_violin() heißt (siehe Abbildung 8.6 (a)). ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_violin() Optional kann zusätzlich das Argument trim auf FALSE gesetzt werden, um das Abschneiden der Enden des Violin Plots zu verhindern. Mit dem Argument können wir explizit Quantile (hier 25%, 50%, 75% ) einzeichnen lassen. Das Ergebnis ist in Abbildung 8.6 (b) zu sehen. ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75) ) Abbildung 8.6: Violin Plots Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.6 Balkendiagramm Bei Balkendiagrammen möchte man in der Regel mehrere Merkmale miteinander vergleichen. Deswegen müssen wir, genau wie auch bereits bei den Boxplots und Violin Plots, den Datensatz erst in ein langes Format bringen. In Kapitel 6.7 wird erläutert, wie das im Detail funktioniert. big5_long &lt;- big5_mod %&gt;% pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) Nun möchte man in Balkendiagrammen häufig Mittelwerte oder andere konkrete Einzelwerte miteinander vergleichen. Deswegen müssen wir vor der graphischen Darstellung erst die Mittelwerte berechnen. Zusätzlich berechnen wir die Standardabweichung zur späteren Erstellung der Fehlerbalken. Wie das funktioniert, wurde bereits in Kapitel 7 eingeführt. big5_means &lt;- big5_long %&gt;% group_by(Faktor) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means # A tibble: 2 x 3 Faktor Mean SD * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion 3.08 0.347 2 Neurotizismus 3.13 0.682 Nun kann man mit der Funktion geom_col() genau diese Mittelwerte abbilden. Auf der x-Achse sind demnach wie zuvor auch die Persönlichkeitsfaktoren und auf der y-Achse die Mittelwerte aufgetragen (siehe Abbildung 8.7 (a)). ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col() Verwechsle geom_col() nicht mit geom_bar(). Die erste Funktion stellt genau das dar, was man ihr übergibt (z.B. Mittelwerte). Letztere Funktion hingegen erstellt Balken mit einer Höhe, die proportional zur Anzahl der Fälle in der jeder Gruppe ist. Dies findet in der Wissenschaft eher seltener Anwendung, weswegen man wahrscheinlich meist mit geom_col() besser bedient ist. Um das Balkendiagramm zu verschönern, können wir auch hier die Füllfarbe (fill) und die Rahmenfarbe (color) entsprechend anpassen. Zusätzlich bilden wir mit geom_errorbar() die Standardabweichung (SD) ab, indem wir das Minimum des Fehlerbalken als Mittelwert minus Standardabweichung und das Maximum als Mittelwert plus Standardabweichung festlegen. Außerdem kann die Breite der Fehlerbalken mit width verändert werden. Beachte an dieser Stelle, dass die Grenzen der Fehlerbalken (ymin und ymax) im Gegensatz zur Fehlerbalkenbreite innerhalb der Funktion aes() definiert werden müssen. Das Ergebnis ist in Abbildung 8.7 (b) dargestellt. Mehrfaktorielle Balkendiagramme werden erst in Kapitel 8.11 eingeführt. ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col(fill = &quot;white&quot;, color = &quot;black&quot;) + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4) Abbildung 8.7: Balkendiagramme Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.7 Liniendiagramm Bei Mittelwertvergleichen in Form von Liniendiagrammen ändert sich im Vergleich zu den Balkendiagrammen nur wenig. Auch hier verwenden wir wieder den Datensatz big5_means, der unsere Mittelwerte und Standardabweichungen für Extraversion und Neurotizismus enthält. Zum Erstellen der Verbindungslinie zwischen den beiden Persönlichkeitsfaktoren muss das group Argument auf 1 gesetzt werden. Nun müssen wir noch die Linie mit geom_line(), die Mittelwerte als Punkte mit geom_point() und die Fehlerbalken mit geom_errorbar() erstellen. Auch hier ändert sich nichts im Vergleich zu den Balkendiagrammen im vorherigen Kapitel. Das Ergebnis ist in Abbildung 8.8 (a) illustriert. ggplot(big5_means, aes(x = Faktor, y = Mean, group = 1)) + geom_line() + geom_point() + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2) Ein weiteres klassisches Beispiel eines Liniendiagramms ist die Abbildung von Zeitreihen. Dafür schauen wir uns den Kurs der Bitcoin Aktie an, welcher im remp Package enthalten ist. bitcoin # A tibble: 731 x 2 Date Price &lt;date&gt; &lt;dbl&gt; 1 2019-01-01 3844. 2 2019-01-02 3943. 3 2019-01-03 3837. 4 2019-01-04 3858. # ... with 727 more rows Auf der x-Achse soll das Datum stehen und auf der y-Achse der Preis bei geschlossener Börse in USD. Die Zeitreihe wird wie zuvor mit geom_line() visualisiert. Wichtig ist hierbei, dass das Datum vom Datentyp date ist (siehe Kapitel 6.12). Zusätzlich können wir, wie beim Streudiagramm in Kapitel 8.3, mit stat_smooth() eine am besten passendste Kurve zur Kursbeschreibung hinzufügen. Abschließend greifen wir an dieser Stelle etwas vor und verändern noch die Benennung der x-Achse mithilfe von scale_x_date(). Dabei gibt es verschiedene Möglichkeiten der Anzeige, die jeweils mit einem Prozentzeichen angeführt werden müssen. Hier zeigen wir den abgekürzten Monatsnamen (%b) und das entsprechende Jahr (%Y). Das Ergebnis ist in Abbildung 8.8 (b) illustriert. ggplot(bitcoin, aes(x = Date, y = Price)) + geom_line() + stat_smooth(color = &quot;black&quot;) + scale_x_date(date_labels = &quot;%b %Y&quot;) Eine weitere Anwendung finden Liniendiagramme bei Scree Plots zur Auswahl der Anzahl der Faktoren für explorative Faktorenanalysen (siehe Kapitel 10.2). Dafür benötigen wir den kompletten Big Five Rohdatensatz mit den einzelnen Fragen zu den Persönlichkeitsfaktoren namens big_five_comp. Mit der im remp enthaltenen Funktion data_eigen() können praktisch die entsprechenden Eigenvalues berechnet werden, die wir im Scree Plot abbilden wollen. big5_scree &lt;- big_five_comp %&gt;% select(-Geschlecht) %&gt;% data_eigen() big5_scree # A tibble: 51 x 2 Eigenvalues Dimension &lt;dbl&gt; &lt;int&gt; 1 8.25 1 2 4.59 2 3 3.62 3 4 3.57 4 # ... with 47 more rows Es ändert sich im Prinzip nichts im Vergleich zum vorherigen Beispiel. Auf der x-Achse haben wir unsere verschiedenen Dimensionen und auf der y-Achse die Eigenvalues. geom_point() und geom_line() modifizieren wir selbsterklärend zusätzlich optisch leicht. Neu ist an dieser Stelle die Funktion geom_hline() (für horizontal line), welche eine horizontale Linie beim Schnittpunkt mit der y-Achse von 1 einzeichnet (siehe Abbildung 8.8 (b)). ggplot(big5_scree, aes(x = Dimension, y = Eigenvalues)) + geom_point(shape = 19, size = 2) + geom_line(size = 0.6) + geom_hline( aes(yintercept = 1), size = 0.8, linetype = &quot;longdash&quot; ) Abbildung 8.8: Abbildung von Liniendiagrammen als (a) Mittelwertsvergleich (b) Zeitreihe und (c) Scree Plot Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.8 Odds Ratios Der Datensatz odds_ratios ist im remp Package enthalten. Es wurde überprüft, ob es einen Unterschied in der Häufigkeit höher ausgeprägter Persönlichkeitsfaktoren zwischen den Geschlechtern gibt. Wie man selbst einen derartigen Datensatz erstellen kann, wird in Kapitel 9.8.1 im Rahmen des Fisher-Exakt Tests eingeführt. odds_ratios # A tibble: 3 x 4 Faktor OR conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion 1.15 0.628 2.11 2 Neurotizismus 0.448 0.241 0.824 3 Vertraeglichkeit 0.962 0.499 1.87 Jetzt ändert sich eine entscheidende Sache im Vergleich zu den Abbildungen zuvor. Unsere diskrete Variable der Persönlichkeitsfaktoren mit den Merkmalsausprägungen Extraversion, Neurotizismus und Vertraeglichkeit wird nach Konvention bei der Illustration von Odds Ratios (OR) auf der y-Achse abgebildet. Wir vertauschen als das x und y Argument innerhalb von aes() im Mapping Layer. Dadurch ändert sich auch die Funktion für die Fehlerbalken zu geom_errorbarh() (horizontal). Auch möchten wir zusätzlich mit geom_vline() eine vertikale Linie bei OR = 1 darstellen. Beachte hier die geänderten Argumente xmin, xmax und xintercept im Vergleich zu den bereits kennengelernten Äquivalenten der y-Achse. Das Ergebnis ist in Abbildung 8.9 abgebildet. ggplot(odds_ratios, aes(x = OR, y = Faktor)) + geom_point() + geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.1) + geom_vline(xintercept = 1) Abbildung 8.9: Odds Ratios Die beiden Achsen einfach innerhalb von aes() zu vertauschen, ist erst seit ggplot 3.3.0 möglich. Vorher musste man ein normales Liniendiagramm erstellen und dieses mit coord_flip() zum Schluss umdrehen. Achte also darauf, eine aktuelle Version des Packages installiert zu haben. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.9 Q-Q Plot Um mithilfe eines Q-Q Plots die Quantile zweier Verteilungen zu überprüfen, können wir geom_qq() und geom_qq_line() verwenden. Ein häufiger Anwendungsfall ist die graphische Überprüfung einer Variable auf Normalverteilung. Daher ist dies auch die Standardeinstellung innerhalb der Funktionen. Die interessierende Variable (hier Alter) muss dem sample Argument übergeben werden (siehe Abbildung 8.10). ggplot(big5_mod, aes(sample = Alter)) + geom_qq() + geom_qq_line() Abbildung 8.10: Q-Q Plots Möchte man die Verteilung einer Spalte mit einer anderen Verteilung vergleichen, können mit dem distribution Argument die Quantile einer anderen Verteilung wie qbinom (Binomialverteilung) oder qt (t-Verteilung) ohne Anführungszeichen festgelegt werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.10 Ellipse Das Zeichnen von Ellipsen kann beispielsweise für das Visualisieren mehrerer Cluster (siehe Kapitel 10.3) praktisch sein. An dieser Stelle tragen wir jedoch nur Extraversion und Alter gegeneinander auf. Neu ist die Funktion stat_ellipse(). Das Präfix stat steht für statistical transformation und ist hinter den Kulissen die Basis für alle die bisher gelernten Funktionen. Für alle häufiger verwendeten Funktionen wurde allerdings zusätzlich eine geom_*() Funktion geschrieben, die sich zumeist nur durch die Wahl von für die meisten sinnvollen Standardwerten ausmacht. Ansonsten besteht zwischen stat und geom Funktion kein bedeutender Unterschied. Die Ellipse ist in Abbildung 8.11 illustriert. ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) + stat_ellipse() Abbildung 8.11: Ellipse Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.11 Mehrfaktorielle Abbildungen In den bisherigen Kapiteln haben wir bislang nur einfaktorielle Abbildungen besprochen. Wir haben also einfach eine Variable auf der x-Achse gegen eine Variable auf der y-Achse aufgetragen. Was aber, wenn man nach mehreren Faktoren gruppieren möchte? Zum Beispiel könnte man die Farbe der Balkendiagramme je nach Geschlecht verändern. Da wir nun mehr als einen Faktor haben, sprechen wir von zweifaktoriell. Die Anzahl der Ausprägungsgrade, also ob zwei oder fünf Geschlechter erhoben hat, spielt dabei keine Rolle. Diese zweifaktoriellen Abbildungen können mit selbsterklärenden Gruppierungselementen wie color, fill und linetype implementiert werden. Wenn man drei- oder vierfaktorielle Abbildungen kreieren möchte, muss man auf Facets zurückgreifen. Im Sinne der Barrierefreiheit bezüglich Farbenblindheit, stellen wir die Standardfarben auf die Viridis Farbenpalette um, die genau für diesen Zweck kreiert wurden. Der Suffix d steht für discrete und c für continouus. Diese vier Befehle stellen global (also für die gesamte R Session) die Standardfarben um. scale_fill_discrete &lt;- scale_fill_viridis_d scale_fill_continuous &lt;- scale_fill_viridis_c scale_colour_discrete &lt;- scale_color_viridis_d scale_colour_continuous &lt;- scale_color_viridis_c Dabei handelt es sich um dieselbe Farbpalette wie in diesem Buch. Weitere Informationen über die Anpassung der Farbpaletten finden sich in Kapitel 8.13. 8.11.1 Gruppierungsargumente Gruppierungsargumente können grundsätzlich erst einmal fast alles innerhalb von aes() sein. Je nach gewünschter Abbildung kann color, fill, linetype aber auch size und shape beispielsweise verwendet werden. Im Vergleich zu vorher übergeben wir diesen Argumenten nun kein Charakter (wie \"black\"), sondern das gruppierende Argument (z.B. Geschlecht). Zusätzlich benötigen wir dann nur noch ein position Argument, welches spezifiziert, wie die Gruppen zueinander stehen. Eine häufige Wahl hierfür ist die Funktion position_dodge(0.95) (engl. für ausweichen), welches die Gruppen direkt nebeneinander darstellt. Die Zahl in der Klammer steht für den genauen Abstand zwischen den Balken oder Linien (je nach Funktion). Alternativ könnte man auch position = \"stack\" für eine aufeinander gestapelte Ansicht pro Kategorie verwenden. Wenn man stattdessen position = \"fill\" verwendet, werden diese übereinander gestapelten Anteile auf 1 standardisiert, sodass man die Verhältnisse besser vergleichen kann. Beim Erstellen eines gruppierten Balkendiagramms, müssen wir nun nur noch den Datensatz mit einem zusätzlichen Faktor (hier Geschlecht) mit group_by() gruppieren. big5_means2 &lt;- big5_long %&gt;% group_by(Faktor, Geschlecht) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means2 # A tibble: 4 x 4 # Groups: Faktor [2] Faktor Geschlecht Mean SD &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f 3.05 0.358 2 Extraversion m 3.11 0.328 3 Neurotizismus f 3.25 0.633 4 Neurotizismus m 2.96 0.718 Jetzt setzen wir noch fill = Geschlecht für die Füllfarbe und fügen entsprechend wie bereits besprochen die Positionsargumente jeder Funktion hinzu (siehe Abbildung 8.12). ggplot(big5_means2, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95)) + geom_errorbar( aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) Abbildung 8.12: Zweifaktorielle Balkendiagramme Ein anderes Beispiel wären gruppierte Liniendiagramme. Grundsätzlich ändern wir an dieser Stelle nur fill zu linetype und müssen zusätzlich das Gruppenargument Geschlecht den einzelnen Funktionen übergeben, da sonst keine Linien zwischen den Gruppen gezeichnet werden würden (siehe Abbildung 8.13). ggplot(big5_means2, aes(x = Faktor, y = Mean, linetype = Geschlecht)) + geom_line( aes(group = Geschlecht), position = position_dodge(0.2) ) + geom_point(position = position_dodge(0.2)) + geom_errorbar( aes(group = Geschlecht, ymin = Mean - SD, ymax = Mean + SD), width = 0.2, position = position_dodge(0.2) ) Abbildung 8.13: Zweifaktorielle Liniendiagramme Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.11.2 Facets Bei dreifaktoriellen Abbildungen müssen wir erst einmal einen weiteren Faktor (hier Gruppe) der Funktion group_by() übergeben. big5_means3 &lt;- big5_long %&gt;% group_by(Faktor, Geschlecht, Gruppe) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means3 # A tibble: 12 x 5 # Groups: Faktor, Geschlecht [4] Faktor Geschlecht Gruppe Mean SD &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f Jung 3.07 0.373 2 Extraversion f Mittel 3.07 0.299 3 Extraversion f Weise 2.83 0.269 4 Extraversion m Jung 3.12 0.324 # ... with 8 more rows Zum Darstellen unsere drei Altersgruppen können wir nun mit facet_wrap() auswählen, ob die Gruppen zeilenweise (facet_wrap(Gruppe ~)) oder wie in dem Beispiel spaltenweise (facet_wrap(~ Gruppe)) dargestellt werden sollen (siehe Abbildung 8.14). ggplot(big5_means3, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95)) + geom_errorbar( aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) + facet_wrap(~ Gruppe) Abbildung 8.14: Dreifaktorielle Abbildung. Das Prinzip bleibt auch bei vierfaktoriellen Abbildungen das gleiche. Wir erfinden dafür noch eine zusätzliche Spalte mit zwei Messzeitpunkten. Die Funktion rep() (repeat, engl. für wiederholen) wiederholt dabei die beiden Messzeitpunkte T1 und T2 jeweils 200 mal, da wir 400 Zeilen in unserem Datensatz haben. Das ist natürlich in der Praxis nicht notwendig, wenn wir bereits eine Spalte haben, nach der wir zusätzlich gruppieren möchten. Ansonsten fügen wir wie zuvor diesen weiteren Faktor zu group_by() hinzu. big5_means4 &lt;- big5_long %&gt;% mutate(Zeitpunkt = rep(c(&quot;T1&quot;, &quot;T2&quot;), each = 200)) %&gt;% group_by(Faktor, Geschlecht, Gruppe, Zeitpunkt) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means4 # A tibble: 24 x 6 # Groups: Faktor, Geschlecht, Gruppe [12] Faktor Geschlecht Gruppe Zeitpunkt Mean SD &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f Jung T1 3.08 0.391 2 Extraversion f Jung T2 3.06 0.362 3 Extraversion f Mittel T1 2.93 0.121 4 Extraversion f Mittel T2 3.13 0.336 # ... with 20 more rows Anschließend müssen wir nur noch facet_wrap() zu facet_grid() tauschen, da letztere Funktion sowohl zeilenweise (links der Tilde) als auch spaltenweise (rechts der Tilde) zeitgleich gruppieren kann (siehe Abbildung 8.15). ggplot(big5_means4, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95)) + geom_errorbar( aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) + facet_grid(Zeitpunkt ~ Gruppe) Abbildung 8.15: Vierfaktorielle Abbildungen Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.12 Anordnen mehrerer Graphen Nicht selten möchte man mehrere Graphen innerhalb einer Abbildung darstellen, die optimalerweise auch noch beschriftet sind. Dafür verwenden wir das Package patchwork. library(patchwork) Zuerst müssen wir die Abbildungen in Variablen abspeichern. Exemplarisch nutzen wir an dieser Stelle das Histogramm, das Streudiagramm, das Balkendiagramm und die Q-Q Plots aus den vorherigen Kapiteln und speichern diese jeweils als a, b, c und d. a &lt;- ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = 0.2 ) b &lt;- ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) c &lt;- ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col(fill = &quot;white&quot;, color = &quot;black&quot;) + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4) d &lt;- ggplot(big5_mod, aes(sample = Alter)) + geom_qq() + geom_qq_line() Nun müssen wir die verschiedenen Graphen lediglich in der gewünschten Reihenfolge addieren. Die Funktion plot_layout() spezifiziert unter anderem die Anzahl der Spalten (ncol) und plot_annotation() ergänzt mit dem tag_levels Argument Beschriftungen zu jeder Abbildung (siehe Abbildung 8.16). a + b + c + d + plot_layout(ncol = 2) + plot_annotation(tag_levels = &quot;A&quot;) Abbildung 8.16: Anordnung mehrerer Graphen Neben \"A\" kann der Funktion plot_annotation() außerdem \"i\", \"I\", \"a\" und \"1\" übergeben werden. Die tag_levels können durch tag_prefix und tag_suffix weiter an die eigenen Bedürfnisse angepasst werden. Außerdem können Abbildungen auch in unterschiedlicher Anzahl neben- und untereinander gesetzt werden. Dafür muss man lediglich die oben stehenden Abbildungen mit vertikalen Linien unterteilen und diese dann durch die Abbildung, die unten stehen soll, teilen (siehe Abbildung 8.17). (a | b | c) / d + plot_annotation( tag_levels = &quot;I&quot;, tag_prefix = &#39;Abb. &#39;, tag_suffix = &#39;:&#39; ) Abbildung 8.17: Alternative Anordnung mehrerer Graphen Für ein komplexeres Beispiel mit verschiedenen Verhältnissen zwischen den Abbildungen sei auf das Kapitel 8.15 verwiesen. 8.13 Anpassen des Aussehens Wir werden uns anhand der Abbildung aus Kapitel 8.11.1 die verschiedenen Anpassungsmöglichkeiten anschauen. big5_means2 &lt;- big5_long %&gt;% group_by(Faktor, Geschlecht) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) Bleiben wir zunächst beim Datensatz. Wenn wir die Bezeichnungen innerhalb unserer Legende (also der Spalte Geschlecht) umbenennen wollen, nutzen wir fct_recode(). Zum Verändern der Reihenfolge kann fct_relevel() verwendet werden. Die Funktionen zur Veränderungen von Faktoren wurden bereits in Kapitel 6.11 eingeführt. big5_means2 &lt;- big5_means2 %&gt;% mutate( Geschlecht = fct_recode( Geschlecht, &quot;weiblich&quot; = &quot;f&quot;, &quot;männlich&quot; = &quot;m&quot;), Geschlecht = fct_relevel(Geschlecht, &quot;männlich&quot;) ) Nun erstellen wir zunächst die Abbildung und speichern diese als plot. Grundsätzlich könnten wir alle Anpassungen auch einfach mithilfe einer sehr langen Aneinanderkettung von Befehlen erledigen. Der übersichtshalber speichern wir an dieser Stelle Zwischenergebnisse immer wieder ab. plot &lt;- ggplot(big5_means2, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95)) + geom_errorbar( aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) Das Farbschema kann durch verschiedene Farbpaletten adaptiert werden. Häufig sind im wissenschaftlichen Kontext jedoch nur Graustufen erwünscht, welche wir mit scale_fill_grey() erstellen können. Wichtig ist an dieser Stelle das start und end Argument der Farbpalette, da die Grautöne sonst zu dunkel für das Erkennen von Fehlerbalken sind. Hätten wir in unserem ggplot nicht die Füllfarbe (fill), sondern die Rahmenfarbe (color) verändert, würde man entsprechend scale_color_grey() verwenden. plot &lt;- plot + scale_fill_grey(start = 0.4, end = 0.7) Alternativ könnten wir wie in Kapitel 8.11 bereits erwähnt, eine Farbpalette manuell einstellen, die auch bei Farbenblindheit eine Differenzierung der Farben erlaubt. Dafür würde man beispielsweise scale_fill_viridis_d(start = 0.27, end = 0.72, opt = \"C\") benutzen. Das ist die Farbpalette, die in diesem Buch verwendet wird. Die Option bezieht sich damit auf eine der fünf in Viridis enthaltenen Farbpaletten. Für genaue Informationen über die einzelnen Farbpaletten sei auf die offizielle Dokumentation verwiesen. Auch der graue Standardhintergrund und die fehlende Visualisierung der Achsen ist in der Regel in der Wissenschaft nicht erwünscht. Ein gutes minimales Thema ist theme_classic(), welchem wir die grundlegende Größe aller Textelemente übergeben können. plot &lt;- plot + theme_classic(base_size = 15) Genauere Anpassungen des Themas sind mit der theme Funktion möglich. Hier können wir jedes kleinste Detail manuell anpassen. Hervorzuheben ist an dieser Stelle die zwingende Notwendigkeit der element_text() Funktion, der die Argumente wie size übergeben werden müssen. Nützlich ist bei langen Achsenbeschriftungen auch angle (engl. für Winkel) in Kombination mit hjust (Akronym für horizontal adjustment). Strip steht für die Überschriften bei den Facets für mehrfaktorielle Abbildungen (siehe Kapitel 8.11.2). Mithilfe von element_blanck() könnte man den Hintergrund der Überschriftsboxen entfernen. Mit legend.position kann man die Koordinaten der Legende festlegen, mit legend.direction die Ausrichtung. Wenn man letzteres Argument weglässt, wird die Legende vertikal ausgegeben. plot + theme( axis.title = element_text(size = 15), axis.text.x = element_text( angle = 45, hjust = 1, size = 12 ), axis.text.y = element_text(size = 12), strip.background = element_blank(), strip.text = element_text(size = 15), legend.title = element_text(size = 14), legend.text = element_text(size = 13), legend.position = c(0.2, 0.9), legend.direction = &quot;horizontal&quot; ) Nun kommen wir im nächsten Schritt zu den Achsen. Mit dem expand Argument entfernt man den extra Raum zwischen y-Achse und x-Achse, wodurch die Balken nicht mehr schweben. Mithilfe von labs() (Akronym für labels) kann die Achsen- sowie die Legendbeschriftung angepasst werden. ggtitle() könnte der Abbildung einen Titel hinzufügen. Da dies häufig jedoch nicht erwünscht ist, nutzen wir das an dieser Stelle nur, um einen etwas größeren Platz über der y-Achse zu verschaffen. plot1 &lt;- plot + scale_y_continuous( expand = c(0, 0), limits = c(0, 5), breaks = c(0, 1, 2, 3, 4, 5) ) + labs( x = &quot;Persönlichkeitsfaktor&quot;, y = &quot;Mittlere Ausprägung&quot;, fill = &quot;Geschlecht (Auswahl)&quot; ) + ggtitle(&quot;&quot;) Alternativ zur Festlegung der Grenzen mit limits innerhalb von scale_y_continouus() könnte man ebenfalls die Funktion lims(), xlims() oder ylims() verwenden. Bei vielen Breaks oder welchen mit ganz bestimmten Abständen könnte man ebenfalls seq() verwenden. seq(from = 0, to = 4000, by = 500) [1] 0 500 1000 1500 2000 2500 3000 3500 4000 Abschließen schauen wir uns noch an, wie man mit sehr langen Achsenbeschriftungen umgehen kann. Mithilfe von guide_axis() können wir beispielswiese festlegen, dass überlappende (check.overlap = TRUE) Achsenbeschriftungen in 2 Zeilen (n.dodge = 2) angezeigt werden. plot + scale_x_discrete(guide = guide_axis( n.dodge = 2, check.overlap = TRUE) ) Eine andere Möglichkeit besteht darin, lange Achsenbeschriftungen untereinander zu schreiben, wenn sie eine bestimmte Breite (z.B. width = 10) erreichen. Dafür würde man die Funktion str_wrap() aus dem stringr Package verwenden (siehe Kapitel 6.10). plot + scale_x_discrete( labels = function(x) str_wrap(x, width = 10) ) 8.14 Speichern von Graphen Abbildungen werden einfach mit ggsave() in dem aktuellen durch das Projekt festgelegten Ordner gespeichert. Dabei sind vor allem drei Argumente wichtig. Mit filename kannst du festlegen, wie die Abbildung heißen soll und in welchem Dateiformat (z.B. jpg oder png) selbige ausgegeben wird. Wenn man mit plot nichts genaueres festlegt, wird einfach die als letztes kreierte Abbildung gespeichert. Mit width und height legt man die Weite in Inches fest. ggsave( filename = &quot;plotA.png&quot;, width = 5, height = 5 ) Wenn man irgendeine Abbildung und nicht unbedingt die Letzte speichern möchte, muss man diesen dem plot Argument übergeben. Die festgelegte Breite und Höhe hat einen erheblich Einfluss auf die Auflösung. ggsave( filename = &quot;plotB.png&quot;, plot = plot1, width = 8, height = 7 ) In Abbildung 8.18 (a) ist plotA.png mit einer Breite und Höhe von 5 Inches abgebildet. In Abbildung 8.18 (b) wurde mit einer Breite von 8 und eine Höhe von 7 die Visualisierung als plotB.png gespeichert. Während in (a) die Achsenbeschriftungen groß sind und die Legende sogar mit der y-Achse überlappt, ist dies in (b) nicht zu beobachten. Abbildung 8.18: Auflösungsunterschiede je nach Größe der gespeicherten Abbildung. Die Größe der Abbildung wird durch die Breite (width) und Höhe (height) festgelegt. durch unterschiedliche Größen wird auch die Auflösung innerhalb der Abbildung beeinflusst, sodass man je nachdem zum Beispiel noch die Textgrößen der Achsenbeschriftungen anpassen muss. Die gespeicherte Abbildung hat nicht das selbe Format wie die Ausgabe innerhalb von RStudio und kann dementsprechend davon abweichen. 8.15 Anwendungsbeispiel Wir werden uns abschließend in diesem Kapitel anhand eines Eye-Tracking Datensatzes namens eye_tracking anschauen, wie wir mithilfe der bisher gelernten Funktionen, eine fortgeschrittene und optische ansprechende Abbildung kreieren können. Diese ist in Abbildung 8.19 illustriert. Abbildung 8.19: Anzahl fixierter Gesichter in Abhängigkeit beobachteter Einwohner pro Quadratkilometer. Diese Abbildung besteht im Prinzip aus vier Teilen, welche nacheinander erstellt und dann miteinander kombiniert werden müssen. Wir haben oben links und rechts unten jeweils eine gruppierte Wahrscheinlichkeitsdichte. Links unten ist unsere Hauptabbildung, mit Komponenten, die wir alle bereits kennengelernt haben. Neu hinzu gekommen ist lediglich geom_rug(), welches ebenfalls eine Möglichkeit zu Verteilungsdarstellung darstellt. Die Hauptabbildung wird als plot1 gespeichert. plot1 &lt;- ggplot(eye_tracking, aes(x = Density, y = Face_sum, color = Group)) + geom_point(aes(color = Group), size = 3) + geom_point(shape = 1, color = &quot;black&quot;, size = 3) + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE) + geom_rug() + labs( x = &quot;Einwohner pro Quadratkilometer&quot;, y = &quot;Anzahl fixierter Gesichter&quot;, color = &quot;Gruppe&quot; ) + scale_y_continuous(limits = c(0, 205), expand = c(0, 0)) + scale_x_continuous(limits = c(1, 4), expand = c(0, 0)) + theme_classic() + theme(legend.position = c(0.15, 0.9)) Bei den beiden Abbildungen zur Wahrscheinlichkeitsdichte müssen wir zusätzlich das Thema (auch die Achsen) mithilfe von theme_void() vollständig ausblenden. Außerdem entfernen wir die Legende. Gespeichert werden die beiden Ergebnisse als dens1 und dens2. dens1 &lt;- ggplot(eye_tracking, aes(x = Density, fill = Group)) + geom_density(alpha = 0.4) + theme_void() + theme(legend.position = &quot;none&quot;) dens2 &lt;- ggplot(eye_tracking, aes(y = Face_sum, fill = Group)) + geom_density(alpha = 0.4) + theme_void() + theme(legend.position = &quot;none&quot;) Jetzt müssen wir die Abbildungen nur noch zusammenfügen. Neu ist an dieser Stelle zum einen die Funktion plot_spacer(), die oben rechts den leeren Raum ausfüllt. Auf der anderen Seite verwenden wir innerhalb von plot_layout() nun die widths und height Argumente, die uns die Spezifizierung der Verhältnisse ermöglicht. Schließlich möchten wir die Wahrscheinlichkeitsdichte oben links und rechts unten kleiner dargestellt haben als unsere Hauptabbildung links unten. dens1 + plot_spacer() + plot1 + dens2 + plot_layout( ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4) ) 8.16 Exemplarische Erweiterungen Wir erinnern uns, dass die beiden gs in ggplot für Grammar of Graphics stehen. Daraus resultiert nicht nur eine konsistente Anwendung, wie wir es in den bisherigen Kapiteln kennengelernt haben. Zusätzlich gibt es diverse Erweiterungen, die mal mehr und mal weniger konsistent anzuwenden sind. Alle haben jedoch die gleiche Basis, einen ggplot, gemein. Dadurch können wir unsere Visualisierung wie zuvor anpassen und erweitern. Die meisten Erweiterungen halten sich an eine einheitliche Namensgebung. Vor dem Zweck des Packages steht also auch bei den Erweiterungspackages immer ein gg (z.B. ggridges oder gghighlight). Leider hält sich nicht an eine konsistente Namensgebung und Anwendung. Ein Beispiel hierfür sehen wir in Kapitel 8.16.3. Im Folgenden werden wir uns vier Beispiele für Erweiterungen anschauen. 8.16.1 Ridgeline Plot Für dieses Kapitel muss das ggridges Package installiert und geladen sein. library(ggridges) Sogenannte Ridgeline Plots erlauben unter anderem den Vergleich von mehreren Dichtefunktionen innerhalb einer Abbildung. In Kapitel 8.2 haben wir die Wahrscheinlichkeitsdichte und somit die Verteilung der Werte für eine Variable angezeigt. In Kapitel 8.15 haben wir gesehen, wie man diese nach einer gruppierenden Variable aufteilen kann. Um mehrere Dichten untereinander innerhalb einer Abbildung zu vergleichen, müssen wir den Datensatz zuerst in das lange Datenformat bringen (siehe Kapitel 6.7. big_five_long &lt;- big_five %&gt;% pivot_longer( cols = Extraversion:Gewissenhaftigkeit, values_to = &quot;Auspraegung&quot;, names_to = &quot;Faktor&quot; ) %&gt;% relocate(Faktor, Auspraegung) big_five_long # A tibble: 800 x 14 Faktor Auspraegung Alter Geschlecht O1 O2 O3 O4 O5 O6 O7 O8 O9 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extra~ 3 36 m 5 1 5 1 4 1 5 5 4 2 Neuro~ 1.9 36 m 5 1 5 1 4 1 5 5 4 3 Vertr~ 3.4 36 m 5 1 5 1 4 1 5 5 4 4 Gewis~ 3.3 36 m 5 1 5 1 4 1 5 5 4 # ... with 796 more rows, and 1 more variable: O10 &lt;dbl&gt; Nun können wir anhand der neuen Funktion geom_density_ridges() die Verteilungen von Extraversion, Neurotizismus, Verträglichkeit und Gewissenhaftigkeit auf einen Schlag abbilden. Zusätzlich übergeben wir das Argument alpha, welches die Dichten leicht durchsichtig macht. ggplot(big_five_long, aes(x = Auspraegung, y = Faktor)) + geom_density_ridges(alpha = 0.8) Abbildung 8.20: Mehrere Dichteverteilungen untereinander mit Ridgeline Plots. Etwas ungewöhnlich ist an dieser Stelle, dass wir auf der x-Achse die diskrete Variable haben, die wir vergleichen und auf der y-Achse die mittlere Ausprägung des jeweiligen Persönlichkeitsfaktors. 8.16.2 Hervorheben bestimmter Werte Im Rahmen dieses Kapitels muss das gghighlight Package installiert und geladen werden. library(gghighlight) Manchmal möchte man ein bestimmtes Augenmerk auf Teile einer Abbildung legen, die für die Beantwortung der Fragestellung eine besondere Bedeutung haben. Dies schauen wir uns anhand des Beispiels der Balkendiagramme an (siehe Kapitel 8.6). Zuerst müssen wir also die deskriptiven Statistiken in Form des Mittelwerts (Mean) und der Standardabweichung (SD) berechnen. big5_means2 &lt;- big_five_long %&gt;% group_by(Faktor) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means2 # A tibble: 4 x 3 Faktor Mean SD * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion 3.08 0.347 2 Gewissenhaftigkeit 3.16 0.361 3 Neurotizismus 3.13 0.682 4 Vertraeglichkeit 3.2 0.367 Wie gewohnt visualisieren wir die Balken anschließend mit geom_col() (siehe Abbildung 8.21 (a)). ggplot(big5_means2, aes(x = Faktor, y = Mean)) + geom_col() Um etwas bestimmtes hervorzuheben, können wir der neuen gghighlight() Funktion jede Bedingung übergeben. Alles, worauf diese Bedingung zutrifft, wir hervorgehoben. Beachte dabei, dass die Abfrage innerhalb der Funktion vom logischen Datentyp sein muss. In Abbildung 8.21 (b) sehen wir exemplarisch den Balken des Persönlichkeitsfaktors Gewissenhaftigkeit hervorgehoben. ggplot(big5_means2, aes(x = Faktor, y = Mean)) + geom_col() + gghighlight(Faktor == &quot;Gewissenhaftigkeit&quot;) Mit diesem Package haben wir schon etwas Inkonsistenz in den Erweiterungspackages kennengelernt. Wir haben hier zwar noch die üblich Syntax mit ggplot() + geom_*(), allerdings weicht die Namensgebung bei gghighlight() ab. Eigentlich sollten wir auch hier ein geom_ oder stat_ als Präfix haben. Abbildung 8.21: Hervorheben bestimmter Balken oder anderer geometrischen Formen. 8.16.3 Kaplan-Meier-Kurve Für dieses Kapitel müssen sowohl das survival als auch das survminer Package installiert und geladen werden. Wenn man die kumulativen Inzidenzen visualisieren möchte, muss außerdem das cmprsk Package zur Verwendung bereit sein. library(survival) library(survminer) library(cmprsk) Aus dem survival Package schauen wir uns den Datensatz lung an. Dabei interessieren uns die Spalten time, status und sex. Die Variable time gibt die Überlebenszeit in Tagen an, status beinhaltet die Information über den Tod (1) oder für ein zensiertes Ereignis (2). Die Variable sex beinhaltet das biologische Geschlecht in Form männlich (1) und weiblich (2). lung &lt;- as_tibble(lung) lung # A tibble: 228 x 10 inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 306 2 74 1 1 90 100 1175 NA 2 3 455 2 68 1 0 90 90 1225 15 3 3 1010 1 56 1 0 90 90 NA 15 4 5 210 2 57 1 1 90 60 1150 11 # ... with 224 more rows Bevor wir eine Kaplan-Meier-Kurve zur Visualisierung der Überlebenszeiten erstellen können, müssen wir zuerst eine mithilfe von survfit() aus dem survival Package die Überlebenswahrscheinlichkeiten schätzen. Für genauere Informationen über die Verwendung dieser Funktion sei auf das Kapitel 9.6.6 verwiesen. res &lt;- survfit(Surv(time = time, event = status) ~ sex, data = lung) Mit der Funktion ggsurvplot() aus dem survminer Package können wir anschließend die Überlebenszeiten visualisieren. Zusätzlich können wir mit conf.int ein Konfidenzinterval und mit risk.table die Anzahl betroffener Personen hinzufügen. ggsurvplot(res, conf.int = TRUE, risk.table = TRUE) Abbildung 8.22: Abbildung von Überlebensraten mit Risikotabelle. Es gibt noch diverse weitere Argumente zum Anpassen dieser Abbildung. Leider hat sich der Packageentwickler an dieser Stelle dazu entscheiden, stark von der bisherigen Syntax abzuweichen. Wir haben an dieser Stelle nicht mehr die Kombination aus ggplot() und geom_*(), sondern nur noch eine Funktion, die im Hintergrund alles weitere übernimmt. Außerdem ist das Ergebnis dieser Funktion zwar optisch noch basierend auf einem ggplot, allerdings wurde die Klasse so stark verändert, sodass wir nicht wie gewohnt die Skalen und das Thema anpassen können. Deshalb haben wir in dieser Abbildung anstelle der veränderten, für Farbenblindheit angepasste Farben die Standard ggplot Farben vorliegen (hellrot und türkis). Alternativ zur Überlebenswahrscheinlichkeit können wir auch die kumulative Hazard Ratio über die Zeit anzeigen lassen. Dafür müssen wir dem fun Argument zusätzlich cumhaz übergeben. ggsurvplot(res, fun = &quot;cumhaz&quot;) Abbildung 8.23: Darstellung kumlativer Hazards Eine weitere Möglichkeit ist die Abbildung der kumulativen Inzidenzen mithilfe der cuminc() Funktion aus dem cmprsk Package. Dafür müssen wir der Funktion die Spalten aus unserem Datensatz einzeln übergeben. Zusätzlich müssen wir explizit cencode festlegen, da das zensierte Ereignis in der Statusvariable hier mit 2 und nicht mit 0 kodiert ist. inc_result &lt;- cuminc( ftime = lung$time, fstatus = lung$status, group = lung$sex, cencode = 2 ) Anschließend kann das Ergebnis mithilfe der ggcompetingrisks() Funktion visualisiert werden. Das mutiple_panels Argument setzen wir an dieser Stelle auf FALSE, damit die Ergebnisse nicht in verschiedene Facets aufgeteilt wird (siehe Kapitel 8.11.2). ggcompetingrisks(inc_result, multiple_panels = FALSE) Abbildung 8.24: Abbildung kumulativer Inzidenzen. 8.16.4 Directed Acyclic Graphs Zur Abbildung von so genannten Directed Acyclic Graphs (DAGs) müssen die Packages dagitty und ggdag installiert und geladen werden. library(dagitty) library(ggdag) DAGs werden dafür verwendet, vermutete Beziehungen zwischen Variablen explizit und nachvollziehbar darzustellen. Konkretere und komplexere Beispiele für DAGs werden in Kapitel 10.4 gezeigt. Hier beschränken wir uns auf das einfachste Beispiel einer Mediation. Dabei sei X eine Behandlung, M ein Mediator und Y unser Outcome. Die Syntax innerhalb der dagitty() Funktion ist etwas ungewöhnlich für R-Verhältnisse, da im Hintergrund auf ein externes Programm zurückgegriffen wird, welches nicht direkt in R geschrieben ist. Wir übergeben also in einem Character (also in Anführungszeichen) die Beziehungen des DAGs. Einmal den indirekten Pfad von X über M zu Y und einmal den direkten Pfad von X zu Y. Das ausgegebene Ergebnis sieht erst einmal unscheinbar aus. dag &lt;- dagitty(&quot;dag{X -&gt; M -&gt; Y; X -&gt; Y}&quot;) dag dag { M X Y M -&gt; Y X -&gt; M X -&gt; Y } Bevor wir nun zur Visualisierung kommen, müssen wir noch die Koordinaten festlegen. Auch hier weichen wir etwas von der bisher kennengelernten Syntax ab. Grundsätzlich müssen für jeden der oben definierten Variablen (X, M, Y) die Koordinaten festgelegt werden. Auf der x-Achse soll zuerst X, dann M und als letztes Y abgebildet werden. Auf der y-Achse sollen X und Y auf der gleichen Höhe sein, während M etwas weiter oben ist. coordinates(dag) &lt;- list( x = c(X = 1, M = 2, Y = 3), y = c(X = 0, M = 1, Y = 0) ) Dieses minimale Beispiel kann am schnellsten mit ggdag() abgebildet werden. Zusätzlich entfernt theme_dag() den Hintergrund und die Achsenbeschriftungen der Abbildung. ggdag(dag) + theme_dag() Abbildung 8.25: Directed Acyclic Graph einer Mediation mit M als Mediator. Das ggdag Package bietet außerdem die Möglichkeit das DAG genauer anzuschauen. tidy_dagitty(dag) # A DAG with 3 nodes and 3 edges # # A tibble: 4 x 8 name x y direction to xend yend circular &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; 1 M 2 1 -&gt; Y 3 0 FALSE 2 X 1 0 -&gt; M 2 1 FALSE 3 X 1 0 -&gt; Y 3 0 FALSE 4 Y 3 0 &lt;NA&gt; &lt;NA&gt; NA NA FALSE Dies können wir uns für komplexere Designs zunutze machen, indem wir auf die klassische ggplot Syntax zurückgreifen. Die Kreise um die Variablen werden mit geom_dag_point(), die Pfeile mit geom_dag_edges() und der Text mit geom_dag_text() erstellt. Das Ergebnis unterscheidet sich in dem Fall zwar nicht von ggdag(), allerdings können damit einige Modifikationen vorgenommen werden, die je nach Kontext nützlich sein können. ggplot(dag, aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point() + geom_dag_edges() + geom_dag_text(col = &quot;white&quot;) + theme_dag() "],["inductive.html", "Kapitel 9 Inferenzstatistik 9.1 Einführung 9.2 Normalverteilung prüfen 9.3 Varianzhomogenität 9.4 Ein- und Zweistichprobenszenarien 9.5 Unterschiede mehrerer Gruppen 9.6 Regressionsmodelle 9.7 Korrelationsanalysen 9.8 Kontingenztafeln", " Kapitel 9 Inferenzstatistik 9.1 Einführung Die Informationen aus dem Einführungskapitel treffen so nur auf Funktionen des inductive Packages zu. Für die verschiedenen inferenzstatistischen Verfahren verwenden wir die Funktionen aus dem inductive Package. Die darin enthaltenen Funktionen beginnen alle mit dem Präfix sta_* (für statistical) und folgen den selben Regeln (sta_wilcox() für den Wilcoxon Test). Dabei unterscheiden wir zwischen eindimensionalen und hierarchischen Modellierungen. Eindimensionale Methoden sind jene, die Zwischensubjektfaktoren miteinander vergleichen. Also beispielsweise Personen aus zwei Gruppen in Bezug auf irgendein Merkmal wie Geschlecht oder Alter zu vergleichen. Hierarchische Modelle erlauben hingegen nicht nur den Zwischensubjektvergleich, sondern auch jenen innerhalb der Person beziehungsweise Beobachtung. Ein klassisches Beispiel hierfür sind Messwiederholungen der selben Personen zu unterschiedlichen Zeitpunkten. Schauen wir uns zuerst eindimensionale Modelle an. In Abbildung 9.1 ist ein exemplarischer Anfang eines Datensatzes mit den Spalten Personenname, Extraversion, Neurotizismus und Geschlecht. Abbildung 9.1: Breiter Datensatz mit Geschlecht als zusätzlicher Zwischensubjektfaktor. Dem jeweiligen statistischen Test übergibt man immer die Variablen als Formel, dem sogenannten formula Argument. In Gleichung (9.1) ist der einfachste Fall, das Einstichprobenszenario illustriert. Würden wir beispielsweise überprüfen wollen, ob ein Merkmal normalverteilt ist, könnte man wie hier gezeigt einfach den Spaltennamen als Argument übergeben. Einstichproben-Szenario: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\mathrm{Extraversion} \\tag{9.1} \\end{equation}\\] Wenn wir zwei Merkmale wie Extraversion und Neurotizismus miteinander hinsichtlich Unterschiede bezüglich der mittleren Ausprägung untersuchen möchten, muss man diese Variablen lediglich mit einer sogenannten Tilde (~) trennen. Diese haben wir zwar bereits im Kontext von Lambda Funktionen kennengelernt, allerdings dienen sie hier einem anderen Zweck. In Gleichung (9.2) sehen wir das gerade beschriebene Beispiel. Die Reihenfolge ist beim Vergleich von zwei Variablen nicht weiter von Bedeutung. Vergleich zweier Vektoren: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\mathrm{Extraversion} \\sim \\mathrm{Neurotizismus} \\tag{9.2} \\end{equation}\\] Wenn die zweite eine gruppierende Variable wie Geschlecht ist, können wir auch einfach diese der Funktion übergeben. Dabei muss die gruppierende Variable auf der rechten Seite der Tilde stehen (siehe Gleichung (9.3)) . Vergleich mehrerer Gruppen: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\mathrm{Extraversion} \\sim \\mathrm{Geschlecht} \\tag{9.3} \\end{equation}\\] Wenn wir den Einfluss im Rahmen eines Regressionsmodells von unabhängigen Variablen auf unsere abhängige Variable herausfinden wollen, schreiben wir die abhängige immer auf die linke Seite und die unabhängigen immer auf die rechte Seite der Tilde. Wenn wir den Einfluss der unabhängigen Variablen getrennt voneinander betrachten möchten, trennen wir diese mit einem Plus. Würden wir eine Interaktion (Moderation) zwischen beispielsweise Neurotizismus und Geschlecht erwarten, würde man stattdessen einen Asterisk (*, das Multiplikationszeichen) verwenden. In Gleichung (9.4) ist dieser Anwendungsfall illustriert. Untersuchung einer abhängigen Variable: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\overbrace{\\mathrm{Extraversion}}^{\\text{abhaengige Variable}} \\sim \\overbrace{\\mathrm{Neurotizismus} + \\mathrm{Geschlecht}}^{\\text{unabhaengige Variablen}} \\tag{9.4} \\end{equation}\\] Dies ist in selbiger Form auf die Untersuchung von Unterschiedshypothesen übertragbar (z.B. bei Varianzanalysen). Für hierarchische Modellierungen muss der Datensatz erst in das lange Format umgewandelt werden. Falls dir das nichts sagst, solltest du vorm Rechnen dieser Modelle noch einmal Kapitel 6.7 studieren. In Abbildung 9.2 ist dies für das einfach Beispiel der Messwiederholung bezüglich der Persönlichkeitsfaktoren Extraversion und Neurotizismus illustriert. Abbildung 9.2: Langer Datensatz mit Persönlichkeitsfaktor als Innersubjektfaktor und Alter sowie Geschlecht als Zwischensubjektfaktoren. Bei der hierarchischen Modellierung ändert sich nichts für die Analyse von Zwischensubjektfaktoren wie Geschlecht. Möchten wir nun zusätzlich die hierarchische Struktur der Daten berücksichtigen, müssen wir einen eingeklammerten Term hinzufügen. Das einfachste Beispiel hierfür ist die Messwiederholung aus der obigen Abbildung. Dafür schreiben wir die in die Klammer den Persönlichkeitsfaktor gegeben (|) einer Identifikation der jeweiligen Person. Dieser Anwendungsfall ist in Gleichung (9.5) abgebildet. Hierarchische Modelle: \\[\\begin{equation} \\overbrace{\\mathrm{Extraversion}}^{\\text{abhaengige Variable}} \\sim \\overbrace{\\mathrm{Geschlecht}}^{\\text{Fixed Effect(s)}} + \\underbrace{(\\text{Faktor} \\mid \\text{Person})}_{\\text{Random Effect(s)}} \\tag{9.5} \\end{equation}\\] Alle statistischen Tests innerhalb des inductive Packages funktionieren nach dem selben Prinzip. Sie beginnen alle mit dem Präfix sta_*() und nur die data und formula Argumente. Wenn du die in diesem Kapitel eingeführte Formelsyntax verstanden hast, kannst du alle in diesem Buch vorgestellten statistischen Tests anwenden. Solange das inductive Package noch nicht fertig ist, muss für die schön formatierte Ausgabe der statistischen Tests das broom Package installiert und geladen sein. 9.2 Normalverteilung prüfen 9.2.1 Shapiro-Wilk Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Beim Shapiro-Wilk Test zur Überprüfung der Normalverteilung müssen wir der Funktion shapiro.test() lediglich die entsprechende Spalte des Datensatzes als Zahlenreihe (respektive Vektor) übergeben. shapiro.test(big5$Extraversion) %&gt;% tidy() # A tibble: 1 x 3 statistic p.value method &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.957 0.00000984 Shapiro-Wilk normality test Da der p-Wert hier bei einem \\(\\alpha\\) Niveau von 5% signifikant ist, würden wir an dieser Stelle von einer Verletzung der Normalverteilung sprechen. Die Annahme der Normalverteilung muss demnach abgelehnt werden. 9.2.2 Kolmogorov-Smirnov Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Der Kolmogorov-Smirnov Test wird fast gleich wie der Shapiro-Wilk Test angewandt. Mit dem y Argument muss man allerdings zusätzlich die Verteilung, mit der wir die Spalte Extraversion vergleichen (hier Normalverteilung), explizit festlegen. ks.test(big5$Extraversion, y = rnorm) %&gt;% tidy() # A tibble: 1 x 4 statistic p.value method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 2.98 0 One-sample Kolmogorov-Smirnov test two-sided Der statistische Test kommt auf die gleiche Schlussfolgerung wie der des vorherigen Kapitels. Die beobachteten Extraversionwerte sind nicht normalverteilt. Der Kolmogorov Smirnov Test funktioniert nur gut, wenn es keine doppelten Werte gibt. Dies ist bei intervallskalierten im Regelfall gegeben. Da wir hier lediglich aus ordinalen Daten einen Mittelwert gebildet haben, sind die Extraversionswerte natürlich trotzdem nicht intervallskaliert. 9.3 Varianzhomogenität 9.3.1 F Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Mit dem F Test können wir die Hypothese der Varianzgleichheit zwischen zwei Gruppen (hier Männer und Frauen) testen. var.test(Extraversion ~ Geschlecht, data = big5) %&gt;% tidy() # A tibble: 1 x 9 estimate num.df den.df statistic p.value conf.low conf.high method alternative &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1.19 117 81 1.19 0.413 0.787 1.76 F test to compare tw~ two.sided Da der p-Wert mit 0.413 größer als 0.05 (häufig gewähltes \\(\\alpha\\) Niveau) ist, würden wir hier von Varianzhomogenität (Varianzgleichheit) ausgehen. Wir erhalten außerdem den F Wert (statistic), die Freiheitsgrade und das Konfidenzintervall, welches bei Gültigkeit der Nullhypothese die 1 enthalten soll. 9.3.2 Levene Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für den Levene Test wird derzeit noch das car Package benötigt. library(car) Nun können wir der Funktion leveneTest() in gewohnter Formelsyntax die Spalten Extraversion und Geschlecht des Datensatzes big5 übergeben. leveneTest(Extraversion ~ Geschlecht, data = big5) %&gt;% tidy() # A tibble: 2 x 4 term df statistic p.value &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 &quot;group&quot; 1 0.0000925 0.992 2 &quot;&quot; 198 NA NA Wir erhalten auch hier die Freiheitsgrade, Teststatistik und den p-Wert. Dabei kommen wir ausgehend vom p-Wert auf den gleichen Schluss wie beim F Test. Der Vorteil des Levene und des Bartletts Test ist der, dass wir auf einen Schlag die Varianz mehrere Gruppen miteinander vergleichen könnten. 9.3.3 Bartletts Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Der Bartlett Test funktioniert genau wie die beiden bisher eingeführten Tests zur Überprüfung der Varianzhomogenität. bartlett.test(Extraversion ~ Geschlecht, data = big5) %&gt;% tidy() # A tibble: 1 x 4 statistic p.value parameter method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.690 0.406 1 Bartlett test of homogeneity of variances Auch hier kommen wir auf die selbe Schlussfolgerung der vorhandenen Varianzhomogenität zwischen Männern und Frauen hinsichtlich der mittleren Extraversionsausprägung. 9.4 Ein- und Zweistichprobenszenarien 9.4.1 t Test und Welch Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für einen Einstichproben t Test, indem wir die Hypothese testen, ob der Mittelwert gleich 0 ist, müssen wir erneut die Spalte Extraversion aus dem Datensatz big5 herausziehen. Die zu testende Hypothese kann mit dem mu Argument entsprechend angepasst werden. Außerdem müssen wir var.equal = TRUE setzen, da ansonsten mit der gleichen Funktion der Welch Test berechnet wird. t.test(big5$Extraversion, var.equal = TRUE) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 3.08 126. 2.04e-191 199 3.03 3.12 One Sample t-test two.sided Die Hypothese kann mit dem alternative Argument auf einseitige Tests (\"less\" oder \"greater\") umgestellt werden. Die Standardeinstellung ist ein zweiseitiger Test. Das \\(\\alpha\\) Niveau kann über das Argument conf.level modifiziert werden. Ausgelassen wird ein Konfidenzlevel von 0.95 verwendet. Für einen Mittelwertsvergleich zwischen zwei Gruppen (hier Männer und Frauen) nutzen wir die bekannte Formelsyntax. Exemplarisch sei hier der Welch Test berechnet (var.equal = FALSE ist die Standardeinstellung der Funktion). Diesen würde man verwenden, wenn zwar Normalverteilung aber keine Varianzhomogenität gegeben wäre. t.test(Extraversion ~ Geschlecht, data = big5) %&gt;% tidy() # A tibble: 1 x 10 estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 -0.0634 3.05 3.11 -1.29 0.197 183. -0.160 0.0332 Welch~ # ... with 1 more variable: alternative &lt;chr&gt; Für einen Test auf abhängige Stichproben, kann das paired Argument hinzugefügt werden. t.test(Extraversion ~ Geschlecht, data = big5, paired = TRUE) %&gt;% tidy() 9.4.2 Wilcoxon Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Der Wilcoxon Test (auch Mann Whitney U-Test genannt) wird bei Verletzungen der Normalverteilung und Varianzhomogenität verwendet. Die Formelsyntax bleibt dabei die selbe wie bei den anderen Tests. wilcox.test(Extraversion ~ Geschlecht, data = big5) %&gt;% tidy() # A tibble: 1 x 4 statistic p.value method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 4265 0.153 Wilcoxon rank sum test with continuity correction two.sided 9.4.3 Brunner Munzel Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für den Brunner Munzel Test wird das brunnermunzel Package benötigt. library(brunnermunzel) Für den Brunner Munzel Test verwenden wir erneut die Formalsyntax. brunnermunzel.test(Extraversion ~ Geschlecht, data = big5) %&gt;% tidy() # A tibble: 1 x 7 estimate statistic p.value parameter conf.low.lower conf.high.upper method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.559 1.44 0.151 180. 0.478 0.640 Brunner-Munzel Test Falls man keine gruppierende Variable, sondern zwei einzelne Spalten vorliegen hat, müssen die Spalten wieder als Zahlenreihen (respektive Vektoren) aus dem Datensatz herausgezogen werden. brunnermunzel.test(big5$Extraversion, big5$Neurotizismus) %&gt;% tidy() 9.5 Unterschiede mehrerer Gruppen 9.5.1 AN(C)OVA Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Varianzanalysen (auch ANOVA genannt) benötigen für Typ 2 und Typ 3 Quadratsummen in R das Package car. library(car) Von hier an muss zuerst mit aov() (Akronym für analysis of variances, engl. für Varianzanalyse) das eigentliche Modell aufgestellt werden. Die Syntax ist wie im Einführungskapitel beschrieben. Wenn man die verschiedenen unabhängigen Variablen mit einem Plus-Zeichen hinzufügt, werden die Unterschiede unabhängig voneinander geprüft. result &lt;- aov(Extraversion ~ Gruppe + Alter, data = big5_mod) result %&gt;% Anova(type = 3) %&gt;% tidy() # A tibble: 4 x 5 term sumsq df statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 101. 1 851. 3.14e-73 2 Gruppe 0.314 2 1.32 2.69e- 1 3 Alter 0.0836 1 0.705 4.02e- 1 4 Residuals 23.2 196 NA NA Alternativ kann das Plus-Zeichen mit einem Asterisk (beziehungsweise Muliplikationszeichen) ausgetauscht werden. So würde man die Interaktion der Kovariaten berücksichtigen und somit eine so genannte ANCOVA (Akronym für analysis of covariance, engl. für Kovarianzanalyse) berechnen. aov(Extraversion ~ Gruppe * Alter, data = big5_mod) %&gt;% Anova(type = 3) %&gt;% tidy() # A tibble: 5 x 5 term sumsq df statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 55.7 1 472. 7.66e-54 2 Gruppe 0.410 2 1.74 1.78e- 1 3 Alter 0.0834 1 0.707 4.02e- 1 4 Gruppe:Alter 0.334 2 1.42 2.45e- 1 # ... with 1 more row Die Interaktionseffekte werden mit einem Doppelpunkt in der Ausgabe markiert (hier Geschlecht:Alter). Für die genauer Untersuchung des Haupteffekts Gruppe, müssen wir bei Signifikanz noch einen Post-Hoc Test machen. Eine Möglichkeit dafür ist der Test von Tukey mit der Funktion TukeyHSD() (Akronym für Honest Significant Differences). TukeyHSD(result, &quot;Gruppe&quot;) Warning in replications(paste(&quot;~&quot;, xx), data = mf): non-factors ignored: Alter Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = Extraversion ~ Gruppe + Alter, data = big5_mod) $Gruppe diff lwr upr p adj Mittel-Jung 0.008320251 -0.1381335 0.15477398 0.9901253 Weise-Jung -0.210544218 -0.4379624 0.01687397 0.0760155 Weise-Mittel -0.218864469 -0.4721886 0.03445966 0.1053580 Eine Alternative dazu stellt der t Test dar, welcher die einzelnen Gruppen bezüglich der Extraversion paarweise vergleicht. Dafür müssen wir der Funktion pairwise.t.test() die entsprechenden Spalten als Vektoren übergeben. pairwise.t.test(big5_mod$Extraversion, big5_mod$Gruppe) Pairwise comparisons using t tests with pooled SD data: big5_mod$Extraversion and big5_mod$Gruppe Jung Mittel Mittel 0.89 - Weise 0.09 0.09 P value adjustment method: holm 9.5.2 ANOVA mit Messwiederholung Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für die Berechnung von Varianzanalysen mit Messwiederholung benötigen wir das afex Package. library(afex) library(emmeans) Bevor wir nun zur Berechnung kommen, müssen wir den Datensatz erst in ein langes Format bringen. big5_long &lt;- big5_mod %&gt;% pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) result2 &lt;- aov_4(Auspraegung ~ Gruppe + (Faktor | ID), data = big5_long) result2 Anova Table (Type 3 tests) Response: Auspraegung Effect df MSE F ges p.value 1 Gruppe 2, 197 0.30 2.69 + .014 .070 2 Faktor 1, 197 0.27 0.10 &lt;.001 .749 3 Gruppe:Faktor 2, 197 0.27 1.60 .008 .205 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Als Post-Hoc Test können wir in diesem Fall die geschätzten Randsummen mithilfe von emmeans() (Akronym für estimated marginal means) ausgeben lassen. post_aov &lt;- emmeans(result2, ~ Gruppe) NOTE: Results may be misleading due to involvement in interactions post_aov Gruppe emmean SE df lower.CL upper.CL Jung 3.21 0.0534 197 3.11 3.32 Mittel 3.10 0.0617 197 2.98 3.22 Weise 3.00 0.0783 197 2.85 3.16 Results are averaged over the levels of: Faktor Warning: EMMs are biased unless design is perfectly balanced Confidence level used: 0.95 Für Signifikanztests der jeweiligen Randsummen können wir jene der Funktion pair() übergeben. pairs(post_aov) contrast estimate SE df t.ratio p.value Jung - Mittel 0.1097 0.0702 197 1.563 0.2642 Jung - Weise 0.2083 0.1090 197 1.911 0.1382 Mittel - Weise 0.0986 0.1214 197 0.812 0.6959 Results are averaged over the levels of: Faktor P value adjustment: tukey method for comparing a family of 3 estimates 9.5.3 Kruskal-Wallis Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Den Kruskal-Wallis Test können wir anwenden, wenn wir mehr als zwei Gruppen untersuchen und wir nicht von Normalverteilung und Varianzhomogenität ausgehen können. Die Syntax bleibt dabei wie gewohnt. kruskal.test(Extraversion ~ Gruppe, data = big5_mod) %&gt;% tidy() # A tibble: 1 x 4 statistic p.value parameter method &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; 1 4.65 0.0976 2 Kruskal-Wallis rank sum test 9.5.4 Friedman Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Den Friedman Test verwenden wir in den selben Anwendungsfällen wie den Kruskal-Wallis Test. Der Unterschied ist an dieser Stelle die zusätzliche Berücksichtigung von Innersubjektfaktoren. Wir benötigen daher wie im Kontext der Varianzanalyse mit Messwiederholung in Kapitel 9.5.2 den Datensatz wieder im langen Format. Von da unterscheidet sich die Syntax nur insofern, als das wir keine Klammern um den Messwiederholungsterm auf der rechten Seite der Tilde schreiben. friedman.test(Auspraegung ~ Faktor | ID, data = big5_long) %&gt;% tidy() # A tibble: 1 x 4 statistic p.value parameter method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1.35 0.246 1 Friedman rank sum test 9.6 Regressionsmodelle 9.6.1 Lineare Regression Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Die lm() Funktion (Akronym für lineares Modell) haben wir bereits im Kontext von Varianzanalysen kennengelernt (siehe Kapitel 9.5.1). Für Regressionsmodelle können wir uns einen Schritt sparen. Die Syntax bleibt allerdings die gleiche. Beachte an dieser Stelle, dass die unabhängigen Variablen bei Regressionsanalysen entweder binär (0, 1) oder intervallskaliert sein müssen. model &lt;- lm(Extraversion ~ Geschlecht + Alter, data = big5) Die Zusammenfassung der Ergebnisse erhalten wir wieder schön formatiert mit tidy(). tidy(model) # A tibble: 3 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3.06 0.0327 93.4 4.12e-165 2 Geschlechtm 0.0597 0.0499 1.19 2.34e- 1 3 Alter -0.000119 0.000126 -0.945 3.46e- 1 Zusätzliche Ausgaben wie Informationskriterien (AIC, BIC) erhalten wir durch glance(), welches auch im broom Package enthalten ist. glance(model) # A tibble: 1 x 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.0126 0.00259 0.346 1.26 0.286 2 -70.1 148. 161. 23.6 197 # ... with 1 more variable: nobs &lt;int&gt; Falls wir, wie bei einer Varianzanalyse, eine gruppierende Variable wie Altersgruppe (Gruppe) mit mehr als 2 Stufen innerhalb einer Regression untersuchen möchten, müssen wir zuerst so genannte Dummy Variablen erstellen. Das heißt, wir erstellen uns für jede Altersgruppe außer unserer Referenzgruppe (z.B. Jung) eine binäre neue Spalte. Während wir das ebenfalls manuell mit if_else() bewerkstelligen könnten, verwenden wir an dieser Stelle Faktoren, welche die Aufgabe für uns intern übernehmen. big5_mod1 &lt;- big5_mod %&gt;% mutate(Gruppe = as.factor(Gruppe)) Wenn die gruppierende Variable ein Faktor ist, erstellt R von selbst die dummy Variablen und gibt \\(\\beta\\) Koeffizienten mit entsprechendem Signifikanztest für jede der Faktorstufen (außer der Referenzgruppe) aus. lm(Extraversion ~ Gruppe, data = big5_mod1) %&gt;% tidy() # A tibble: 3 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3.09 0.0284 109. 6.14e-178 2 GruppeMittel 0.00832 0.0620 0.134 8.93e- 1 3 GruppeWeise -0.211 0.0962 -2.19 2.98e- 2 9.6.2 Lineares gemischtes Modell Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für die Berechnung eines gemischten Modells muss das lmerTest Package installiert und geladen sein. library(lmerTest) Bei der im vorherigen Kapitel kennengelernten linearen Regressionen haben wir nur so genannte Fixed Effects (wie Alter oder Geschlecht) miteinander verglichen. In einem gemischten Modell haben wir nicht nur Fixed Effects, sondern zusätzlich Random Effects. Diese erlauben beispielsweise eine zufällige Variation auch innerhalb der Personen. Andere Namen für dieses Modell sind Mixed Model, Multilevel Model oder hierarchisches Modell. Fälschlicherweise wird vom hierarchischem Modell von manchen Autoren eine hierarchische Regression unterschieden. Mit der hierarchischen Regression ist dort eine Art Stepwise Regression (engl. für Schrittweise Regression) gemeint, welche beispielsweise anhand von Informationskriterien verschiedene Modelle miteinander vergleicht. In diesem Kapitel meinen wir mit hierarchischer Modellierung oder Regression allerdings ausdrücklich die zusätzliche Modellierung der zufälligen Intercepts. Zur eigentlichen Modellierung müssen wir unseren Datensatz zuerst ins lange Datenformat bringen (siehe Kapitel 6.7). Die Altersgruppe muss an dieser Stelle als Faktor kodiert werden, da die Variable mehr als 2 Stufen hat. big5_mod1 &lt;- big5_mod %&gt;% pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Wert&quot; ) %&gt;% mutate(Gruppe = as.factor(Gruppe)) big5_mod1 # A tibble: 400 x 6 Alter Geschlecht Gruppe ID Faktor Wert &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 36 m Mittel 1 Extraversion 3 2 36 m Mittel 1 Neurotizismus 1.9 3 30 f Jung 2 Extraversion 3.1 4 30 f Jung 2 Neurotizismus 3.4 # ... with 396 more rows Die Funktion für die hierarchische Modellierung lautet lmer(). Wie in Formel (9.5) bereits eingeführt, können wir die zufälligen Effekte nun innerhalb einer runden Klammer hinzufügen. fit &lt;- lmer(Wert ~ Gruppe + Geschlecht + Faktor + (1 | ID), data = big5_mod1) summary(fit) Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] Formula: Wert ~ Gruppe + Geschlecht + Faktor + (1 | ID) Data: big5_mod1 REML criterion at convergence: 651 Scaled residuals: Min 1Q Median 3Q Max -3.3611 -0.5524 -0.0624 0.5407 2.7249 Random effects: Groups Name Variance Std.Dev. ID (Intercept) 0.0118 0.1086 Residual 0.2760 0.5253 Number of obs: 400, groups: ID, 200 Fixed effects: Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 3.15460 0.04684 345.46356 67.351 &lt;2e-16 *** GruppeMittel -0.09970 0.06990 195.99995 -1.426 0.1554 GruppeWeise -0.21237 0.10827 195.99995 -1.962 0.0512 . Geschlechtm -0.10802 0.05582 195.99995 -1.935 0.0544 . FaktorNeurotizismus 0.05700 0.05253 198.99993 1.085 0.2792 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) GrppMt GrppWs Gschlc GruppeMittl -0.276 GruppeWeise -0.210 0.133 Geschlechtm -0.470 -0.074 0.019 FktrNrtzsms -0.561 0.000 0.000 0.000 9.6.3 Logistische Regression Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Falls die abhängige Variable nicht intervallskaliert (respektive metrisch) sondern binär (0, 1) ist, müssen wir eine logistische Regression verwenden. In R trägt die Funktion den Namen glm() (Akronym für Generalisiertes Lineares Modell). Exemplarisch erstellen wir uns zu Beginn eine binäre abhängige Variable (Geschlecht). big5_bin &lt;- big5 %&gt;% mutate(Geschlecht = if_else(Geschlecht == &quot;m&quot;, 1, 0)) Nun können wir genau wie bei der linearen Regression das Modell aufstellen und mit tidy() eine schöne Ausgabe erstellen. model2 &lt;- glm(Geschlecht ~ Extraversion + Alter, data = big5_bin) tidy(model2) # A tibble: 3 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 0.0475 0.313 0.152 0.880 2 Extraversion 0.121 0.101 1.19 0.234 3 Alter -0.000186 0.000179 -1.03 0.302 Mit glance() erhalten wir auch hier wieder zusätzliche Informationen. glance(model2) # A tibble: 1 x 8 null.deviance df.null logLik AIC BIC deviance df.residual nobs &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 48.4 199 -141. 289. 302. 47.7 197 200 9.6.4 Baseline Logit Modell Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Zur Berechnung von Baseline Logit Modellen muss das VGAM Package installiert und geladen sein. library(VGAM) Baseline Logit Modelle verwenden wir dann, wenn unsere abhängige Variable nicht binär ist, sondern mehrere ungeordnete Stufen hat. Die abhängige Variable ist also multinomial. Hier schauen wir uns exemplarisch den Einfluss von Geschlecht und Alter auf eine Frage zur Offenheit (O1) an. Diese ist zwar eigentlich ordinal skaliert, allerdings bleiben wir der einfachheitshalber bei dem Beispiel. Entscheiden ist hier das family Argument. Wir müssen festlegen, welche Faktorstufe die Referenzkategorie ist, mit der wir die restlichen Kategorien vergleichen wollen. vglm(O1 ~ Geschlecht + Alter, data = big5, family = multinomial(refLevel = 1)) %&gt;% summary() Call: vglm(formula = O1 ~ Geschlecht + Alter, family = multinomial(refLevel = 1), data = big5) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 -0.43014 1.56406 -0.275 0.783 (Intercept):2 0.85767 1.50833 0.569 0.570 (Intercept):3 0.98431 1.46124 0.674 0.501 (Intercept):4 0.66895 1.46387 0.457 0.648 Geschlechtm:1 1.57251 1.20827 1.301 0.193 Geschlechtm:2 1.70249 1.16454 1.462 0.144 Geschlechtm:3 0.25837 1.15895 0.223 0.824 Geschlechtm:4 0.89654 1.15546 0.776 0.438 Alter:1 0.05434 0.06709 0.810 0.418 Alter:2 0.02951 0.06614 0.446 0.655 Alter:3 0.06521 0.06442 1.012 0.311 Alter:4 0.06540 0.06442 1.015 0.310 Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1]), log(mu[,4]/mu[,1]), log(mu[,5]/mu[,1]) Residual deviance: 534.8599 on 788 degrees of freedom Log-likelihood: -267.43 on 788 degrees of freedom Number of Fisher scoring iterations: 9 No Hauck-Donner effect found in any of the estimates Reference group is level 1 of the response Beachte auch, dass wir hier summary() und nicht tidy() zur Ergebnisausgabe verwenden müssen. 9.6.5 Kumulatives Logit Modell Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Zur Berechnung von Kumulative Logit Modelle muss das VGAM Package installiert und geladen sein. library(VGAM) Kumulative Logit Modell verwenden wir bei ordinal skalierter abhängiger Variable. Die Variable O1 ist eine Frage zur Offenheit die von 0 (trifft gar nicht zu) bis 5 (trifft zu) bewertet wird. Im Vergleich zur Baseline Logit Modellen verändert sich an dieser Stelle nur das family Argument. Hier müssen wir die cumulative() Funktion verwenden. vglm(O1 ~ Geschlecht + Alter, data = big5, family = cumulative(parallel = TRUE)) %&gt;% summary() Call: vglm(formula = O1 ~ Geschlecht + Alter, family = cumulative(parallel = TRUE), data = big5) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 -3.8522994 0.4725677 -8.152 3.58e-16 *** (Intercept):2 -2.0844977 0.2465282 -8.455 &lt; 2e-16 *** (Intercept):3 -0.7750703 0.1905505 -4.068 4.75e-05 *** (Intercept):4 0.6701122 0.1882403 3.560 0.000371 *** Geschlechtm 0.4445943 0.2618293 1.698 0.089502 . Alter -0.0005654 0.0007258 -0.779 0.435967 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Names of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2]), logitlink(P[Y&lt;=3]), logitlink(P[Y&lt;=4]) Residual deviance: 552.8905 on 794 degrees of freedom Log-likelihood: -276.4453 on 794 degrees of freedom Number of Fisher scoring iterations: 5 Warning: Hauck-Donner effect detected in the following estimate(s): &#39;(Intercept):1&#39; Exponentiated coefficients: Geschlechtm Alter 1.5598572 0.9994348 9.6.6 Cox Regression Zur Berechnung einer sogenannten Cox Regression, muss das survival Package installiert und geladen sein. library(surival) Für unser Beispiel nehmen wir uns den lung Datensatz aus dem survival Package zur Hand. lung # A tibble: 228 x 10 inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 306 2 74 1 1 90 100 1175 NA 2 3 455 2 68 1 0 90 90 1225 15 3 3 1010 1 56 1 0 90 90 NA 15 4 5 210 2 57 1 1 90 60 1150 11 # ... with 224 more rows Mit der Funktion coxph() können wir nun in fast gewohnter Manier unser Modell schätzen. Besonders ist an dieser Stelle, dass wir der Funktion keine einzelne abhängige Variable übergeben. Stattdessen übergeben wir der Funktion Surv() sowohl die Überlebenszeit in Tagen als auch die Information über den Tod beziehungsweise Zensierung (event = status). Falls eine Modellierung in Abhängigkeit der Start- und Endzeit gewünscht ist, kann die Startzeit dem time und die Endzeit dem time2 Argument übergeben werden. Hier schauen wir uns exemplarisch den Einfluss der Kovariate Geschlecht auf die Überlebenszeit an. Dabei wird das biologische Geschlecht in männlich (1) und weiblich (2) kodiert. cox_res &lt;- coxph(Surv(time = time, event = status) ~ sex, data = lung) summary(cox_res) Call: coxph(formula = Surv(time = time, event = status) ~ sex, data = lung) n= 228, number of events= 165 coef exp(coef) se(coef) z Pr(&gt;|z|) sex -0.5310 0.5880 0.1672 -3.176 0.00149 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 exp(coef) exp(-coef) lower .95 upper .95 sex 0.588 1.701 0.4237 0.816 Concordance= 0.579 (se = 0.021 ) Likelihood ratio test= 10.63 on 1 df, p=0.001 Wald test = 10.09 on 1 df, p=0.001 Score (logrank) test = 10.33 on 1 df, p=0.001 In der Ausgabe sehen wir die Ergebnisse des Likelihood Ratio, Rao Score und Wald Tests. Die entsprechende Hazard Ratio der Kovariaten kann unter exp(coef) abgelesen werden (hier 0.5880). Eine Hazard Ratio unter 1 sagt in unserem Beispiel also eine Risikoreduktion bei weiblichem Geschlecht aus. 9.7 Korrelationsanalysen 9.7.1 Korrelationstests Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Mit der Funktion cor.test() können die Produkt-Moment Korrelation nach Pearson (pearson), Rangkorrelation nach Spearman (spearman) oder Kendall (kendall) angewendet werden. Dafür muss nur das method Argument entsprechend angepasst werden. Als Argumente übergeben wir außerdem die beiden numerischen Spalten, die wir mit dem Dollar-Operator aus dem Datensatz herausziehen müssen. cor.test(big5$Extraversion, big5$Neurotizismus, method = &quot;pearson&quot;) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 0.0697 0.984 0.327 198 -0.0697 0.206 Pearson&#39;s product-moment~ two.sided Für polychorische Korrelationen muss das polycor Package installiert und geladen sein. library(polycor) Die Funktion polychor() benötigt lediglich die zwei numerischen Spalten, die untersucht werden sollen. Dabei muss die Spalte als Zahlenreihe (Vektor) aus dem Datensatz herausgezogen werden. polychor(big5$Extraversion, big5$Neurotizismus) [1] 0.05354389 9.7.2 Korrelationstabellen Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Wenn mehrere Korrelationen auf einmal berechnet werden sollen, können wir die Funktion cor() benutzen. Dabei können wir der Funktion grundsätzlich so viele numerische Spalten übergeben, wie wir wollen. big5 %&gt;% select(Alter, Extraversion, Neurotizismus) %&gt;% cor() Alter Extraversion Neurotizismus Alter 1.00000000 -0.07385301 -0.03123432 Extraversion -0.07385301 1.00000000 0.06972529 Neurotizismus -0.03123432 0.06972529 1.00000000 9.8 Kontingenztafeln Für die Auswertung von Kontingenztafeln nutzen wir die in Kapitel 7.3 erstellte Vierfeldertafel, die die Extraversionsausprägung über 3 gegen das Geschlecht aufträgt. extraversion &lt;- big5_mod %&gt;% pull(Extraversion) sex &lt;- big5_mod %&gt;% pull(Geschlecht) tbl &lt;- table(extraversion &gt; 3, sex) 9.8.1 Fisher-Exact Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Nach Erstellung der Kontingenztafel mit table() können wir diese einfach der Funktion fisher.test() übergeben. fisher.test(tbl) %&gt;% tidy() # A tibble: 1 x 6 estimate p.value conf.low conf.high method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1.15 0.666 0.628 2.11 Fisher&#39;s Exact Test for Count Data two.sided Als Ausgabe erhalten wir unter anderem die Odds Ratio (estimate) und Konfidenzintervalle, die exemplarisch in Kapitel 8.8 zur Visualisierung der Odds Ratios verwendet wurden. Auch hier können wir mit dem alternative Argument auf einseitiges Testen umstellen (less, greater). 9.8.2 Chi Quadrat-Tests Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für die Berechnung des \\(\\chi^2\\) Tests nach Pearson verwenden wir die Funktion chisq.test(). chisq.test(tbl) %&gt;% tidy() # A tibble: 1 x 4 statistic p.value parameter method &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; 1 0.118 0.731 1 Pearson&#39;s Chi-squared test with Yates&#39; continuity correction Bei abhängigen Gruppen würde man hingegen stattdessen Mcnemars \\(\\chi^2\\) Test verwenden. mcnemar.test(tbl) %&gt;% tidy() # A tibble: 1 x 4 statistic p.value parameter method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.674 0.412 1 McNemar&#39;s Chi-squared test with continuity correction "],["latente-variablenmodelle.html", "Kapitel 10 Latente Variablenmodelle 10.1 Hauptkomponentenanalyse 10.2 Explorative Faktorenanalyse 10.3 k-Means Clustering 10.4 Strukturmodelle", " Kapitel 10 Latente Variablenmodelle 10.1 Hauptkomponentenanalyse Für die Hauptkomponentenanalyse verwenden wir eine Funktion aus dem psych Package, welches daher installiert und geladen werden muss. library(psych) Wir schauen uns einen Teil des vollständigen Big Five Datensatzes big_five_comp an, aus dem wir jeweils die ersten drei Items (Fragen zu den Persönlichkeitsfaktoren) auswählen. Diesen speichern wir als big_five_comp1 ab. big_five_comp1 &lt;- big_five_comp %&gt;% select(E1:E3, N1:N3, A1:A3, C1:C3, O1:O3) big_five_comp1 # A tibble: 5,000 x 15 E1 E2 E3 N1 N2 N3 A1 A2 A3 C1 C2 C3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4 2 5 1 5 2 1 5 1 4 1 5 4 1 3 2 2 2 3 2 3 4 1 3 3 4 1 3 3 3 3 3 5 1 1 5 1 5 5 1 5 4 1 5 4 5 5 4 2 5 2 5 4 4 2 5 4 3 3 4 4 3 5 # ... with 4,996 more rows Um eine Hauptkomponentenanalyse durchzuführen, müssen wir lediglich der Funktion pca() (Akronym für principal component analysis, engl. für Hauptkomponentenanalyse) aus dem psych Package den Datensatz übergeben. Die Voraussetzung ist dabei ein numerischer Datentyp aller Variablen im Datensatz. Das nfactors() Argument legt die Anzahl der erwarteten Komponenten fest. res_pca &lt;- big_five_comp1 %&gt;% pca(nfactors = 5) res_pca Principal Components Analysis Call: principal(r = r, nfactors = nfactors, residuals = residuals, rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, missing = missing, impute = impute, oblique.scores = oblique.scores, method = method, use = use, cor = cor, correct = 0.5, weight = NULL) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 RC4 RC3 RC5 h2 u2 com E1 0.76 -0.10 -0.01 0.01 0.16 0.62 0.38 1.1 E2 -0.74 -0.02 -0.07 0.07 0.00 0.56 0.44 1.0 E3 0.76 -0.28 -0.03 0.16 -0.13 0.69 0.31 1.4 N1 -0.06 0.82 -0.05 -0.10 0.06 0.69 0.31 1.1 N2 0.16 -0.71 0.04 0.00 0.00 0.53 0.47 1.1 N3 -0.03 0.83 0.01 0.02 -0.01 0.69 0.31 1.0 A1 -0.11 -0.08 -0.19 0.13 0.76 0.65 0.35 1.3 A2 0.61 0.01 0.12 0.00 -0.37 0.52 0.48 1.7 A3 0.03 0.14 0.16 -0.26 0.71 0.61 0.39 1.5 C1 0.09 -0.08 0.08 0.75 -0.04 0.58 0.42 1.1 C2 0.11 0.09 0.23 -0.65 0.10 0.51 0.49 1.4 C3 0.07 0.12 0.29 0.67 0.05 0.55 0.45 1.5 O1 0.01 -0.04 0.74 0.07 -0.03 0.55 0.45 1.0 O2 0.03 0.23 -0.67 -0.06 0.18 0.53 0.47 1.4 O3 0.10 0.11 0.64 -0.03 0.10 0.45 0.55 1.2 RC1 RC2 RC4 RC3 RC5 SS loadings 2.16 2.07 1.63 1.56 1.32 Proportion Var 0.14 0.14 0.11 0.10 0.09 Cumulative Var 0.14 0.28 0.39 0.50 0.58 Proportion Explained 0.25 0.24 0.19 0.18 0.15 Cumulative Proportion 0.25 0.48 0.67 0.85 1.00 Mean item complexity = 1.3 Test of the hypothesis that 5 components are sufficient. The root mean square of the residuals (RMSR) is 0.08 with the empirical chi square 7243.81 with prob &lt; 0 Fit based upon off diagonal values = 0.75 In der Ergebnisliste sehen wir die Ladungen und einige Zusatzinformationen. Wenn man ausschließlich die Ladungen angezeigt haben möchte, können wir diese mit pluck() (aus dem tidyverse) herausziehen. res_pca %&gt;% pluck(&quot;loadings&quot;) Eine sogenannte naive Hauptkomponentenanalyse könnte mit den direkt in R integrierten Funktionen princomp() oder prcomp() berechnet werden. Naiv bedeutet in dem Kontext, dass keine Vorannahme über die Anzahl der vorhandenen Komponenten getroffen werden muss. 10.2 Explorative Faktorenanalyse Für die explorative Maximum Likelihood Faktorenanalyse verwenden wir im Folgenden den gleichen Datensatz wie im vorherigen Kapitel. big_five_comp1 &lt;- big_five_comp %&gt;% select(E1:E3, N1:N3, A1:A3, C1:C3, O1:O3) big_five_comp1 # A tibble: 5,000 x 15 E1 E2 E3 N1 N2 N3 A1 A2 A3 C1 C2 C3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4 2 5 1 5 2 1 5 1 4 1 5 4 1 3 2 2 2 3 2 3 4 1 3 3 4 1 3 3 3 3 3 5 1 1 5 1 5 5 1 5 4 1 5 4 5 5 4 2 5 2 5 4 4 2 5 4 3 3 4 4 3 5 # ... with 4,996 more rows Zur Berechnung einer explorativen Faktorenanalyse nutzen wir die Funktion factanal() (Akronym für factor analysis, engl. für Faktorenanalyse). Dabei ändert sich das Argument zu factors (anstelle von nfactors). Auch hier müssen alle enthaltenen Spalten des Datensatzes numerischen Datentyps sein. Zusätzlich verwenden wir optional noch die Funktion print(), mit der wir auf zwei Nachkommastellen runden, sortieren und nur alle Werte über 0.3 anzeigen lassen können. big_five_comp1 %&gt;% factanal(factors = 5) %&gt;% print(digits = 2, cutoff = 0.3, sort = TRUE) Call: factanal(x = ., factors = 5) Uniquenesses: E1 E2 E3 N1 N2 N3 A1 A2 A3 C1 C2 C3 O1 O2 O3 0.56 0.62 0.35 0.41 0.66 0.45 0.74 0.64 0.69 0.61 0.77 0.72 0.63 0.69 0.80 Loadings: Factor1 Factor2 Factor3 Factor4 Factor5 E1 0.65 E2 -0.60 E3 0.73 N1 0.75 N2 -0.55 N3 0.74 O1 0.60 O2 -0.51 C1 0.60 A1 0.47 A2 0.49 -0.32 A3 0.49 C2 -0.43 C3 0.47 O3 0.43 Factor1 Factor2 Factor3 Factor4 Factor5 SS loadings 1.62 1.57 0.94 0.88 0.63 Proportion Var 0.11 0.10 0.06 0.06 0.04 Cumulative Var 0.11 0.21 0.28 0.33 0.38 Test of the hypothesis that 5 factors are sufficient. The chi square statistic is 556.98 on 40 degrees of freedom. The p-value is 2.82e-92 Die Art der Rotation kann mit dem rotation Argument entweder auf orthogonal ( varimax) oder auf schräg respektive Oblique (promax) festgelegt werden. Eine Ausgabe bereit zum Weiterverarbeiten in Form eines tibbles erhalten wir an dieser Stelle wieder mit der tidy() Funktion aus dem broom Package. big_five_comp1 %&gt;% factanal(factors = 5) %&gt;% tidy() # A tibble: 15 x 7 variable uniqueness fl1 fl2 fl3 fl4 fl5 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 E1 0.561 0.646 -0.100 0.0220 -0.00242 0.105 2 E2 0.624 -0.602 0.0182 -0.0923 0.0617 0.0151 3 E3 0.350 0.729 -0.255 -0.0250 0.160 -0.164 4 N1 0.414 -0.0648 0.753 -0.0479 -0.0979 0.0615 # ... with 11 more rows Genau wie im vorherigen Kapitel können die Ladungen darüber hinaus ebenfalls mit pluck(\"loadings\") extrahiert und ausgegeben werden. Eine Alternative stellt die fa() Funktion aus dem psych Package dar. 10.3 k-Means Clustering Wir bleiben auch in diesem Kapitel beim Vorzeigen des k-Means Clustering beim Ausschnitt aus dem vollständigen Big Five Datensatz big_five_comp. big_five_comp1 &lt;- big_five_comp %&gt;% select(E1:E3, N1:N3, A1:A3, C1:C3, O1:O3) big_five_comp1 # A tibble: 5,000 x 15 E1 E2 E3 N1 N2 N3 A1 A2 A3 C1 C2 C3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4 2 5 1 5 2 1 5 1 4 1 5 4 1 3 2 2 2 3 2 3 4 1 3 3 4 1 3 3 3 3 3 5 1 1 5 1 5 5 1 5 4 1 5 4 5 5 4 2 5 2 5 4 4 2 5 4 3 3 4 4 3 5 # ... with 4,996 more rows Auch beim k-Means Clustering verändert sich zu den beiden Funktionen aus den vorherigen Kapiteln nicht viel. Die Cluster erstellen wir mithilfe der direkt in R integrierten Funktion kmeans(), der wir mit dem centers Argument die Anzahl der erwarteten Cluster übergeben. Wir können so jede Person einem dieser 3 Cluster zuteilen. k &lt;- big_five_comp1 %&gt;% kmeans(centers = 3) Anschließend könnte man die Cluster dem ursprünglichen Datensatz hinzufügen, um diese beispielsweise im nächsten Schritt gruppiert mithilfe von Ellipsen (siehe Kapitel 8.10) zu visualisieren. big_five_comp1 %&gt;% mutate(Cluster = as.factor(k$cluster)) %&gt;% relocate(Cluster) # A tibble: 5,000 x 16 Cluster E1 E2 E3 N1 N2 N3 A1 A2 A3 C1 C2 C3 O1 O2 &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 4 2 5 1 5 2 1 5 1 4 1 5 4 1 2 3 2 2 3 2 3 4 1 3 3 4 1 3 3 3 3 1 5 1 1 5 1 5 5 1 5 4 1 5 4 5 4 1 2 5 2 5 4 4 2 5 4 3 3 4 4 3 # ... with 4,996 more rows, and 1 more variable: O3 &lt;dbl&gt; 10.4 Strukturmodelle Für alle Strukturmodelle der Unterkapitel verwenden wir das Package lavaan, welches installiert und geladen werden muss. Dieses Package befindet sich noch in der Testphase (Beta Phase), weswegen die Verwendung ausdrücklich ohne Garantie erfolgt. Auf mögliche Alternativen wird im entsprechenden Unterkapitel hingewiesen. library(&quot;lavaan&quot;) Der erste Schritt in der Analyse eines Strukturmodells ist immer die Visualisierung der Beziehungen mithilfe eines Directed Acyclic Graphs (DAGs). Wie diese direkt in R erstellt werden können, wurde bereits in Kapitel 8.16.4 erläutert. Die in den folgenden Kapiteln vorgestellten Methoden werden häufig gedankenlos zur Untersuchung kausaler Beziehungen verwendet. Die geschätzten Pfadkoeffizienten werden dabei fälschlicherweise ohne Berücksichtigung der dafür notwendigen Voraussetzungen einfach kausal interpretiert. Dies ist allerdings nur unter speziellen, sehr strengen Bedingungen möglich. Für eine umfangreiche Diskussion dieser Probleme sowie für eine Einführung in die Interpretation kausaler Beziehungen sei auf The Book Of Why von Pearl &amp; Mackenzie (2019) verwiesen. 10.4.1 Mediation Zur exemplarischen Untersuchung einer möglichen Mediation verwenden wir an dieser Stelle den Datensatz literacy_mod, welcher unter anderem Daten aus einer Erhebung zur statistischen Bildung und dem Risikoverhalten beinhaltet. literacy_mod # A tibble: 146 x 3 Risikoverhalten Schulbildung Literacy_total &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 6.33 4 0.643 2 3.83 4 0.5 3 1.5 1 0.643 4 5.83 5 0.786 # ... with 142 more rows Der erste Schritt ist das Zeichnen eines DAGs (siehe Kapitel 8.16.4). Wir gehen davon aus, dass der Einfluss der statistischen Bildung auf das Risikoverhalten durch die allgemeine Schulbildung mediiert wird (siehe Abbildung 10.1). Abbildung 10.1: Der Einfluss der statistischen Bildung (Literacy) auf das Risikoverhalten (Risiko) wird durch die allgemeine Schulbildung mediiert. Als nächstes stellen wir genau dieses Modell in lavaan Syntax dar. Das Modell muss dabei vom Datentyp Character sein (vorne und hinten ein Anführungszeichen). Für jeden Pfad nehmen wir einen willkürlichen Buchstaben. Der direkte Pfad von der statistischen Bildung (Literacy_total) auf das Risikoverhalten wird mit c bezeichnet. Für den Mediator haben wir einmal den Pfad a von statistischer Bildung auf allgemeine Schulbildung und einmal Pfad b von der Schulbildung zum Risikoverhalten. Der indirekte Effekt ergibt sich dann aus dem Produkt dieser beiden Pfade a und b. Diesen können wir innerhalb des Modells mithilfe des Operators := als ab definieren. Ähnlich können wir auch den Totalen Effekt definieren, welcher aus der Summe von c und ab besteht. Jetzt müssen wir dieses Modell gemeinsam mit dem Datensatznamen nur noch der sem() Funktion aus dem lavaan Package übergeben. Die Rauten im Modell signalisieren dem Programm einen Kommentar und werden daher bei der Berechnung ignoriert. model &lt;- &quot; # Direkter Effekt Risikoverhalten ~ c*Literacy_total # Mediator Schulbildung ~ a*Literacy_total Risikoverhalten ~ b*Schulbildung # Indirekter Effekt (a*b) ab := a*b # Totaler Effekt total := c + (a*b) &quot; fit &lt;- sem(model, data = literacy_mod) summary(fit) lavaan 0.6-7 ended normally after 24 iterations Estimator ML Optimization method NLMINB Number of free parameters 5 Used Total Number of observations 145 146 Model Test User Model: Test statistic 0.000 Degrees of freedom 0 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Regressions: Estimate Std.Err z-value P(&gt;|z|) Risikoverhalten ~ Litrcy_ttl (c) 0.392 0.553 0.710 0.478 Schulbildung ~ Litrcy_ttl (a) 0.648 0.337 1.923 0.054 Risikoverhalten ~ Schulbldng (b) 0.227 0.134 1.685 0.092 Variances: Estimate Std.Err z-value P(&gt;|z|) .Risikoverhaltn 2.189 0.257 8.515 0.000 .Schulbildung 0.835 0.098 8.515 0.000 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) ab 0.147 0.116 1.268 0.205 total 0.539 0.551 0.979 0.328 In der Ergebnisliste können nur nun die einzelnen Pfadkoeffizienten a, b und c unter Estimate betrachten. Auch der indirekte und totale Effekt ist aus der Liste ablesbar. Abgesehen vom lavaan Package kann auch das Package mediation verwendet werden. Dieses Modell setzt neben einer linearen Beziehung unter anderem voraus, dass keine Confounder und Collider existieren, keine Interaktion der Fehlerterme von Schulbildung und Risikoverhalten besteht und die statistische Bildung nicht mit anderen Variablen interagiert. Die Pfadkoeffizienten können nicht ohne weiteres kausal interpretiert werden (siehe auch die Merkbox in Kapitel 10.4). 10.4.2 Konfirmatorische Faktorenanalyse Zum Berechnen einer konfirmatorischen Faktorenanalyse verwenden wir an dieser Stelle eine Auswahl aus dem vollständigen big_five_comp Datensatz. big_five_comp %&gt;% select(E1:E3, N1:N3, A1:A3, C1:C3, O1:O3) # A tibble: 5,000 x 15 E1 E2 E3 N1 N2 N3 A1 A2 A3 C1 C2 C3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4 2 5 1 5 2 1 5 1 4 1 5 4 1 3 2 2 2 3 2 3 4 1 3 3 4 1 3 3 3 3 3 5 1 1 5 1 5 5 1 5 4 1 5 4 5 5 4 2 5 2 5 4 4 2 5 4 3 3 4 4 3 5 # ... with 4,996 more rows Jeder Persönlichkeitsfaktor ist in dem Datensatz durch 10 verschiedene Items (Fragen zur Persönlichkeitseigenschaft) definiert. In Abbildung 10.2 ist die mit 3 Items vereinfachte erwartete Struktur der Faktoren in Form eines DAGs dargestellt (siehe Kapitel 8.16.4). Abbildung 10.2: Vereinfachte Abbildung der vermuteten Faktorenstruktur mithilfe eines DAGs. Die Funktionsweise des lavaan Package ist bei einer konfirmatorischen Faktorenanalyse der aus dem vorherigen Mediationskapitel sehr ähnlich. Neu ist an dieser Stelle der =~ Operator, welcher auf der linken Seite der Gleichung eine neue latente Variable definiert. Hierbei ist es wichtig, dass dieser Name nicht als Spaltennamen bereits im Datensatz enthalten ist. Auf der rechten Seite der Gleichung definieren wir jeweils, welche Items zu welchem Persönlichkeitsfaktor gehörten. Da die Ausgabe ohnehin schon sehr lang ist, wählen wir an dieser Stelle nur jeweils 5 der Items aus. Abschließend verwenden wir die cfa() Funktion anstelle von sem(). Dies hat zwar derzeit noch keine praktischen Konsequenzen, allerdings kann sich dies in der Zukunft noch ändern, da sich das lavaan Package noch in einer Testphase befindet (siehe Kapitel 10.4). model2 &lt;- &quot; Extrav =~ E1 + E2 + E3 + E4 + E5 Neurot =~ N1 + N2 + N3 + N4 + N4 Gewiss =~ C1 + C2 + C3 + C4 + C5 Vertra =~ A1 + A2 + A3 + A4 + A5 Offenh =~ O1 + O2 + O3 + O4 + O5 &quot; fit2 &lt;- cfa(model2, data = big_five_comp) fit2 lavaan 0.6-7 ended normally after 51 iterations Estimator ML Optimization method NLMINB Number of free parameters 58 Number of observations 5000 Model Test User Model: Test statistic 5907.845 Degrees of freedom 242 P-value (Chi-square) 0.000 Um nun die vollständige standardisierte Ergebnisliste mit den wichtigsten Indizes herauszubekommen, würde man summary() verwenden. Da die Ergebnisliste sehr lang ist und wir im nächsten Kapitel ausführlich darauf zu sprechen kommen, sei an dieser Stelle darauf verzichtet. summary(fit2, fit.measures = TRUE, standardized = TRUE) Einzelne Ergebnisse wie die Indizes (fitMeasures(fit2)) oder die Residuen (resid(fit2)) können mit dedizierten Funktionen aus der Ergebnisliste extrahiert werden. Die einzelnen Schätzer können mithilfe der Funktion parameterEstimates() ausgegeben werden. parameterEstimates(fit2) %&gt;% as_tibble() %&gt;% print(n = 10) # A tibble: 63 x 9 lhs op rhs est se z pvalue ci.lower ci.upper &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extrav =~ E1 1 0 NA NA 1 1 2 Extrav =~ E2 -1.11 0.0296 -37.6 0 -1.17 -1.06 3 Extrav =~ E3 1.19 0.0291 40.9 0 1.13 1.25 4 Extrav =~ E4 -1.04 0.0277 -37.7 0 -1.10 -0.990 5 Extrav =~ E5 1.30 0.0308 42.3 0 1.24 1.37 6 Neurot =~ N1 1 0 NA NA 1 1 7 Neurot =~ N2 -0.671 0.0200 -33.5 0 -0.711 -0.632 8 Neurot =~ N3 0.783 0.0212 37.0 0 0.742 0.825 9 Neurot =~ N4 -0.422 0.0197 -21.5 0 -0.461 -0.384 10 Gewiss =~ C1 1 0 NA NA 1 1 # ... with 53 more rows Anstelle des lavaan Packages kann auch das Package sem verwendet werden. 10.4.3 Strukturgleichungsmodell Für das Beispiel eines Strukturgleichungsmodell erweitern wir im folgenden das Messmodell aus dem vorherigen Kapitel. Neben den jeweiligen Items für die einzelnen Persönlichkeitsfaktoren sind schließlich noch die soziodemographischen Variablen Geschlecht und Alter im Datensatz enthalten. big_five_comp # A tibble: 5,000 x 52 Geschlecht E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 N1 N2 N3 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 f 4 2 5 2 5 1 4 3 5 1 1 5 2 2 m 2 2 3 3 3 3 1 5 1 5 2 3 4 3 m 5 1 1 4 5 1 1 5 5 1 5 1 5 4 m 2 5 2 4 3 4 3 4 4 5 5 4 4 # ... with 4,996 more rows, and 38 more variables: N4 &lt;dbl&gt;, N5 &lt;dbl&gt;, N6 &lt;dbl&gt;, N7 &lt;dbl&gt;, # N8 &lt;dbl&gt;, N9 &lt;dbl&gt;, N10 &lt;dbl&gt;, A1 &lt;dbl&gt;, A2 &lt;dbl&gt;, A3 &lt;dbl&gt;, A4 &lt;dbl&gt;, A5 &lt;dbl&gt;, A6 &lt;dbl&gt;, # A7 &lt;dbl&gt;, A8 &lt;dbl&gt;, A9 &lt;dbl&gt;, A10 &lt;dbl&gt;, C1 &lt;dbl&gt;, C2 &lt;dbl&gt;, C3 &lt;dbl&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, # C6 &lt;dbl&gt;, C7 &lt;dbl&gt;, C8 &lt;dbl&gt;, C9 &lt;dbl&gt;, C10 &lt;dbl&gt;, O1 &lt;dbl&gt;, O2 &lt;dbl&gt;, O3 &lt;dbl&gt;, O4 &lt;dbl&gt;, # O5 &lt;dbl&gt;, O6 &lt;dbl&gt;, O7 &lt;dbl&gt;, O8 &lt;dbl&gt;, O9 &lt;dbl&gt;, O10 &lt;dbl&gt;, Alter &lt;dbl&gt; Der Schritt besteht erneut im Erstellen eines DAGs (siehe Kapitel 8.16.4). In Abbildung 10.3 erweitern wir das bisherige Messmodell mit einem weiteren Messmodell. Die Unterscheidung zwischen exogenem und endogenem Messmodell ist heute eher formsache und spiel mathematisch keine Rolle. Daher legen wir das Messmodell, welches die Persönlichkeitsfaktoren durch die jeweiligen Items festlegt als exogenes Messmodell fest. Das endogene Messmodell besteht aus der neuen soziodemographischen Variable SDV, welche sich aus dem Alter und Geschlecht der Personen zusammensetzt. Das eigentliche Strukturmodell besteht aus dem Zusammenhang der latenten Variablen des exogenen und endogenen Messmodells. In diesem Fall würden wir eventuell einen Einfluss von SDV auf Extraversion, Neurotizismus und Offenheit erwarten, während die Gewissenhaftigkeit und Verträglichkeit davon weitgehend unberührt bleibt. Außerdem erwarten wir zur Demonstration noch einen Einfluss von Extraversion auf Neurotizismus. Abbildung 10.3: Visualisierungs des Strukturgleichungsmodells mittels DAG. Im eigentlichen lavaan Modell finden sich keine Neuerungen im Vergleich zu den beiden vorhergehenden Kapiteln. Zuerst wird das exogene Messmodell genau wie in Kapitel 10.2 definiert. Anschließend fügen wir das neue endogene Messmodell auf die selbe Art und Weise hinzu. Das eigentliche Strukturmodell wird als Regressionen zwischen den latenten Variablen festgelegt. Die Rauten werden auch hier vom Programm ignoriert, da diese Kommentare signalisieren. Zusätzlich könnte man mit der doppelten Tilde (~~) noch explizit die Kovarianzen der Residuen festlegen. model3 &lt;- &quot; # Exogenes Messmodell Extrav =~ E1 + E2 + E3 + E4 + E5 Neurot =~ N1 + N2 + N3 + N4 + N4 Gewiss =~ C1 + C2 + C3 + C4 + C5 Vertra =~ A1 + A2 + A3 + A4 + A5 Offenh =~ O1 + O2 + O3 + O4 + O5 # Endogenes Messmodell SDV =~ Geschlecht + Alter # Strukturmodell Extrav ~ SDV Offenh ~ SDV Neurot ~ Extrav + SDV &quot; fit3 &lt;- sem(model3, data = big_five_comp) summary(fit3, fit.measures = TRUE, standardized = TRUE) lavaan 0.6-7 ended normally after 117 iterations Estimator ML Optimization method NLMINB Number of free parameters 60 Number of observations 5000 Model Test User Model: Test statistic 6605.216 Degrees of freedom 291 P-value (Chi-square) 0.000 Model Test Baseline Model: Test statistic 30429.571 Degrees of freedom 325 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.790 Tucker-Lewis Index (TLI) 0.766 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -202088.028 Loglikelihood unrestricted model (H1) NA Akaike (AIC) 404296.056 Bayesian (BIC) 404687.087 Sample-size adjusted Bayesian (BIC) 404496.428 Root Mean Square Error of Approximation: RMSEA 0.066 90 Percent confidence interval - lower 0.065 90 Percent confidence interval - upper 0.067 P-value RMSEA &lt;= 0.05 0.000 Standardized Root Mean Square Residual: SRMR 0.069 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Extrav =~ E1 1.000 0.781 0.628 E2 -1.114 0.030 -37.562 0.000 -0.870 -0.660 E3 1.190 0.029 40.858 0.000 0.929 0.744 E4 -1.045 0.028 -37.692 0.000 -0.816 -0.663 E5 1.306 0.031 42.320 0.000 1.020 0.789 Neurot =~ N1 1.000 1.012 0.768 N2 -0.689 0.021 -33.344 0.000 -0.698 -0.591 N3 0.798 0.022 36.091 0.000 0.808 0.708 N4 -0.422 0.020 -21.025 0.000 -0.428 -0.349 Gewiss =~ C1 1.000 0.641 0.581 C2 -1.166 0.044 -26.418 0.000 -0.748 -0.546 C3 0.566 0.029 19.554 0.000 0.363 0.362 C4 -1.224 0.043 -28.379 0.000 -0.785 -0.629 C5 1.251 0.044 28.453 0.000 0.802 0.633 Vertra =~ A1 1.000 0.628 0.457 A2 -1.014 0.041 -24.969 0.000 -0.637 -0.593 A3 0.682 0.037 18.381 0.000 0.429 0.353 A4 -1.132 0.043 -26.335 0.000 -0.711 -0.684 A5 1.292 0.049 26.573 0.000 0.812 0.709 Offenh =~ O1 1.000 0.505 0.447 O2 -1.711 0.069 -24.635 0.000 -0.865 -0.754 O3 0.707 0.040 17.883 0.000 0.357 0.356 O4 -1.404 0.058 -24.317 0.000 -0.710 -0.633 O5 0.830 0.040 20.584 0.000 0.420 0.444 SDV =~ Geschlecht 1.000 0.109 0.226 Alter -0.781 2.405 -0.325 0.745 -0.085 -0.006 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Extrav ~ SDV 3.600 0.312 11.530 0.000 0.504 0.504 Offenh ~ SDV 0.998 0.125 7.971 0.000 0.216 0.216 Neurot ~ Extrav -0.469 0.033 -14.156 0.000 -0.362 -0.362 SDV 1.209 0.260 4.654 0.000 0.131 0.131 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Gewiss ~~ Vertra -0.057 0.008 -6.893 0.000 -0.141 -0.141 SDV 0.023 0.003 7.969 0.000 0.325 0.325 Vertra ~~ SDV -0.060 0.005 -11.499 0.000 -0.878 -0.878 .Neurot ~~ .Offenh -0.087 0.010 -8.670 0.000 -0.184 -0.184 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .E1 0.935 0.021 43.806 0.000 0.935 0.605 .E2 0.981 0.023 42.633 0.000 0.981 0.564 .E3 0.698 0.018 38.052 0.000 0.698 0.447 .E4 0.848 0.020 42.508 0.000 0.848 0.560 .E5 0.632 0.019 34.152 0.000 0.632 0.378 .N1 0.713 0.028 25.547 0.000 0.713 0.410 .N2 0.906 0.022 41.094 0.000 0.906 0.650 .N3 0.651 0.020 32.073 0.000 0.651 0.499 .N4 1.316 0.028 47.812 0.000 1.316 0.878 .C1 0.805 0.021 39.166 0.000 0.805 0.662 .C2 1.314 0.032 41.019 0.000 1.314 0.701 .C3 0.876 0.019 46.889 0.000 0.876 0.869 .C4 0.943 0.026 36.066 0.000 0.943 0.605 .C5 0.963 0.027 35.743 0.000 0.963 0.599 .A1 1.499 0.033 45.691 0.000 1.499 0.792 .A2 0.748 0.018 40.944 0.000 0.748 0.648 .A3 1.293 0.027 47.692 0.000 1.293 0.876 .A4 0.574 0.016 35.060 0.000 0.574 0.532 .A5 0.652 0.020 32.951 0.000 0.652 0.498 .O1 1.025 0.023 45.130 0.000 1.025 0.801 .O2 0.568 0.024 23.842 0.000 0.568 0.432 .O3 0.880 0.019 47.224 0.000 0.880 0.873 .O4 0.752 0.021 35.507 0.000 0.752 0.599 .O5 0.719 0.016 45.218 0.000 0.719 0.803 .Geschlecht 0.223 0.005 47.896 0.000 0.223 0.949 .Alter 227.290 4.546 49.999 0.000 227.290 1.000 .Extrav 0.455 0.025 17.927 0.000 0.746 0.746 .Neurot 0.922 0.037 24.926 0.000 0.900 0.900 Gewiss 0.411 0.022 18.463 0.000 1.000 1.000 Vertra 0.395 0.027 14.781 0.000 1.000 1.000 .Offenh 0.244 0.017 13.959 0.000 0.953 0.953 SDV 0.012 0.002 5.997 0.000 1.000 1.000 In der Ergebnisausgabe sehen wir detailliert die geschätzten Koeffizienten für die verschiedenen Pfade. Außerdem werden durch das Argument fit.measures = TRUE zusätzlich Indizes der Modellpassung (wie AIC, BIC, CFI und RMSEA) ausgegeben. Alternativ zum lavaan Package kann auch hier das Package sem verwendet werden. "],["ergebnisse-exportieren.html", "Kapitel 11 Ergebnisse exportieren 11.1 Einführung 11.2 Berichte 11.3 Tabellen", " Kapitel 11 Ergebnisse exportieren 11.1 Einführung Nachdem die Analysen berechnet sind, möchte man die Ergebnisse davon gerne in das Programm exportieren, in dem die man die eigentliche Arbeit schreibt. Bevor wir uns in den folgenden Kapiteln anschauen, wie das genau funktioniert, müssen wir zuvor ein paar Begrifflichkeiten klären. R besitzt eine nahezu perfekte Möglichkeit, die Ergebnisse mithilfe des sogenannten rMarkdowns in verschiedene Formate umzuwandeln. Genauer gesagt kann ein spezielles R Dokument (rMarkdown) direkt in HTML, PDF oder ein Word Dokument umgewandelt werden. HTML. Das sogenannte Hypertext Markup Language (HTML) Format ist das Rückrad des Internets. Es gibt die Form von Internetseiten vor, die von Browsern (z.B. Firefox, Chrome oder Brave) gelesen werden. Dadurch ist es sehr praktisch zum gegenseitigen Teilen von Inhalten, da jeder einen Browser auf dem Computer installiert hat. Man benötigt zum Umwandeln von rMarkdown in HTML auch keinerlei zusätzliche Software. PDF. Das PDF Format wird durch LaTeX erstellt. LaTeX ist ein Programm, was in abgeänderter Form bereits seit Ende der 70er Jahre vor allem von Naturwissenschaftlern für die Erstellung wissenschaftlicher Arbeiten verwendet wird. Es ist also notwendig, dieses Programm auf dem Computer installiert zu haben, wenn man seine Analyse in ein PDF umwandeln möchte. Die Installation wird in den Folgekapitel an geeigneter Stelle erläutert. Word. Die Erstellung von Word Dokumenten funktioniert hinter den Kulissen durch Pandoc. Glücklicherweise musst du Dir um die Installation seit RStudio 1.3 keine Sorgen mehr machen, da es bereits vorinstalliert ist. Grundsätzlich sind Word Dokumente immer ein kleines Sorgenkind, da es sich hierbei nicht um ein frei zugängliches Format handelt. Deswegen ist es für Package-Entwickler auch deutlich schwieriger, dafür Erweiterungen zu entwickeln. Komplexe Tabellen sind daher in HTML und PDF deutlich schöner als in Word. Tatsächlich wird dort kein .docx Dokument sondern ein .rtf (Rich Text Format) kreiert. Dies erlaubt den Entwicklern etwas mehr Flexibilität. Eine Möglichkeit für Word-Nutzer besteht darin, die Werte mit rMarkdown umzuwandeln und alles weitere, potentiell komplexere dann direkt in Word anzupassen. rMarkdown. Markdown ist an sich eine Möglichkeit, einfacher und intuitiver HTML Inhalte zu schreiben (z.B. Texte für Internetseiten). Man kann beispielsweise ohne viel Aufwand Wörter fett oder kursiv schreiben und Abbildungen integrieren. rMarkdown erweitert dies um einige Funktionen und ermöglicht es uns, direkt aus R verschiedene Dokumenttypen zu erstellen. Das rmarkdown Package ist dabei direkt in RStudio integriert. Es handelt sich hierbei nicht um ein R Skript, sondern um einen eigenen Dateityp, welcher mit .Rmd und nicht mit .R endet. 11.2 Berichte Für das Kreieren der Tabellen im Rahmen dieses Kapitel wird das knitr Package benötigt. library(knitr) Öffne zum Erstellen von rMarkdown (.Rmd) Dokumenten das Dropdown-Menü mit dem Papier und dem grünen Plus-Zeichen unter dem Reiter File. Abbildung 11.1: Erster Schritt in der Erstellung eines neuen R Markdown Skripts. Wähle dort R Markdown aus. Die Punkte implizieren, dass noch weitere Informationen vor dem Erstellen notwendig sind. Anschließen kann man den gewünschten Ausgabetypen, Titel und Autor festlegen. Beim Klicken auf OK wird eine Vorlage erstellt, die man nach belieben anpassen kann. Abbildung 11.2: Zweiter Schritt zur Erstellung eines neuen R Markdown Skripts. Die wichtigsten Formatierungsmöglichkeiten in Markdown sehen wie folgt aus: Fett gedruckt: **fett** ergibt fett Kursiv: *kursiv* ergibt kursiv Code Integration im Text: `mean(x)` ergibt mean(x) Links: [Klicke hier](https://cran.r-project.org/) wird zu: Klicke hier (im gebundenen Buch sind Links nicht extra farblich hervorgehoben) Überschriften können mit einer führenden Raute (# Überschrift 1) erstellt werden. Für Abschnitte innerhalb des Hauptkapitels können beliebig viele Rauten hinzugefügt werden (z.B. ## Unterkapitel 1). In der Praxis sähe das wie folgt aus:  Beginn der Datei Beispiel.Rmd  --- title: &quot;Hausaufgaben&quot; author: &quot;Jan Philipp Nolte&quot; date: &quot;20.11.2020&quot; output: html_document --- ```{r setup, include = FALSE} library(tidyverse) library(knitr) library(remp) data(big_five) ``` ## Aufgabe 1 Berechne die deskriptiven Statistiken der *numerischen* Variablen Extraversion und Neurotizismus des Datensatzes `big_five`. ```{r} big_five %&gt;% select(Extraversion, Neurotizismus) %&gt;% descriptive() %&gt;% kable() ``` ## Aufgabe 2 Visualisiere den Zusammenhang zwischen **Extraversion** und **Neurotizismus**. ```{r, echo = FALSE} ggplot(big_five, aes(x = Extraversion, y = Neurotizismus)) + geom_point(position = &quot;jitter&quot;) ```  Ende der Datei Beispiel.Rmd  Die Ausgabe nach Umwandlung in ein HTML Dokument ist in Abbildung 11.3 zu sehen. Abbildung 11.3: Umwandlung von Markdown in HTML. Jedes rMarkdown Dokument (Endung .Rmd) beginnt mit einem so genannten YAML-Kopf, der durch drei Bindestriche oben und unten vom restlichen Dokument abgegrenzt ist. In diesem Beispiel wird der Titel, Autor, das Datum und Ausgabeformat festgelegt. Für weitere Anpassungsoptionen sei auf das Buch R Markdown - The Definitive Guide von Xie, Allaire und Grolemund verwiesen. Als nächstes sehen wir die grau hinterlegten Code Abschnitte. Diese werde mithilfe von drei Backticks (```) erstellt und erlauben das Ausführen von R Code. Den Backticks folgen geschweifte Klammern. Die erste Information ist die verwendete Programmiersprache (hier R). Getrennt mit einem Komma können hier diverse weitere Argumente angegeben werden. Zum Beispiel kann mit echo = FALSE der Code versteckt werden. So könnte man ein Dokument erstellen, in dem nur die Ergebnisse in Form von Tabellen und kein R Code enthalten ist. Umwandeln können wir das rMarkdown Dokument durch Klicken auf das Wort Knit (engl. für stricken) in der Leiste unter dem Reiter der geöffneten Dateien. Wenn man nur auf das Symbol drückt, wird die Datei in das an erster Stelle im YAML Kopf festgelegten Format umgewandelt. Besser ist es jedoch, das Dropdown-Menü durch einen Klick auf den Pfeil nach unten zu öffnen und den gewünschten Dateityp auszuwählen. Abbildung 11.4: Umwandlung des R Markdown Skripts in HTML, PDF oder Word. Die eigentliche Umwandlung übernimmt das knitr Package, was im Hintergrund geladen wird. Zur Umwandlung zu HTML muss nichts weiter beachtet werden. Bei der Umwandlung in Word wird das Programm Pandoc (https://pandoc.org/installing.html) benötigt, welches in neueren RStudio Versionen direkt integriert ist. Möchte man in ein PDF umwandeln, muss eine Version von LaTeX auf dem Computer installiert sein. Die einfachste Möglichkeit hierfür ist die Installation des R Packages tinytex. library(tinytex) rMarkdown Dokumente können nur umgewandelt werden, wenn alle nötigen Informationen enthalten sind. Es müssen also innerhalb der Datei alle Packages und Datensätze explizit geladen werden. Dies trifft auch zu, obwohl man die Packages oder Datensätze möglicherweise bereits vorher verwendet hat. 11.3 Tabellen Während es selten ein Problem darstellt, die Ergebnisse einer einfach Regressionsanalyse tabellarisch im Textverarbeitungsprogramm des Vertrauens darzustellen, bereitet es schon deutlich weniger Freude, eine große Korrelationsmatrix oder diverse Ladungen im Rahmen einer Faktoranalyse händisch aus R in Word oder LaTeX zu übertragen. 11.3.1 Word, PDF und HTML Für die Tabellen benötigen wir auch hier das knitr Package, für die Umwandlung der Tabellen direkt als PDF brauchen wir tinytext. library(knitr) library(tinytext) --- title: &#39;Beispiel&#39; output: pdf_document: default html_document: default word_document: default fontsize: 12pt --- ```{r setup, include = FALSE} library(tidyverse) library(knitr) library(remp) data(big_five) ``` ```{r, echo=FALSE} big_five %&gt;% select(Extraversion, Neurotizismus) %&gt;% descriptive() %&gt;% kable() ``` Es gibt verschiedene Möglichkeiten, Tabellen im Word Format durch R Code zu erstellen. Wir werden uns hier auf das bereits in RStudio integrierte Pandoc und das in Kapitel 11.2 kennengelernte rMarkdown verlassen. Denn mit nur einer Funktion aus dem knitr Package, welches diverse Modifikationen für rMarkdown Dokumente bietet, können schöne Tabellen erstellt werden. Der Funktion kable() aus besagtem Package wird lediglich der Datensatz übergeben. Dann muss man nur noch das rMarkdown Dokument mithilfe von Pandoc in ein Word-Dokument umwandeln (siehe Kapitel 11.2. Abbildung 11.5: Beispielhafter Output einer Tabelle von R Markdown in Word. Das Aussehen kann innerhalb von Word dann entsprechend angepasst werden. Beachte die Breite einer gewöhnlichen Din A4 Seite in Word. Wenn du eine gigantische Korrelationsmatrix ausgeben möchtest, solltest du entweder eine Word Vorlage in Querformat verwenden (die Vorlage muss den gleichen Dateinamen wie das rMarkdown Dokument haben) oder die Tabelle in Teilen ausgeben lassen. 11.3.2 LaTeX Für dieses Kapitel wird das xtable Package benötigt. library(xtable) Es gibt auch die Möglichkeit, tibbles aus R direkt in LaTeX Code umzuwandeln, um diesen dann in die zugehörige TeX Datei zu kopieren. Zuvor wurde LaTeX nur hinter den Kulissen direkt zum Erstellen von PDFs verwendet. Wenn man die Arbeit direkt in LaTeX schreibt, benötigt man allerdings den tatsächlichen LaTeX Code. Dafür greift man auf die Funktion xtable() aus dem gleichnamigen Package zurück. Das wichtigste Argument der Funktion ist digits, mit dem man pro Spalte die Anzahl der gerundeten Nachkommastellen festlegt. Dabei kann man entweder eine einzelne Zahl eingeben und somit für alle Spalten die gleiche Rundung generieren oder für jede einzeln als Vektor mithilfe von c(). Da in der Funktion auch die Zeilennamen berücksichtigt werden, muss der Vektor immer um eins länger sein als die Anzahl der Spalten des eigentlichen Datensatzes. big_five %&gt;% select(Extraversion, Neurotizismus) %&gt;% descriptive() %&gt;% select(Variable, Min, Mean, Median, Max, SE) %&gt;% xtable(digits = c(0, 1, 2, 2, 2, 2, 3)) %&gt;% print(include.rownames = FALSE) % latex table generated in R 4.0.4 by xtable 1.8-4 package % Mon Apr 12 17:07:28 2021 \\begin{table}[ht] \\centering \\begin{tabular}{lrrrrr} \\hline Variable &amp; Min &amp; Mean &amp; Median &amp; Max &amp; SE \\\\ \\hline Extraversion &amp; 2.30 &amp; 3.08 &amp; 3.00 &amp; 4.30 &amp; 0.020 \\\\ Neurotizismus &amp; 1.40 &amp; 3.13 &amp; 3.10 &amp; 4.60 &amp; 0.050 \\\\ \\hline \\end{tabular} \\end{table} Die Funktion print() mit dem Argument include.rownames verhindert, dass jede Zeile im Datensatz nummeriert ausgegeben wird. 11.3.3 Komplexe Tabellen Für dieses Kapitel muss das gt Package installiert werden. Es handelt sich hierbei noch um ein relativ junges Package, weshalb leichte Änderungen des im Folgenden gezeigten Codes nicht auszuschließen sind. library(gt) Das gt Package verfolgt dabei eine ähnliche Philosophie wie ggplot für Abbildungen. Anstelle einer grammar of graphics steht gt allerdings für grammar of tables. Wir haben also auch hier verschiedene Einzelkomponenten, die wir in nahezu beliebiger Reihenfolge je nach Bedarf hinzufügen können. In Abbildung 11.6 ist der grundsätzliche Aufbau einer derartigen Tabelle gezeigt. Die Begriffe sind an dieser Stelle auf Englisch belassen, da die einzelnen Funktionen (wie dort ebenfalls ersichtlich) die Bezeichnungen direkt enthalten. Abbildung 11.6: Vereinfachte Darstellung des Aufbaus einer Tabelle. Nun erstellen wir uns zuerst einige deskriptiven Statistiken aus unserem big_five Datensatz. descr &lt;- big_five %&gt;% select(Geschlecht, Extraversion, Neurotizismus) %&gt;% group_by(Geschlecht) %&gt;% descriptive() %&gt;% ungroup() %&gt;% select(Variable, Geschlecht, Min, Mean, Median, Max, SE) %&gt;% mutate(Geschlecht = if_else( condition = Geschlecht == &quot;f&quot;, true = &quot;Weiblich&quot;, false = &quot;Männlich&quot;) ) descr # A tibble: 4 x 7 Variable Geschlecht Min Mean Median Max SE &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion Weiblich 2.4 3.05 3 4.3 0.03 2 Neurotizismus Weiblich 1.4 3.25 3.3 4.6 0.06 3 Extraversion Männlich 2.3 3.11 3 4.1 0.04 4 Neurotizismus Männlich 1.6 2.96 2.9 4.6 0.08 Von hier an sind die Ähnlichkeiten zu ggplot unverkennbar. Zuerst rufen wir die Funktion gt() auf, der wir entweder das Argument rowname_col oder groupname_col() übergeben können  je nachdem wie wir die Werte in der Tabelle gruppiert haben möchten. Ab dem Punkt sind die Funktionen selbsterklärend, wenn man vergleichend Abbildung 11.6 betrachtet. Wichtig ist an der Stelle nur, dass die ausgewählten Spalten der vars() Funktion übergeben werden müssen. descr %&gt;% gt(rowname_col = &quot;Geschlecht&quot;) %&gt;% tab_spanner( label = &quot;Mittlere Lage&quot;, columns = vars(Mean, Median) ) %&gt;% tab_stubhead(label = &quot;Geschlecht&quot;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #imhxmgaxkg .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #imhxmgaxkg .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #imhxmgaxkg .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #imhxmgaxkg .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #imhxmgaxkg .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #imhxmgaxkg .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #imhxmgaxkg .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #imhxmgaxkg .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #imhxmgaxkg .gt_column_spanner_outer:first-child { padding-left: 0; } #imhxmgaxkg .gt_column_spanner_outer:last-child { padding-right: 0; } #imhxmgaxkg .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #imhxmgaxkg .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #imhxmgaxkg .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #imhxmgaxkg .gt_from_md > :first-child { margin-top: 0; } #imhxmgaxkg .gt_from_md > :last-child { margin-bottom: 0; } #imhxmgaxkg .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #imhxmgaxkg .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #imhxmgaxkg .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #imhxmgaxkg .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #imhxmgaxkg .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #imhxmgaxkg .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #imhxmgaxkg .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #imhxmgaxkg .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #imhxmgaxkg .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #imhxmgaxkg .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #imhxmgaxkg .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #imhxmgaxkg .gt_sourcenote { font-size: 90%; padding: 4px; } #imhxmgaxkg .gt_left { text-align: left; } #imhxmgaxkg .gt_center { text-align: center; } #imhxmgaxkg .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #imhxmgaxkg .gt_font_normal { font-weight: normal; } #imhxmgaxkg .gt_font_bold { font-weight: bold; } #imhxmgaxkg .gt_font_italic { font-style: italic; } #imhxmgaxkg .gt_super { font-size: 65%; } #imhxmgaxkg .gt_footnote_marks { font-style: italic; font-size: 65%; } Geschlecht Variable Min Mittlere Lage Max SE Mean Median Weiblich Extraversion 2.4 3.05 3.0 4.3 0.03 Weiblich Neurotizismus 1.4 3.25 3.3 4.6 0.06 Männlich Extraversion 2.3 3.11 3.0 4.1 0.04 Männlich Neurotizismus 1.6 2.96 2.9 4.6 0.08 Alternativ wird mit dem groupname_col Argument eine separate Zeile für die Gruppennamen eingefügt. table &lt;- descr %&gt;% gt(groupname_col = &quot;Geschlecht&quot;) %&gt;% tab_spanner( label = &quot;Mittlere Lage&quot;, columns = vars(Mean, Median) ) %&gt;% tab_footnote( footnote = &quot;Mittlere Ausprägung von Neurotizismus&quot;, locations = cells_body( columns = vars(Variable), rows = 2) ) %&gt;% tab_footnote( footnote = &quot;Standardfehler&quot;, locations = cells_column_labels( columns = vars(SE)) ) table html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nqzaabeabo .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #nqzaabeabo .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nqzaabeabo .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #nqzaabeabo .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #nqzaabeabo .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nqzaabeabo .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nqzaabeabo .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #nqzaabeabo .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #nqzaabeabo .gt_column_spanner_outer:first-child { padding-left: 0; } #nqzaabeabo .gt_column_spanner_outer:last-child { padding-right: 0; } #nqzaabeabo .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #nqzaabeabo .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #nqzaabeabo .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #nqzaabeabo .gt_from_md > :first-child { margin-top: 0; } #nqzaabeabo .gt_from_md > :last-child { margin-bottom: 0; } #nqzaabeabo .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nqzaabeabo .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nqzaabeabo .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nqzaabeabo .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nqzaabeabo .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nqzaabeabo .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nqzaabeabo .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #nqzaabeabo .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nqzaabeabo .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nqzaabeabo .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #nqzaabeabo .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nqzaabeabo .gt_sourcenote { font-size: 90%; padding: 4px; } #nqzaabeabo .gt_left { text-align: left; } #nqzaabeabo .gt_center { text-align: center; } #nqzaabeabo .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nqzaabeabo .gt_font_normal { font-weight: normal; } #nqzaabeabo .gt_font_bold { font-weight: bold; } #nqzaabeabo .gt_font_italic { font-style: italic; } #nqzaabeabo .gt_super { font-size: 65%; } #nqzaabeabo .gt_footnote_marks { font-style: italic; font-size: 65%; } Variable Min Mittlere Lage Max SE1 Mean Median Weiblich Extraversion 2.4 3.05 3.0 4.3 0.03 Neurotizismus2 1.4 3.25 3.3 4.6 0.06 Männlich Extraversion 2.3 3.11 3.0 4.1 0.04 Neurotizismus 1.6 2.96 2.9 4.6 0.08 1 Standardfehler 2 Mittlere Ausprägung von Neurotizismus Als weiteres Beispiel schauen wir uns an, wie wir die Ladungen einer explorativen Faktorenanalyse als Tabelle exportieren können. Dafür erstellen wir im ersten Schritt die Faktorenanalyse aus Kapitel 10.2 und formatieren die Ladungen mit der Funktion tidy() aus dem broom Package in einen tibble. table1 &lt;- big_five_comp %&gt;% select(E1:E3, N1:N3, A1:A3, C1:C3, O1:O3) %&gt;% factanal(factors = 5) %&gt;% tidy() table1 # A tibble: 15 x 7 variable uniqueness fl1 fl2 fl3 fl4 fl5 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 E1 0.561 0.646 -0.100 0.0220 -0.00242 0.105 2 E2 0.624 -0.602 0.0182 -0.0923 0.0617 0.0151 3 E3 0.350 0.729 -0.255 -0.0250 0.160 -0.164 4 N1 0.414 -0.0648 0.753 -0.0479 -0.0979 0.0615 # ... with 11 more rows Der Übersichtlichkeit halber runden wir im ersten Schritt mithilfe von mutate() (siehe Kapitel 6.4) auf zwei Nachkommastellen. Anschließend schreiben wir einen NA Wert (fehlender Wert) in jede Zelle, welche eine Ladung mit einem Betrag (abs()) von weniger als 0.3 hat. Wir möchten schließlich die positiven wie negativen Ladungen mit einer Ladung von größer als 0.3 ausgegeben haben. table1 &lt;- table1 %&gt;% mutate(across(uniqueness:fl5, round, 2)) %&gt;% mutate(across(fl1:fl5, ~ if_else( condition = abs(.x) &lt; 0.3, true = NA_real_, false = .x)) ) table1 %&gt;% print(n = 10) # A tibble: 15 x 7 variable uniqueness fl1 fl2 fl3 fl4 fl5 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 E1 0.56 0.65 NA NA NA NA 2 E2 0.62 -0.6 NA NA NA NA 3 E3 0.35 0.73 NA NA NA NA 4 N1 0.41 NA 0.75 NA NA NA 5 N2 0.66 NA -0.55 NA NA NA 6 N3 0.45 NA 0.74 NA NA NA 7 A1 0.74 NA NA NA NA 0.47 8 A2 0.64 0.49 NA NA NA -0.32 9 A3 0.69 NA NA NA NA 0.49 10 C1 0.61 NA NA NA 0.6 NA # ... with 5 more rows Abschließend müssen wir die Ladungen noch in den Datentyp Character umwandeln und dann jede der fehlenden Werte (NAs) mit einem leeren Character () ersetzen. Für das eigentliche Erstellen der Tabelle genügt dann die Funktion gt(). table1 %&gt;% mutate(across(fl1:fl5, as.character)) %&gt;% mutate(across(fl1:fl5, ~ replace_na(.x, &quot;&quot;))) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qsllrofxtf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qsllrofxtf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qsllrofxtf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qsllrofxtf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #qsllrofxtf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qsllrofxtf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qsllrofxtf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qsllrofxtf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qsllrofxtf .gt_column_spanner_outer:first-child { padding-left: 0; } #qsllrofxtf .gt_column_spanner_outer:last-child { padding-right: 0; } #qsllrofxtf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #qsllrofxtf .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #qsllrofxtf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qsllrofxtf .gt_from_md > :first-child { margin-top: 0; } #qsllrofxtf .gt_from_md > :last-child { margin-bottom: 0; } #qsllrofxtf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qsllrofxtf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #qsllrofxtf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qsllrofxtf .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #qsllrofxtf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qsllrofxtf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qsllrofxtf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qsllrofxtf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qsllrofxtf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qsllrofxtf .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #qsllrofxtf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qsllrofxtf .gt_sourcenote { font-size: 90%; padding: 4px; } #qsllrofxtf .gt_left { text-align: left; } #qsllrofxtf .gt_center { text-align: center; } #qsllrofxtf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qsllrofxtf .gt_font_normal { font-weight: normal; } #qsllrofxtf .gt_font_bold { font-weight: bold; } #qsllrofxtf .gt_font_italic { font-style: italic; } #qsllrofxtf .gt_super { font-size: 65%; } #qsllrofxtf .gt_footnote_marks { font-style: italic; font-size: 65%; } variable uniqueness fl1 fl2 fl3 fl4 fl5 E1 0.56 0.65 E2 0.62 -0.6 E3 0.35 0.73 N1 0.41 0.75 N2 0.66 -0.55 N3 0.45 0.74 A1 0.74 0.47 A2 0.64 0.49 -0.32 A3 0.69 0.49 C1 0.61 0.6 C2 0.77 -0.43 C3 0.72 0.47 O1 0.63 0.6 O2 0.69 -0.51 O3 0.80 0.43 Alternativ könnte man natürlich auch anstelle von gt() die in Kapitel 11.3.1 eingeführte Funktion kable() verwenden und diese dann mithilfe von rMarkdown entsprechend exportieren. Das Speichern einer gt() Tabelle läuft hingegen ähnlich ab, wie das Speichern einer Abbildung des ggplot Packages. Anstelle von ggsave() verwenden wir hier gtsave(). Dabei kann man sich entscheiden, ob die Tabelle als LaTeX (.tex), Word (.rtf) oder HTML Datei ausgegeben wird. Beachte an der Stelle, dass die Ausgabe als Word Datei derzeitig noch einige optische Mängel aufweist. gtsave(table, &quot;tabelle.tex&quot;) gtsave(table, &quot;tabelle.rtf&quot;) gtsave(table, &quot;tabelle.html&quot;) Möchte man die als .tex gespeicherte Tabelle nun in LaTeX einfügen, muss man sich mithilfe von gt_latex_dependencies() über die notwendigen Packages für die Tabelle bewusst werden. Für die hier erstellte Tabelle würde ein minimales Beispiel wie folgt aussehen. \\documentclass[a4paper,12pt]{article} \\usepackage[utf8]{inputenc} \\usepackage[T1]{fontenc} \\usepackage{amsmath} \\usepackage{booktabs} \\usepackage{caption} \\usepackage{longtable} \\begin{document} % Hier Tabelle einfügen \\end{document} An der Stelle des Prozentzeichens würde man dann die gespeicherte Tabelle einfügen. "],["datatypes.html", "Kapitel 12 Datenstrukturen 12.1 Vektor 12.2 Matrix 12.3 Data.frame und tibble 12.4 Liste 12.5 Umwandlungen 12.6 Objektorientierung", " Kapitel 12 Datenstrukturen Es mag Dir bereits aufgefallen sein, dass immer wieder Verweise auf dieses Kapitel im Verlauf des Buches gemacht wurden. Noch vor einigen Jahren gab es für niemanden einen Zweifel daran, dass Datenstrukturen in Programmiersprachen direkt zu Beginn gelehrt werden sollten. Wenn du es bis hierher geschafft hast, wirst du jedoch gemerkt haben, auch prima ohne diese Grundlagen der Programmiersprache zurecht gekommen zu sein. Trotzdem ist ein Verständnis der verschiedenen Datenstrukturen, wie man darauf zugreift und diese ineinander umwandelt, eine essentielle Fertigkeit, wenn man sich tiefer mit R beschäftigt. Das tidyverse bietet viele Funktionen, um diesen klassischen in base R implementierten Umgang mit Datenstrukturen zu umgehen. Das hat vor allem Gründe der Inkonsistenz und unpraktischen Designentscheidungen, die aus historischen Gründen bestehen. R gibt es mittlerweile seit über 25 Jahren. Nicht alles, was aus damaliger Sicht Sinn gemacht hat, ist in der Form heute auch noch vernünftig für Anwender. Für Anwendungen, die über das tidyverse hinaus gehen, ist es unabdingbar, dieses Wissen zu haben. Wir schauen uns nun die Datenstrukturen und wie man auf sie zugreift der Reihe nach an. 12.1 Vektor Manche sagen, alle Datenstrukturen in R sind Objekte. Während das zwar grundsätzlich korrekt ist, ist es verwirrend im Sinne dessen, was normalerweise mit Objektorientierung gemeint ist. Deswegen gehen wir an dieser Stelle eine Abstraktionsstufe hinunter und sagen guten Gewissens: Alles in R ist ein Vektor. Ein Skalar, also ein einzelner Zahlenwert, wie zum Beispiel 42 existiert demnach in R nicht. Wenn man 42 in R in einer Variable speichert, hat man einen Vektor der Länge 1. Vektoren sind uns im Verlaufe des Buches schon oft begegnet. Jede Spalte innerhalb eines tibbles ist für sich genommen ein Vektor. Damit kommen wir direkt zur wichtigsten Besonderheit. Vektoren können immer nur einen Datentypen enthalten. Wenn eine Zahl mit einem Wort kombiniert wird, wird der ganze Werte zum Typ Character umgewandelt. Einen Vektor kann man auf verschiedene Art und Weise erstellen. Beispielsweise mit vector(), rep(), seq() oder c(). Die für uns wichtigste ist die bereits verwendete Combine Funktion c(). Alle mit einem Komma getrennten Argumente innerhalb von c() werden aneinander gebunden und als Vektor ausgegeben. c(1, 3, 2, 4) [1] 1 3 2 4 Auch bereits verwendet wurde der Doppelpunkt als Äquivalent für von Zahl 1 bis Zahl 2. Zahlen in geordneter Reihenfolge von 1 bis 4 währen demnach 1:4 [1] 1 2 3 4 Auf Elemente (hier Werte) innerhalb eines Vektors kann mithilfe von eckigen Klammern zugegriffen werden. Möchte man beispielsweise nur das dritte Element des Vektors c(1, 3, 2, 4) gespeichert in der Variable vec ausgeben lassen, würde man vec[3] [1] 2 schreiben. Da diese 3 im Prinzip auch nur ein Vektor der Länge 1 ist, kann diese Syntax auch mit c() und dem Doppelpunkt kombiniert werden, um mehrere Elemente ausgeben zu lassen. vec[c(1, 4)] vec[1:2] Probiere es ruhig selbst einmal aus und spiele ein wenig damit herum. 12.2 Matrix Wenn man nun mehrere Vektoren eines Datentyps aneinander bindet, erhält man eine Matrix. Für zeilenweises Binden der Vektoren verwendet mit rbind() (für row bind). Dabei müssen die Vektoren die selbe Länge haben. rbind( c(1, 3, 2, 4), 1:4 ) [,1] [,2] [,3] [,4] [1,] 1 3 2 4 [2,] 1 2 3 4 Es können im Übrigen auf diese Weise auch tibble zusammen gebunden werden. Das spaltenweise Äquivalent ist cbind() (für column bind). cbind( c(1, 3, 2, 4), 1:4 ) [,1] [,2] [1,] 1 1 [2,] 3 2 [3,] 2 3 [4,] 4 4 Seltener in der Datenanalyse benutzt, aber trotzdem manchmal nützlich, ist der matrix() Befehl. Als Argumente müssen der Vektor, die Anzahl der Zeilen oder Spalten sowie die Information übergeben werden, ob es zeilenweise (byrow) abgebildet werden soll. matrix( 1:9, ncol = 3, byrow = TRUE ) [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 Angenommen, die vorhergehende Matrix sei in der Variable mat gespeichert. Da nun zwei Dimensionen involviert sind, müssen zum Zugreifen auf Elemente innerhalb der Matrix auch zwei Parameter berücksichtigt werden: die Spalten- und Zeilenposition. Dabei werden innerhalb der eckigen Klammern getrennt von einem Komma zuerst die Zeilen und dann die Spalten angegeben. Möchte man den Wert aus Zeile 2 und Spalte 3 erhalten, würde man [2, 3] an den Variablennamen hängen. mat[2, 3] [1] 6 Wenn man eine ganze Zeile oder Spalte zurückgeben möchte, lässt man schlichtweg das auszulassende Argument weg. Für die erste Zeile schreibt man folglich: mat[1, ] [1] 1 2 3 Das Leerzeichen nach dem Komma ist zwar nicht zwingend notwendig, allerdings macht es deutlich, dass dort ein zweiter Wert fehlt. Auch das Zugreifen auf Elemente einer Matrix kann, wie bereits im Kontext der Vektoren in Kapitel 12.1 beschrieben, mit c() und dem Doppelpunkt auf mehr als eine Auswahl erweitert werden. Genau wie Vektoren können auch in Matrizen nur Daten von einem Typ gespeichert werden. Bei Vermischung von Datentypen werden automatisch die gleichen Umwandlungsregelungen angewandt. Außerdem müssen die Vektoren innerhalb der Matrix die selbe Länge haben. Aufgrund der Limitation, nur einen Datentyp enthalten zu können, fällt die Matrix als Datenstruktur in der gewöhnlichen Datenanalyse in der Regel aus. Deswegen bedarf es einer allgemeineren Datenstruktur, den data.frames und tibbles. 12.3 Data.frame und tibble Bevor wir auf die Datenstrukturen eingehen, sollte erst einmal geklärt werden, was es mit der Unterscheidung zwischen data.frame und tibble auf sich hat. Grundsätzlich ist der Zweck und Einsatz beider Strukturen fast identisch. Es gibt aber doch ein paar sehr nützlich Erweiterungen von tibbles gegenüber data.frames. Ein data.frame ist ein Datenformat, was in base R integriert ist. tibbles hingegen sind aus dem gleichnamigen tibble Package, welches im tidyverse enthalten ist. Der wohl praktischste Vorteil ist die übersichtlichere Ausgabe. Es werden nur 10 Zeilen ausgegeben, auf einen Blick sieht man die Datentypen der Spalten und die Dimensionen des Datensatzes. Außerdem sind die Zahlen zur besseren Übersichtlichkeit entsprechend eingerückt und negative Werte rot hervorgehoben. Das automatische Runden von tibbles bei der Anzeige ist hingegen nicht immer ein Vorteil. Während es beim explorativen Anschauen der Daten sehr praktisch ist, muss beim deskriptiven oder inferenzstatistischen Betrachten der Daten eine bestimmte Anzahl von Kommastellen sichtbar sein, um sie in einer wissenschaftlichen Arbeit einzusetzen. Grundsätzlich können Funktionen, die auf data.frames angewendet werden können, abgesehen von wenigen Ausnahmen auch auf tibbles angewendet werden. Im Folgenden werden wir nur noch von tibbles reden, da alles Gesagte auch für data.frames gilt. tibbles sind wie Matrizen mit dem Unterschied, dass jede Spalte einen anderen Datentyp beinhalten darf. Innerhalb jeder Spalte muss der Datentyp trotzdem gleich bleiben, da jede diese Spalten letztlich ein Vektor ist. Einen tibble selbst zu erstellen kommt in der gewöhnlichen Datenanalyse denkbar selten vor. Schließlich erhebt man die Daten und schreibt sie gewöhnlich in eine Form von Spreadsheets. Es ist eher umständlich, einen ganzen Datensatz manuell direkt in R einzutragen. Im Vergleich zum Erstellen von Matrizen mithilfe von rbind() oder cbind ändert sich im Grunde nicht viel. Die Funktion tibble() erstellt den tibble. Vor dem Gleichheitszeichen kann eine Name zugewiesen werden. Eine Besonderheit von tibbles ist die Möglichkeit, direkt im Erstellungsprozess für das Erstellen einer weiteren Spalte, eine vorher erstellte Spalte zu verwenden. tibble( a = 1:5, b = 6:10, sum = a + b ) # A tibble: 5 x 3 a b sum &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 6 7 2 2 7 9 3 3 8 11 4 4 9 13 # ... with 1 more row Wichtig sind die verschiedenen Arten, auf einen tibble zuzugreifen. Den Großteil des Buches wurde die Helferfunktion select() aus dem tidyverse verwendet. Der Grund, weshalb wir andere Möglichkeiten kennenlernen müssen, ist der, dass select() immer einen tibble zurückgibt. Während das in den meisten Fällen das gewünschte Verhalten ist, gibt es einige Funktionen, denen man die Spalte als Vektor übergeben muss. Wir haben das beispielsweise bereits im Kontext von Kontingenztafeln kennengelernt, bei denen wir mit dem so genannten Dollar-Operator eine Spalte aus dem Datensatz herausgeholt haben. Die Syntax ist dabei allgemeingesprochen datensatzname$spaltenname. Angenommen, der zur erstellte tibble sei als tib gespeichert. Um die Spalte sum aus dem Datensatz als Vektor ausgeben zu lassen, würden wir [1] 7 9 11 13 15 schreiben. Letztlich ist der Dollar-Operator technisch gesehen ein Shortcut für datensatzname[[\"spaltenname\"]]. Im Kontext reiner Datenanalyse kann allerdings guten Gewissens die kurze Version mit dem Dollar-Zeichen verwendet werden. Neben dieser beiden Möglichkeiten zur Ausgabe einer einzelnen Spalte kann ebenfalls wie bei Matrizen mit einfachen eckigen Klammern auf ganze Zeilen zugegriffen werden. Hier gibt es einen weiteren Unterschied zwischen data.frames und tibbles. Während die Standardeinstellung von data.frames dafür sorgt, dass bei Auswahl nur einer Spalte (zum Beispiel tib[ ,1]) ein Vektor zurückgegeben wird, gibt ein tibble immer auch einen tibble zurück. Eine weitere Besonderheit im Vergleich zu Matrizen (sofern deren Spalten unbenannt sind) besteht darin, dass innerhalb der eckigen Klammern auch die Spaltennamen als Character übergeben werden können. tib[ ,c(&quot;a&quot;, &quot;b&quot;)] # A tibble: 5 x 2 a b &lt;int&gt; &lt;int&gt; 1 1 6 2 2 7 3 3 8 4 4 9 # ... with 1 more row Allerdings bringt uns das in dem Fall keinerlei Vorteil zum bereits kennengelernten select(). 12.4 Liste Listen sind der allgemeinste Datentyp. Tatsächlich sind data.frames nur eine besondere Art von Listen. Etwas kontraintuitiv resultiert daraus, dass Listen am wenigsten Zeit bei Berechnungen benötigen. Deswegen wird eine listenorientierte Programmierung mithilfe von Variationen von map() in Kapitel 13.2 eingeführt. Ein Listenelement kann jede Datenstruktur enthalten  sogar ganze tibbles. Beim Erstellen ändert sich der Befehl zu list(). list( Vektor = vec, Matrix = mat, Tibble = tib ) $Vektor [1] 1 3 2 4 $Matrix [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 $Tibble # A tibble: 5 x 3 a b sum &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 6 7 2 2 7 9 3 3 8 11 4 4 9 13 # ... with 1 more row Die Zeichen vor dem Gleichheitszeichen sind dabei die Namen der Listenelemente, die man zum Abrufen verwenden kann. Mit Listen haben wir nun bis zu drei Dimensionen. Die verschiedenen Elemente innerhalb der Liste, die wiederum zwei-dimensionale tibbles enthalten können. Es ist sogar möglich, Listen innerhalb von Listen zu haben. Im Rahmen des Einnistens greifen wir den Gedanken in Kapitel 13.3 wieder auf. Das Prinzip beim Zugreifen ändert sich nicht. Nur die Anzahl der Dimensionen steigt. Würde man also auf den tibble der eben erstellen Liste ls zugreifen wollen, könnte man ls$Tibble oder ls[[3]] schreiben. Möchte man direkt auf Elemente innerhalb des tibbles zugreifen, kann man auf übliche Weise darauf zugreifen. ls[[3]][1 ,2] # A tibble: 1 x 1 b &lt;int&gt; 1 6 Eine der Liste sehr ähnliche Struktur ist der Array, auf den aufgrund seltenen Nutzens im Kontext der Datenanalyse an dieser Stelle verzichtet sei. Eine besondere Art der Liste ist der tibble. Daher können wir grundsätzlich in eine Zelle nicht nur Zahlen oder Buchstaben hineinschreiben, sondern sogar ganze andere Datensätze darin verstecken. df &lt;- tibble( a = c(1, 2, 3), b = list( tibble(a = c(1, 2, 3, 4), b = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;)), tibble(x = 4:5, y = 6:7), Number = 1 ) ) df # A tibble: 3 x 2 a b &lt;dbl&gt; &lt;named list&gt; 1 1 &lt;tibble [4 x 2]&gt; 2 2 &lt;tibble [2 x 2]&gt; 3 3 &lt;dbl [1]&gt; Wie du siehst sind in der Spalte b nun in den ersten zwei Zeilen tibbles enthalten. Wenn wir mit df$b oder df[[2]] nur diese Spalte anschauen, sehen wir eine Liste als Ausgabe. df[[2]] [[1]] # A tibble: 4 x 2 a b &lt;dbl&gt; &lt;chr&gt; 1 1 m 2 2 f 3 3 f 4 4 m [[2]] # A tibble: 2 x 2 x y &lt;int&gt; &lt;int&gt; 1 4 6 2 5 7 $Number [1] 1 12.5 Umwandlungen Sofern die Voraussetzungen erfüllt sind, können Datenstrukturen ineinander überführt werden. Dabei haben die Funktionen immer das Präfix as. und beim tibbles ausnahmsweise as_. as.numeric() as.character() as.vector() as.matrix() as.data.frame() as_tibble() Besonders nützlich ist hierbei as.numeric() zum Umwandeln von fälschlicherweise erstellen Character Spalten und as.data.frame(), um einen tibble in einen data.frame umzuwandeln, falls die angezeigten Rundungen zu ungenau sind. Beachte, dass as_tibble() das vorherige Laden des Packages tibble oder gleich des gesamten tidyverse benötigt. 12.6 Objektorientierung Für dieses Kapitel muss das sloop Package installiert und geladen werden. library(sloop) Grundsätzlich basieren die meisten bisher kennengelernten Datenstrukturen auf dem sogenannten S3 System der Objektorientierung. Das ist für den normalen R Nutzer völlig irrelevant, allerdings gibt es noch die anderen beiden Systeme namens S4 und R6. Diese können wir nicht ohne weiteres mit denen in diesem Buch kennengelernten Funktion verwenden. Ein populäres Beispiel dafür ist die Seite Bioconductor (Alternative zu CRAN), welche diverse Packages mit biologischen Fokus zur Verfügung stellt. Dort haben beinahe alle Packages das S4 System als zugrunde liegen. Dadurch kann beispielsweise nicht mehr mit dem Dollar-Operator auf Spalten zugegriffen werden. Stattdessen würde man das @ Zeichen verwenden. ls@Tibbel Dies sei an dieser Stelle nur deshalb beschrieben, da S4 und R6 Systeme zu seltsamen Fehlermeldungen führen können, wenn man versucht, wie gewohnt damit zu arbeiten. Mit der Funktion otype() aus dem sloop Package kann der Objekttyp herausgefunden werden. otype(big_five) [1] &quot;S3&quot; Wer sich tiefer mit Objektorierentierung in R beschäftigen möchte, sollte einen Blick in Advanced R von Hadley Wickham werfen. "],["iterationmain.html", "Kapitel 13 Iterationen 13.1 Was sind iterative Prozesse? 13.2 Listenbasierte Berechnungen 13.3 Einnisten", " Kapitel 13 Iterationen 13.1 Was sind iterative Prozesse? Wenn wir wiederholt etwas sehr ähnlich machen möchten, wäre die wohl offensichtlichste Möglichkeit Copy &amp; Paste. Man nimmt also den Code für den einen Anwendungsfall und modifiziert ihn leicht für andere Anwendungsfälle. Das große Problem dabei ist die Fehleranfälligkeit. Eine sehr gute Lösung dafür sind iterativ Prozesse. Solange eine Bedingung zutritt, soll ein bestimmter Befehl ausgeführt werden und dabei Kleinigkeiten der Reihe nach anpassen. Das wäre die Beschreibung eines sehr grundlegenden programmatischen Prinzips, den sogenannten Schleifen (z.B. for-Schleife und while-Schleife). Diese werden wir uns innerhalb der folgenden Kapitel allerdings nicht anschauen. Dies hängt von zwei Tatsachen ab: 1. Das Schreiben von fehlerfreien Schleifen ist bei fortgeschritteneren Fällen schwer und 2. ist es schwer, effiziente Schleifen in R zu schreiben, die nicht ewig für ihre Berechnungen brauchen. Im Kontext der Datenanalyse benötigen wir das allerdings auch gar nicht. Wir können mit map() über verschiedene Listenelemente die selbe Funktion anwenden (siehe Kapitel 13.2). Mit nest() kann dies auch direkt innerhalb eines tibbles gemacht werden (siehe Kapitel 13.3). Schauen wir uns einmal Schritt für Schritt die Problematik an. Wenn wir drei separate Regressionsmodelle für den Einfluss von Geschlecht auf Extraversion für jede Altersgruppe machen möchten, könnten wir den Code jeweils kopieren. Wir filtern also zuerst unseren Datensatz. mod1 &lt;- big5_mod %&gt;% filter(Gruppe == &quot;Jung&quot;) mod2 &lt;- big5_mod %&gt;% filter(Gruppe == &quot;Mittel&quot;) mod3 &lt;- big5_mod %&gt;% filter(Gruppe == &quot;Weise&quot;) Anschließend werden die drei Regressionsmodelle berechnet. model1 &lt;- lm(Extraversion ~ Geschlecht, data = mod1) model2 &lt;- lm(Extraversion ~ Geschlecht, data = mod2) model3 &lt;- lm(Extraversion ~ Geschlecht, data = mod3) Die Berechnung ist falsch, sobald wir eine Zahl in den Variablennamen vergessen haben, entsprechend anzupassen. Würden wir 20 oder 30 verschiedene Modelle auf diese Art und Weise rechnen wollen, können wir uns fast sicher sein, dass sich irgendwo ein Fehler einschleicht. 13.2 Listenbasierte Berechnungen Das purrr Package aus dem tidyverse enthält diverse Funktionen für iterative Prozesse. Wir werden uns im Folgenden map() und map_df() genauer anschauen. Dafür müssen wir die verschiedenen Datensätze in eine Liste packen. Pro Listenelement ein Datensatz auf den wir unsere Funktion (hier das Regressionsmodell) anwenden wollen. mod_ls &lt;- list(mod1, mod2, mod3) mod_ls [[1]] # A tibble: 147 x 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 30 f 3.1 3.4 Jung 2 2 23 m 3.4 2.4 Jung 3 3 24 f 3 2.8 Jung 5 4 14 f 2.8 3.5 Jung 6 # ... with 143 more rows [[2]] # A tibble: 39 x 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 32 m 3.5 3.1 Mittel 7 3 42 m 2.9 1.7 Mittel 20 4 34 m 3.5 2.2 Mittel 21 # ... with 35 more rows [[3]] # A tibble: 14 x 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 54 m 3.3 4.2 Weise 4 2 56 f 3.2 2.3 Weise 17 3 59 m 2.7 2.3 Weise 29 4 56 f 2.7 2.3 Weise 55 # ... with 10 more rows Der erste Schritt ist die Berechnung der Regressionsmodelle mit einer Lambda Funktion innerhalb von map(). Als Ergebnis erhalten wir ebenfalls eine Liste mit einem Modell pro Listenelement. Um das ganze angenehmer ausgeben zu lassen, fügen wir als zweiten Schritt noch map_df() (für map data frame) hinzu. Wie der Name suggeriert, werden (falls durch die Datenstruktur möglich) die Listenelemente als tibble/data.frame ausgegeben. Dies erreichen wir wieder mit der Funktion tidy aus dem broom Package. mod_ls %&gt;% map(~ lm(Extraversion ~ Geschlecht, data = .x)) %&gt;% map_df(tidy) # A tibble: 6 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3.07 0.0376 81.6 4.49e-123 2 Geschlechtm 0.0550 0.0598 0.919 3.60e- 1 3 (Intercept) 3.07 0.0649 47.3 1.08e- 34 4 Geschlechtm 0.0563 0.0930 0.605 5.49e- 1 # ... with 2 more rows Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 13.3 Einnisten Durch die Funktion nest() können innerhalb von Zellen eines tibbles Datenstrukturen jeder Art eingenistet werden. Dies wurde bereits in Kapitel 12.4 anhand eines einfachen Beispiels eingeführt. Nun wenden wir das Ganze auf unseren big5_mod Datensatz an. Dafür gruppieren wir wie gewohnt mithilfe von group_by(). Anschließend muss nur noch die Funktion nest() ohne Argumente ausgeführt werden. big5_mod %&gt;% group_by(Gruppe) %&gt;% nest() # A tibble: 3 x 2 # Groups: Gruppe [3] Gruppe data &lt;chr&gt; &lt;list&gt; 1 Mittel &lt;tibble [39 x 5]&gt; 2 Jung &lt;tibble [147 x 5]&gt; 3 Weise &lt;tibble [14 x 5]&gt; Als Ergebnis kriegen wir einen tibble, welcher in der ersten Spalte die Altersgruppen abgebildet hat. Daneben gibt es eine neue zweite Spalte namens data, in der wiederum drei tibbles mit den Dimensionen 39x4, 147x4 und 14x4. Jetzt können wir ähnlich wie bei den listenbasierten Berechnungen im vorherigen Kapitel mit map() eine Funktion auf jeden dieser eingenisteten Datensätze anwenden. Der Unterschied besteht darin, dass wir den Befehl innerhalb von mutate() ausführen müssen. Schließlich haben wir einen tibble vorliegen. Wenn wir dort eine neue Spalte erstellen möchten, benötigen wir dafür mutate(). Im zweiten Schritt nehmen wir die Modelle und geben diese in einem aufgeräumten Format mit tidy() wieder aus. big5_mod %&gt;% group_by(Gruppe) %&gt;% nest() %&gt;% mutate( Modelle = map(data, ~ lm(Extraversion ~ Geschlecht, data = .x)), Ergebnisse = map(Modelle, tidy) ) # A tibble: 3 x 4 # Groups: Gruppe [3] Gruppe data Modelle Ergebnisse &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 Mittel &lt;tibble [39 x 5]&gt; &lt;lm&gt; &lt;tibble [2 x 5]&gt; 2 Jung &lt;tibble [147 x 5]&gt; &lt;lm&gt; &lt;tibble [2 x 5]&gt; 3 Weise &lt;tibble [14 x 5]&gt; &lt;lm&gt; &lt;tibble [2 x 5]&gt; Nun sehen wir eine neue Spalte namens Modelle mit Einträgen des Datentyps &lt;lm&gt; (unsere linearen Modelle). Gleich daneben sind unsere Ergebnisse (p-Werte und Co.) wieder als tibble eingenistet. Damit wir nun an diese Ergebnisse herankommen, müssen wir abschließend lediglich diese mit unnest() aus der eingenisteten Struktur herausholen. big5_mod %&gt;% group_by(Gruppe) %&gt;% nest() %&gt;% mutate( Modelle = map(data, ~ lm(Extraversion ~ Geschlecht, data = .x)), Ergebnisse = map(Modelle, tidy) ) %&gt;% unnest(Ergebnisse) %&gt;% select(Gruppe, term, estimate, p.value) # A tibble: 6 x 4 # Groups: Gruppe [3] Gruppe term estimate p.value &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Mittel (Intercept) 3.07 1.08e- 34 2 Mittel Geschlechtm 0.0563 5.49e- 1 3 Jung (Intercept) 3.07 4.49e-123 4 Jung Geschlechtm 0.0550 3.60e- 1 # ... with 2 more rows Der besseren übersichtshalber haben wir nur die Spalten Gruppe, term, estimate und p.value ausgeben lassen. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). "],["appendix.html", "Appendix Datensatzerläuterungen Literaturverzeichnis Verwendete Softwareversionen", " Appendix Datensatzerläuterungen Big Five Indonesisch Heartoprolol Bitcoin Statistical Literacy Eye Tracking Video Artists Tracks Fitness Tipp WM Literaturverzeichnis McElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed. CRC press. Pearl, J., and Mackenzie, D. 2019. The Book of Why: The New Science of Cause and Effect. Basic books. Wickham, H. 2019. Advanced R. 2nd ed. CRC press. Wickham, H. 2021. ggplot2-Elegant Graphics for Data Analysis. Springer International Publishing. Cham, Switzerland. 3rd ed. Xie, Y. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. Verwendete Softwareversionen Im Sinne der Reproduzierbarkeit und da sich Packages durchaus auch noch verändern können, so dass es mit neueren Versionen möglicher Weise zu Fehlern kommt, sei an dieser Stelle die verwendetete Software angegeben. R: 4.0.4 RStudio: 1.4.1103 --------------------------------------------- Packages: forcats: 0.5.0 stringr: 1.4.0 dplyr: 1.0.4 purrr: 0.3.4 readr: 1.4.0 tidyr: 1.1.2 tibble: 3.0.4 tidyverse: 1.3.0 remp: 0.0.2 psych: 2.0.12 emmeans: 1.5.2.1 lavaan: 0.6.7 polycor: 0.7.10 VGAM: 1.1.4 splines: 4.0.4 stats4: 4.0.4 lmerTest: 3.1.3 afex: 0.28.0 lme4: 1.1.25 Matrix: 1.3.2 brunnermunzel: 1.4.1 car: 3.0.10 carData: 3.0.4 moments: 0.14 gt: 0.2.2 sloop: 1.0.1 here: 1.0.1 patchwork: 1.1.0 xtable: 1.8.4 broom: 0.7.2 lubridate: 1.7.9 knitr: 1.30 rio: 0.5.26 ggridges: 0.5.3 gghighlight: 0.3.1 cmprsk: 2.2.10 survminer: 0.4.9 ggpubr: 0.4.0 survival: 3.2.10 dagitty: 0.3.1 ggdag: 0.2.3 ggplot2: 3.3.2 stats: 4.0.4 graphics: 4.0.4 grDevices: 4.0.4 utils: 4.0.4 datasets: 4.0.4 methods: 4.0.4 base: 4.0.4 "]]
