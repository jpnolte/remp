[["index.html", "R für empirische Wissenschaften Vorwort", " R für empirische Wissenschaften Jan Philipp Nolte 2022-10-02 Vorwort Willkommen zum Buch R für empirische Wissenschaften. Hier wirst du ohne Vorwissen lernen, wie man die Programmiersprache R auf eine moderne Art und Weise im wissenschaftlichen Kontext anwendet. Die Zielgruppen sind im Besonderen Bachelor-, Master- und PhD-Studenten und Studentinnen, die im Rahmen des Studiums oder der Abschlussarbeit Daten auswerten müssen. Auch für WissenschaftlerInnen eignet sich dieses Buch, um erstmals reproduzierbare, für jeden und jede zugängliche Forschung zu machen. Dabei wird ein großer Fokus auf Kohärenz gelegt. Die zentralen Konzepte werden immer zu Beginn der Kapitel eingeführt, bevor sie auf verschiedene konkrete Szenarien angewandt werden. Das gesamt Buch wird durch das remp Package ergänzt, welches diverse Datensätze, Übungen und praktische Funktionen beinhaltet. R für empirische Wissenschaften ist unter der Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License veröffentlicht. Informiere Dich über die Bedingungen einer Veränderung und Weiterverbreitung des Materials. Es handelt sich nicht um die finale Version des Buches. "],["intro.html", "Kapitel 1 Einleitung 1.1 Für wen ist dieses Buch? 1.2 Aufbau und Bearbeitungsstrategie 1.3 Boxen, Übungen und Datensätze 1.4 Ergänzende Literatur", " Kapitel 1 Einleitung 1.1 Für wen ist dieses Buch? Das Buch ist grundsätzlich für jeden geeignet, der R lernen möchte. Ein Vorwissen über Programmiersprachen wird nicht vorausgesetzt. Neben der Grundlagen von R, wirst du nach Lesen dieses Buches einen Datensatz einlesen und bereinigen können. Du wirst verstehen, wie man publikationsreife Visualisierungen und Tabellen erstellt. Auch wirst du deskriptive Maße und die üblichsten statistischen Hypothesentests berechnen können. Die statistischen Verfahren werden nicht separat eingeführt, da es dafür bereits diverse ausführliche Lehrbücher gibt. Dieses Buch kann also eher als eine Erklärung zur computergestützten Datenauswertung verstanden werden. 1.2 Aufbau und Bearbeitungsstrategie Tatsächlich sind die Kapitel in der Reihenfolge aufgebaut, wie man normalerweise mit einem frisch erhobenen Datensatz umgeht. Nachdem alles richtig eingerichtet und aufgesetzt ist (Teil I), werden die Daten bereinigt und aufbereitet (Teil II), bis die Fragestellungen mithilfe statistischer Analysen beantwortet werden (Teil III). Vertiefende Konzepte für eine fortgeschrittene Verwendung runden den Inhalt des Buches schließlich ab (Teil IV). Teil I: Die ersten Schritte. Kapitel eins und zwei bilden den ersten Teil, der auch von Lesern, die bereits mit R gearbeitet haben, gründlich durchgelesen werden sollte. Vor allem die Installation bereitet häufig schon die ersten großen Probleme. Auch werden verschiedene Hilfestellungen eingeführt. Teil II: Vorbereitung. Die meiste Zeit in der Datenanalyse wird von der Datenvorbereitung beansprucht. Die eigentliche Auswertung geht dann meistens vergleichsweise schnell. Daher ist die Datenvorbereitung auch eines der ausführlichsten Kapitel dieses Buches. Außerdem die essentiellen R Projektdateien sowie einige notwendige Grundlagen erläutert. Außerdem wird das Einlesen von Datensätzen verschiedener Dateienarten erklärt. Teil III: Auswertung. Wenn der Datensatz endlich fertig aufbereitet ist, können Abbildungen erstellt sowie deskriptive Statistiken und inferenzstatistische Hypothesentest berechnet werden. Die Visualisierungen und in Tabellen dargestellten Ergebnisse werden dabei direkt publikationsreif ausgegeben. Teil IV: Vertiefung. Hier werden weiterführende und vertiefende Konzepte vorgestellt, die nicht zwingend für die eigentliche Datenanalyse benötigt werden. Es wird erklärt, wie man Tabellen oder ganze Berichte exportieren kann. Die verschiedenen Datenstrukturen werden verglichen und abschließend fortgeschrittener Programmiertechniken vorgestellt. Jeder hat einen individuellen Lernstil und liest ein Lehrbuch auf unterschiedliche Art und Weise. Die einzelnen Kapitel des Buches bauen zwar grundsätzlich aufeinander auf, allerdings wurde darauf geachtet, die Kapitel zum schnellen Nachschlagen gleichzeitig möglichst in sich geschlossen zu halten. Wer also nicht das gesamte Buch Schritt für Schritt durcharbeiten möchte, sollte aber zumindest folgende Kapitel gelesen haben. Dies gilt auch für jene, die nur an einem ganz bestimmten statistischen Test für die Bachelorarbeit interessiert sind. Zeitmangel? Man sollte sich mindestens mit den Kapiteln 2, 3, 4, 5 und 6.1 vertraut machen, da diese essentiell zum Arbeiten mit R sind. 1.3 Boxen, Übungen und Datensätze Es gibt drei Arten von Boxen mit jeweils unterschiedlicher Farbe und Symbol. Die mit der Glühbirne markierten Boxen fassen besonders wichtige Konzepte zusammen, die mit dem Warnzeichnen weisen auf häufige Probleme hin und die mit dem Laptop enthalten praktische Übungen. Diese Übungen werden immer auf dieselbe Art und Weise gestartet. Man muss dafür Namen der Übung innerhalb von hands_on() schreiben. hands_on(&quot;ue_datentypen&quot;) Diese Funktion öffnet deinen Browser, indem du dann die Übungen absolvieren kannst. Dafür wird kein Internet benötigt. Alternativ stehen die Übungsaufgaben und Lösungen auch auf der Internetseite zum Buch bereit (r-empirische-wissenschaften.de). Ein Übungsverzeichnis findest du im Appendix. Alle Übungen erkennst du namentlich am Präfix ue. Im Laufe des Buches werden aus didaktischen Gründen verschiedene Datensätze verwendet, die ebenfalls im Appendix genau vorgestellt werden. Zum Anwenden auf dem heimischen Computer oder Laptop kannst du entweder händisch den im Buch beschrieben Code abtippen oder du verwendest den Copy to Clipboard Button, der jedes Mal rechts oben im Codeblock erscheint. Dieser macht genau dasselbe, als würdest du den Befehl markieren und mit strg + c oder Rechtsklick und Kopieren, kopieren. 1.4 Ergänzende Literatur Statistik: Der Weg zur Datenanalyse. Ein verständliches, deutschsprachiges Buch als Einleitung für die wichtigsten Konzepte der Statistik. Der Fokus liegt tendenziell eher auf Sozial- und WirtschaftswissenschaftlerInnen, allerdings eignet sich das Buch generell als Einführung in die Statistik. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Dieses Buch ist etwas fortgeschrittener als das zuvor genannte, wobei hier alle in diesem Buch verwendeten statistischen Modelle ausführlich erklärt werden. ggplot2: Elegant Graphics for Data Analysis (3rd Edition). In R für empirische Wissenschaften wird ein sehr umfangreiches Erweiterungspaket namens ggplo2 zum Erstellen der Visualisierungen verwendet. Während es hier um konkrete Anwendungen geht, werden im vorgeschlagenen Buch die technischen Hintergründe erläutert. Diese sind vor allem interessant, wenn man darauf basierende Erweiterungen entweder verwenden oder selbst erstellen möchte. Das Buch kann kostenlos unter https://ggplot2-book.org/ eingesehen werden. R Markdown: The Definite Guide. Dieses Buch erläutert die in Kapitel 10.2 eingeführten Exportierungsmöglichkeiten mithilfe von rMarkdown deutlich ausführlicher. Denn mit rMarkdown kann man nicht nur kurze Berichte schreiben, sondern auch Bücher, Internetseiten und Abschlussarbeiten erstellen. Es ist kostenlos lesbar auf https://bookdown.org/yihui/rmarkdown/. Advanced R (2nd Edition). Hier werden tiefe Einblicke in R als Programmiersprache als solche gegeben. Wenn man seine Programmierfähigkeit mit R ausbauen möchte, kommt man an diesem Buch nicht vorbei. Auch hier kann das Buch im Internet (https://adv-r.hadley.nz/) kostenfrei gelesen werden. "],["start.html", "Kapitel 2 Startvoraussetzungen 2.1 Installation von R und RStudio 2.2 Aufbau von RStudio 2.3 RStudio anpassen 2.4 Funktionen und ihre Argumente 2.5 Packages (Erweiterungen) 2.6 Fehler- und Warnmeldungen 2.7 Historische Relikte", " Kapitel 2 Startvoraussetzungen 2.1 Installation von R und RStudio Die Unterscheidung zwischen R und RStudio ist für viele Anfänger verwirrend. Entgegen des ersten Eindrucks, handelt es sich dabei nicht um zwei austauschbare Alternativen. Stattdessen stellt R die Programmiersprache und RStudio eine Programmierumgebung dar. Den Unterschied können wir uns am Zusammenhang eines Motors mit seiner Karosserie vor Augen führen, welcher in Abbildung 2.1 illustriert ist. Man braucht die Programmiersprache R, damit überhaupt etwas voran geht. Dabei kann man das Auto grundsätzlich auch mit einem spartanischen Stahlgerüst fahren. Abbildung 2.1: Illustration des Unterschieds zwischen R und RStudio. Die Programmierumgebung RStudio macht aus diesem minimalistischen Stahlgerüst eine komfortable Luxuslimousine mit Navigationssystem und Sitzheizung. Man kann R folglich auch ohne RStudio benutzen, aber RStudio nicht ohne R. Sonderlich viel Spaß bereitet das allerdings nicht. RStudio bietet eine Vielzahl von großartigen und praktischen Features, weswegen wir im Laufe des Buches nur innerhalb von RStudio arbeiten werden. Es wird aber trotzdem häufig von R die Rede sein, da RStudio lediglich die verwendete Programmierumgebung ist. In den folgenden Kapiteln wird Schritt für Schritt erklärt, wie du R und RStudio auf den gängigsten Betriebssystemen installierst und wichtige Anpassungen innerhalb von RStudio vornimmst. Auch wenn sich RStudio bereits auf deinem Computer oder Laptop befindet, solltest du Kapitel 2.3 unbedingt lesen. 2.1.1 Programmiersprache R Zum Bearbeiten der Übungen aus dem Buch benötigst du die R Version 3.3.3 oder neuer. Falls beim späteren Installieren der Packages (siehe Kapitel 2.5) ein Fehler auftritt, liegt das aller Wahrscheinlichkeit an einer zu alten R Version. Am besten installierst du R, genau wie es hier beschrieben wird, neu. Bei der Installation gibt es Unterschiede zwischen den verschiedenen Betriebssystemen Windows, macOS (Apple) und Ubuntu (Linux). In der späteren Benutzung gibt es hingegen keine Unterschiede. Du musst dir also nur den jeweiligen Abschnitt für dein Betriebssystem anschauen. Nach der Installation musst du mit R nichts weiter machen und kannst sofort zum Herunterladen von RStudio hinübergehen. Windows: Geh auf cloud.r-project.org und wähle Download R for Windows Klicke anschließend auf den Link base. Drücke dann auf Download R for Windows und achte darauf, wohin du die Installationsdatei abspeicherst. Führe die Installationsdatei (z.B. R-4.0.4-win.exe) mit einem Doppelklick aus. Folge schließlich den Installationsanweisungen. Hierbei ist das Entfernen des Häkchen bei Message translations zwingend erforderlich (siehe Abbildung 2.2). Ansonsten muss nichts an den Standardeinstellungen der Installation geändert werden. Nach erfolgreicher Installation kannst du die heruntergeladene Installationsdatei (z.B. R-4.0.4-win.exe) wieder löschen. Abbildung 2.2: Richtige Installation von R ohne Sprachsupport macOS (Apple) : Gehe auf cloud.r-project.org und wähle Download R for (Mac) OS X. Je nachdem wie alt dein Mac ist, muss eine andere Version von R heruntergeladen werden. Deine macOS Version darf zur Benutzung der in diesem Buch vorgestellten Methoden nicht älter als 10.9 Mavericks sein. macOS 10.13 High Sierra und neuer: R-4.0.4.pkg (beziehungsweise die derzeit aktuellste Version) macOS 10.11 El Capitan und neuer: R-3.6.3.pkg macOS 10.9 Mavericks und neuer: R-3.3.3.pkg Achte auf den Speicherort der Installationsdatei und führe diese aus. Falls deine Einstellungen die Installation externer Programme verhindern, musst du das Installieren von externen Paketen (mit der Endung .pkg) explizit erlauben (siehe unten). Bei der Installation kann, ohne was zu verändern, stets auf weiter gedrückt werden. Nach fertiger Installation kann die Installationsdatei (z.B. R-4.0.4.pkg) gelöscht werden. Für englische Fehlermeldungen muss schließlich noch folgender Befehl innerhalb von R ausgeführt werden. Danach muss R geschlossen und erneut geöffnet werden. Alternativ kann man auch zuerst RStudio installieren und den Befehl dort entsprechend eingeben. system(&quot;defaults write org.R-project.R force.LANG en_US.UTF-8&quot;) Zum Schluss müssen wir noch manuell XQuartz (X11) installieren, da dies in neueren macOS Versionen nicht mehr standardmäßig enthalten ist. Dies brauchen wir später, um Datensätze auch innerhalb von RStudio anschauen zu können. Gehe dafür auf xquartz.org, lade die Installationsdatei (z.B. XQuartz-2.8.2.dmg) herunter und führe diese mit Doppelklick aus. Je nach Einstellungen des Betriebssystems kann die Installation externer Software (nicht von Apple zertifiziert) wie R aus Sicherheitsgründen blockiert werden. Um das zu verhindern, musst du mit dem Finder (nicht Launchpad) nach der Installationsdatei (z.B. R-4.0.3.pkg) suchen. Halte anschließend ctrl gedrückt und klicke auf die Datei. Aus dem erscheinenden Kontextmenü, kann schließlich Öffnen ausgewählt werden. Ubuntu (Linux): Drücke die Tastenkombination strg + alt + t oder tippe Terminal in die Suchleiste ein, um den Terminal zu öffnen. Bringe die Informationen deiner Repositories auf den neuesten Stand und installiere ein notwendiges Paket. sudo apt update -qq sudo apt install --no-install-recommends software-properties-common dirmngr Füge nun den Key hinzu, um für jetzt und zukünftige Updates einen sicheren Download zu gewährleisten. wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran\\_ubuntu\\_key.asc Das entsprechende Repository kann anschließend mit folgendem Befehl hinzugefügt werden. sudo add-apt-repository &quot;deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/&quot; Zum Installieren von R führe nun folgenden Befehl aus: sudo apt install --no-install-recommends r-base Zum Einstellen englischer Fehlermeldungen muss LANGUAGE=en an beliebiger Stelle in Renviron.site kopiert und gespeichert werden. Öffne diese dazu mit einem Texteditor wie gedit. Vergiss dabei nicht, die Datei im Anschluss abzuspeichern. sudo gedit /etc/R/Renviron.site 2.1.2 Programmierumgebung RStudio Nachdem R auf deinem Computer oder Laptop eingerichtet ist, kannst du die Installationsdatei für RStudio unter rstudio.com/products/rstudio/download/ herunterladen. Drücke dazu auf den Download Button für RStudio Desktop (Open Source License) und lade die für dein Betriebssystem richtige Version herunter. Wie beim Installieren von R musst du die heruntergeladene Installationsdatei mit der für dein Betriebssystem richtigen Endung (.exe, .pkg, .deb) ausführen und den anschließend eingeblendeten Anweisungen folgen. Nachdem das Programm installiert ist, kannst du die Installationsdatei auch in diesem Fall wieder löschen. Das eigentliche Programm RStudio findest du nun auf dem Computer zum Beispiel beim Drücken der Windows Taste und Eingabe des Wortes RStudio oder bei macOS über den Finder. Von jetzt an solltest du zum Arbeiten mit R immer RStudio öffnen und nicht R. Die extra vorhandene minimalistische Oberfläche von R kannst du indes ignorieren. Es ist bloß wichtig, es auf dem Computer installiert zu haben. 2.2 Aufbau von RStudio Wir schauen uns nun die einzelnen Komponenten innerhalb von RStudio an. Die Oberfläche ist dabei in vier Bereiche unterteilt: Nach frischer Installation ist die Console links unten, das R Skript als Source links oben, Environment &amp; History rechts oben und Plots, Hilfe, Packages und mehr unten rechts. Diese Reihenfolge kann beliebig nach eigener Präferenz verändert werden (siehe Kapitel 2.3). Abbildung 2.3: Aufbau von R Studio mit Skript (oben links), Konsole (unten links), Environment (oben rechts) und u.a. Plots und Help (unten rechts). Console. In der Konsole befindet sich im Prinzip die reine Programmiersprache R. Wir können also jegliche Befehle direkt in die Konsole eingeben, auf Enter drücken und das Ergebnis erhalten. Im Anschluss ist der eingegebene Befehl jedoch weg. Mit der oberen und unteren Pfeiltaste kannst du die in der bisherigen Sitzung bereits ausgeführten Befehle durchgehen. Da das weder sonderlich praktisch noch reproduzierbar ist, sollte man so genannte Skripte verwenden, die in einer gesonderten Datei (mit der Endung .R) gespeichert werden und nach Öffnen immer wieder ausführbar sind. Source. Beim initialen Starten von R, wird kein Skript angezeigt. Die Konsole nimmt also zuerst die gesamte linke Seite ein. Zum Erstellen eines R Skriptes kann man entweder die Tastenkombination strg/cmd + Shift + N verwenden oder auf das unterhalb des Reiters File gelegene Blatt Papier mit dem Plus Zeichen klicken und dort R Script auswählen. Gespeichert wird das R Skript ganz klassisch mit strg + S (oder über das Menü). Die Endung des Skriptes muss dabei .R sein, da die darin enthaltenen Befehle sonst nicht ausgeführt werden können. Man kann die Befehle innerhalb des Skriptes nun mit strg/cmd + Enter ausführen. Möchte man mehrere Zeilen ausführen, müssen diese erst markiert werden. Beispielsweise könnte man das gesamte R Skript ausführen, indem man erst mit strg/cmd + A alles markiert und anschließend mit strg/cmd + Enter bestätigt. Man kann mit der Tastenkombination strg/cmd + 1 und strg/cmd + 2 mit der Tastatur zwischen Konsole und Skript wechseln. Environment &amp; History. In der Environment werden alle gespeicherten Variablen (siehe Kapitel 4.1) angezeigt. Man kann dort auch auf die eingelesenen Datensätze klicken und sich diese innerhalb von RStudio in einem eigenen Reiter ansehen. Vorsicht sei hier bei großen Datensätzen geboten, da diese Ansicht relativ rechenintensiv ist und RStudio dadurch abstürzen kann. Es empfiehlt sich daher, große Datensätze in ein anderes Format wie CSV oder Excel umzuwandeln, um dort einen guten Überblick über den Datensatz zu erhalten (siehe Kapitel 5). Die History zeigt alles an, was in der Konsole innerhalb einer R Sitzung ausgeführt wurde. Wenn du vollständig mit Skripten arbeitest, kannst du die History vollständig ignorieren. Tatsächlich solltest du das automatische Speichern und Laden der History, wie in Kapitel 2.3 erklärt, sogar ausschalten. Schließlich haben wir sämtliche durchgeführten Berechnungen bereits reproduzierbar durch das R Skript gegeben. Plots, Help &amp; Packages. Plots zeigt die erstellten Visualisierungen an (siehe Kapitel 8). Dabei kann man unter Zoom ein eigenes Fenster mit der Abbildung öffnen. Die Abbildung verändert sich, wenn man die Länge oder Breite des Fensters entsprechend verschiebt. Grundsätzlich kann mit Export die Visualisierung direkt in der Form gespeichert werden. Darauf sollte allerdings aufgrund der Auflösungsunterschiede verzichtet werden (siehe Kapitel 8.12). Unter Help wird die Dokumentation der verschiedenen R Funktionen angezeigt (siehe Kapitel 2.6). Der Reiter Packages ist auch nicht weiter von Bedeutung, da wir Packages im Rahmen dieses Buches über die Konsole installieren und laden. 2.3 RStudio anpassen Die ersten zwei in diesem Kapitel erklärten Anpassungen sind essentielle und unabdingbare Voraussetzungen, um das erfolgreiche Ausführen des Codes auch auf anderen Computern zu gewährleisten. Diese Einstellungen nicht vorgenommen zu haben, ist eine häufige und schwer zu findende Fehlerquelle. Die anderen Anpassungen stellen Empfehlungen dar, die dir das Programmieren erleichtern sollen. Niemals den Workspace und die History speichern. Um zu gewährleisten, dass dein Code nicht nur bei dir funktioniert, ist es dringend notwendig das ständige Speichern und Laden des Workspaces auszustellen. Auch gewährleistest du dadurch, dass der Code immer funktioniert, wenn du diesen neu durchlaufen lässt. Gehe zu: Tools/Global Options.../General Entferne den Haken bei: Restore .RData into workspace at startup Ändere Save workspace to .RData on exit zu never Entferne den Haken bei: Always save history Keine Sorge, auf das Speichern deines Codes hat das keine Auswirkung. Standard Zeichenencodierung. Damit man den geschriebenen Code fehlerfrei auf anderen Geräten lesen kann, muss dieselbe Zeichencodierung gewählt werden. Die modernste und am weitesten verbreitetste ist UTF-8. Gehe zu: Tools/Global Options.../Code/Saving Ändere: Default text encoding zu UTF-8 Programmierhilfen. Aller Anfang ist schwer und warum sollte man dann nicht jede zur Verfügung stehende Hilfe nutzen wollen? Hier wird eingestellt, dass Funktionen vom Rest des Codes farblich hervorgehoben werden. Außerdem wirst du darauf hingewiesen, wenn zu wenige oder zu viele Leerzeichen gesetzt wurden. Zum Schluss stellen wir die Vorschläge zur Vervollständigung von Code noch auf eine kürzere Zeit ein. Gehe zu: Tools/Global Options.../Code Wechsel zu Display und mache einen Haken bei Highlight R function calls Wechsel zu Diagnostics und mache einen Haken bei Provide R style diagnostics Wechsel zu Completion und ändere im Abschnitt Completion Delay die Zahlen auf 1 (Character) und 0 (ms) Schickes Aussehen. Es hat einen Grund, weshalb heutzutage viele Internetseiten und Smartphone Apps mit einem dunklen Farbthema angezeigt werden. Das ganze sieht nicht nur besser aus, sondern ist auch deutlich angenehmer für die Augen. Gehe zu: Tools/Global Options.../Appearance Nun kannst du aus verschiedenen Themen wählen und auch die Schriftart und Schriftgröße anpassen. Anordnung der vier Layer. Die Anordnung von Console, Skript und Co ist Geschmackssache. Sinnvoll ist beispielsweise eine Aufteilung mit dem Skript auf der linken oberen Seite und der Console auf der rechten oberen Seite. Die Fenster unten links kannst du dann für immer Minimieren und hast so mehr Platz zum Arbeiten im Skript. Gehe zu: Tools/Global Options.../Pane Layout Oben links: Source Oben rechts: Console Unten links: History, Connections Unten rechts: Environment, Files, Plots, Packages, Help, Build, VCS, Viewer Komfortables Arbeiten. Wenig ist nerviger als dauernd die R Datei im Unterordner des Unterordners zu finden, um das Programm zu öffnen. Deshalb kann man einstellen, dass sich immer das zuletzt verwendete R Skript und Projekt öffnet. Über R Projekte erfährst du mehr in Kapitel 3. Gehe zu: Tools/Global Options.../General Mache einen Haken bei Restore most recently openend project at startup Und bei Restore previously open source documents at startup 2.4 Funktionen und ihre Argumente Eine Funktion erkennst du immer daran, dass sie von zwei runden Klammern gefolgt ist. Wenn du den Anweisungen in Kapitel 2.3 gefolgt bist, wird die Funktion auch extra farblich hervorgehoben. Schauen wir uns exemplarisch die Funktion zur Berechnungen des Mittelwertes an. mean(x = c(3, 4, 7, NA, 2), na.rm = TRUE) [1] 4 Wir haben hier sogar gleich zwei Funktionen: einmal mean() und einmal c(). Die erste berechnet den Mittelwert und letztere bindet die Werte zusammen. Dazu haben wir auch zwei sogenannte Argumente namens x und na.rm, denen wir mit einem Gleichheitszeichen entsprechende Werte zuweisen können. Dem ersten Argument geben wir die für die Berechnung des Mittelwerts zu verwendenen Werte und na.rm = TRUE entfernt potentielle fehlende Werte. Aus diesem Kapitel solltest du den Unterschied zwischen Funktionen und ihren Argument mitnehmen. Außerdem ist es wichtig, dass Argumenten Informationen immer mit einem Gleichheitszeichen übergeben werden. Tatsächlich müssten die Namen der Argumente nicht ausgeschrieben werden, solange die vorgeschriebene Reihenfolge der Argumente eingehalten wird. Wenn also die Funktion mean() als erstes Argument die zur Berechnung verwendeten Zahlen erwartet, könnte man den Argumentennamen x ebenso gut weglassen. mean(c(3, 4, 7, NA, 2), na.rm = TRUE) Das wird im Buch auch immer wieder gemacht, weil es mitunter übersichtlicher zum Lernen und im weiteren Verlauf schneller für dich zum Anwenden ist. 2.5 Packages (Erweiterungen) R bietet von Beginn an eine Bandbreite an Funktionen. Über die Jahre haben sich viele fähige Programmierer an die Arbeit gemacht, die Funktionen von base R (dem normalen R) zu erweitern. So genannte Packages sind kostenlose Erweiterungen, die verschiedenste Aufgaben erheblich erleichtern können. Stell dir vor, du kaufst dir ein neues Smartphone, auf dem von Anfang an verschiedene Apps installiert sind. Darüber hinaus gibt es allerdings auch viele Apps von Drittanbietern, die entweder neue Funktionen bereitstellen oder bestehende Aufgaben einfacher oder komfortabler machen. Genauso sind Packages in R zu verstehen. Auch mit den von Anfang an integrierten Funktionen könnte man die meisten Sachen irgendwie hinbekommen. Nur wäre dies mit deutlich mehr Aufwand verbunden als heutzutage notwendig. Deshalb arbeiten wir im Verlaufe des Buches mit verschiedenen Packages, die erst einmal installiert und geladen werden müssen. Die Packages werden unter anderem auf CRAN, Github oder auf Bioconductor geteilt, von wo sie heruntergeladen und in die eigene Analyse integriert werden können. Es gibt beispielsweise mittlerweile über 11000 Packages alleine auf CRAN. Dabei unterscheiden wir zwischen einer Funktion zum einmaligen Installieren und einer zum wiederholten Laden des Packages innerhalb von R. Abbildung 2.5 versucht den Unterschied der beiden Funktionen zu verdeutlichen. Abbildung 2.4: Vergleich vom (a) Installieren und (b) vom Laden von Packages. Während install.packages() das Package installiert, muss man es mit library() jedes Mal beim Starten von R neu laden. Erstere Funktion kannst du dir wie das Eindrehen einer Glühbirne vorstellen. Es bleibt dunkel im Zimmer, solange du nicht den Lichtschalter betätigst. Wenn du das Zimmer verlässt (R beendest), machst du das Licht wieder aus. Jedes Mal, wenn du das Zimmer erneut betrittst, muss das Licht erneut eingeschaltet werden. Falls bei der Installation Fehler auftreten, kann noch das Argument dependencies = TRUE getrennt mit einem Komma zusätzlich verwendet werden. Damit werden die Packages installiert, von denen unser gewünschtes Package gegebenenfalls zusätzlich abhängt. Das sollte allerdings normalerweise bereits mit dem normalen Befehl der Fall sein. Beim Starten von R muss jedes Mal aufs Neue der Befehl library() für jedes Package ausgeführt werden, welches du nutzen möchtest. 2.5.1 Installieren und laden Wie bereits erwähnt, muss zur Installation install.packages() benutzt werden. Wichtig ist hierbei, dass der Package Name in Anführungszeichen geschrieben wird. install.packages(&quot;packageName&quot;) Damit man auf die Funktionen des Packages zugreifen kann, muss das Package jedes Mal – also nach jedem neuen Öffnen von RStudio – aus der Bibliothek mithilfe von library() erneut geladen werden. Hierbei sind keine Anführungszeichen notwendig. library(packageName) Zur besseren Übersichtlichkeit solltest du alle library() Befehle am Anfang des jeweiligen R Skriptes untereinander schreiben und aufrufen. Wir werden im Rahmen dieses Buches vor allem vier Packages verwenden. Am besten schreibst du beim Üben also direkt oben library(tidyverse) library(here) library(rio) library(remp) in dein R Skript. In diesem Fall ist die Reihenfolge nicht entscheidend. Wenn verschiedene Packages jedoch gleichnamige Funktionen beinhalten, führt dies zu Problemen, auf die in Kapitel 2.5.3 näher eingegangen wird. Nach der Installation der Packages gibt es zwei Gründe, die Packages erneut installieren zu müssen. Der Umstieg auf eine neue Hauptversion von R (z.B. von R 4.0.0 auf R 5.0.0): Dabei wird der Ordner, der die Packages enthält, neu angelegt. Daher müssen alle Packages erneut installiert werden. Es gibt leider keine einfachen Wege, alle installierten Packages automatisch in den neuen Ordner zu kopieren. In einem der Packages gibt es eine neue Funktion, die man verwenden möchte. Der einfachste Weg Packages zu updaten, ist durch die erneute Installation. Es gibt zwar den Befehl update.packages(), allerdings muss dieser als Administrator in R und nicht RStudio ausgeführt werden. Selbst bei Ausführung als Administrator können dabei Probleme auftreten. Also ist man mit der erneuten Installation der Packages im Regelfall besser beraten. In aktuellen Versionen von RStudio wird automatisch eine Benachrichtung am Anfang des Skriptes angezeigt, falls in dem Skript erwähnte Packages nicht installiert sind. Diese kann man dann ganz bequem über Mausklick in RStudio installieren. Die gute Nachricht ist, dass man im Grunde genommen weder R noch die Packages updaten muss. Alle in diesem Buch verwendeten Packages verändern sich nicht mehr nennenswert und auch in R kommen nur sehr selten für die Datenanalyse wirklich relevante Neuerungen hinzu. 2.5.2 Notwendige Packages Diese vier für dieses Buch essentiellen Packages werden wir nun zuerst installieren. Das remotes Package verwenden wir nur einmalig, um das Package zum Buch namens remp von Github herunterladen zu können. Wofür jedes einzelnen Package davon genau zuständig ist, wirst du im Verlaufe des Buches erfahren. Die Installation kann je nach Internet und Computer einige Minuten in Anspruch nehmen. install.packages(c(&quot;tidyverse&quot;, &quot;here&quot;, &quot;rio&quot;, &quot;remotes&quot;)) Um auf die Übungsdatensätze und interaktive Übungen zugreifen zu können, musst du das Package remp installieren. Dafür musst du zuerst das Package remotes laden. Während der Installation des Packages kann es sein, dass du gefragt wirst, ob du bestimmte Packages aktualisieren möchtest. Du solltest dies an der Stelle ohne weiteres Zutun mit Enter verneinen. library(remotes) install_github(&quot;j3ypi/remp&quot;) Alle anderen teilweise speziell auf einen Kontext zugeschnittenen Packages, die im Rahmen des Buches vorgestellt werden, kannst du bei Bedarf installieren. Am Anfang jedes Kapitels mit einem neuen Package, wird immer extra darauf hingewiesen. 2.5.3 Namespace Beim Laden mehrerer Packages kann es sein, dass diese Funktionen mit demselben Namen verwenden (siehe Kapitel 6.1). Während beim tidyverse lediglich zwei selten verwendete base R Funktionen überschrieben werden, kann es beim Arbeiten mit vielen verschiedenen nicht aufeinander abgestimmten Packages durchaus häufiger zur Namensgleichheit kommen. Das ist eine schwierig zu identifizierende Fehlerquelle, weil sich die Funktion mitunter plötzlich nicht mehr so verhält wie erwartet. Dabei verwendet man automatisch die Funktion, die aus dem zuletzt geladenen Package stammt. Lädt man beispielsweise zuerst das Package tidyverse und anschließend data.table, erhält man folgende Meldung: library(tidyverse) library(data.table) Attaching package: ‘data.table’ The following objects are masked from ‘package:dplyr’: between, first, last The following object is masked from ‘package:purrr’: transpose Drei Funktionen (between(), first(), last()) aus dem Package dplyr und eine Funktionen (transpose()) aus dem Package purrr werden von data.table überschrieben. Es ist also wichtig, diese Meldungen beim Laden eines Packages nicht zu ignorieren. Im Notfall kann dies durch die Verwendung von Doppelpunkten package::funktion() verhindert werden. So teilt man R explizit mit, welche Funktion aus welchem Package man meint. Um trotz des späteren Ladens von data.table auf die between() Funktion von dplyr zuzugreifen, würde man also beim Aufrufen der Funktion folgendes schreiben: dplyr::between(1:12, 7, 9) Eine Ausnahme stellt dabei das tidyverse dar. Weil das tidyverse nur andere Packages lädt, kann man nicht tidyverse::funktion() schreiben. Stattdessen muss man das Package, aus dem die Funktion stammt (z.B. dplyr), direkt ansprechen. Für genauere Informationen zum Thema Namespace und die Hintergründe der damit verbundenen Environments sei auf das Buch Advanced R verwiesen. 2.6 Fehler- und Warnmeldungen 2.6.1 Der Unterschied Es gibt einen großen Unterschied zwischen Fehler- und Warnmeldungen. Wie der Name bereits suggeriert, stoppen Fehlermeldungen den Code, während Warnmeldungen ein Ergebnis zurückgeben und nur auf mögliche Probleme hinweisen. Es ist also sehr wichtig, die roten Meldungen in der Konsole genau zu lesen, anstatt direkt in Panik zu geraten. Ein Beispiel für eine Fehlermeldung sehen wir, wenn wir die Zahl 1 mit dem Buchstaben c summieren möchten. 1 + &quot;c&quot; Error in 1 + &quot;c&quot;: non-numeric argument to binary operator Hier ist die Fehlermeldung eindeutig. Wir versuchen einen nicht-numerisches Buchstaben (das c) mit einem numerischen zu addieren. Leider sind Fehlermeldungen in R keineswegs immer so eindeutig zu interpretieren. Nicht selten sind sie kryptisch und vor allem am Anfang wird man oft im Internet nach einer Lösung suchen müssen. Warnmeldungen haben zwar dieselbe erschreckende rote Schrift wie Fehlermeldungen, allerdings starten sie mit Warning Message und stoppen den Code nicht. Warnmeldungen sind für dich als Benutzer gedacht, um auf mögliche Probleme bei deiner Eingabe hinzuweisen. Gerade bei der statistischen Auswertung können Warnungen dir schon einmal häufiger über den Weg laufen und sollten keinesfalls blind ignoriert werden. Die Sprache der Fehler- und Warnmeldungen muss dabei Englisch sein, da sonst nicht vernünftig im Internet danach gesucht werden kann. Bitte beachte die dafür notwendigen Anpassungen bei der Installation, die in Kapitel 2.1.1 erklärt wurden. 2.6.2 Wo bekomme ich Hilfe? Während der ersten Kontakte mit einer Programmiersprache, dauert das Warmwerden möglicherweise eine gewisse Zeit. Damit man nicht gleich die Motivation verliert und aufgibt, sind die richtigen Quellen für eine schnelle Hilfe essentiell. Glücklicherweise ist dies einer der großen Vorteile von R. Es existieren nicht nur ausführliche Dokumentationen der verschiedenen Funktionen mit Anwendungsbeispielen. Darüber hinaus ist die R Community auch besonders hilfsbereit. Falls du verzweifelt vor deinem Computer oder Laptop sitzt und nicht weißt, wieso dein Code schon wieder nicht funktioniert, solltest du besonders an vier Orten nach Antworten suchen. Package Dokumentation. Das naheliegendste ist die Dokumentation der R Funktionen, die auch ohne Internet direkt in R aufgerufen werden können. Dafür kannst du entweder ein Fragezeichen vor die Funktion schreiben und diese ausführen oder mit dem Cursor auf der Funktion F1 drücken. Wenn wir zum Beispiel mehr über die Argumente der Funktion install.packages() erhalten möchten, können wir dies so erreichen: ?install.packages Die meisten modernen Packages stellen außerdem so genannte Vignetten zur Verfügung, in denen häufige Probleme ausführlich diskutiert und erklärt werden. Diese können mit vignette(\"nameDerVignette\") auch direkt innerhalb von R aufgerufen werden. Suchmaschine. In die Suchmaschine gibst du einfach r gefolgt von der Fehlermeldung oder dem Problem ein, an dem du aktuell festhängst. Es gibt kaum eine Fehlermeldung bei der das Programmierforum StackOverflow nicht an erster Stelle angezeigt wird. Denn meistens hat jemand anderes schon einmal genau dasselbe Problem gehabt hat. Stackoverflow. Dies ist wohl das mit Abstand größte Forum für Programmierfragen. Allerdings kann es einschüchternd sein, dort als absoluter Anfänger selbst eine Frage zu stellen. Voraussetzung zum Fragestellen ist dort das Erstellen eines kurzes reproduzierbares Beispiel für den Fehler. Du erreichst die Seite unter https://stackoverflow.com/. RStudio Community. Eine etwas kleiner Alternative zu StackOverflow ist die RStudio Community. Statte ihr bei Bedarf unter https://community.rstudio.com/ einen Besuch ab. 2.6.3 Fehler beheben Während deiner ganzen Zeit beim Programmieren wirst du immer wieder Code schreiben, der unweigerlich zu einem Fehler führt. Ob nun ein Package nicht geladen oder eine Klammer zu wenig gesetzt ist. Irgendetwas läuft häufig schief – so mag es zu Beginn erscheinen. Richtig um Hilfe fragen ist dabei gar nicht so einfach. Es ist zum Beispiel wenig informativ, wenn du jemandem schreibst: “RStudio funktioniert nicht”. Was funktioniert nicht? Wann tritt der Fehler auf? Hat es vorher mal funktioniert? Alle diese Informationen müssen dann erst einmal mühselig erfragt werden. Nun können die bereitzustellenden Informationen grundsätzlich in drei Abschnitte gegliedert werden. Eine kurze Problembeschreibung. Stelle ein minimales, reproduzierbares Beispiel bereit. Erkläre, was du bereits probiert hast. Während Punkt 1 und 3 soweit verständlich sind, bedarf es eine Erklärung für das reproduzierbare Beispiel. Ein reproduzierbares Beispiel enthält alle Informationen, die jemand anderes zum exakten Replizieren des Fehlers benötigt. Es müssen also zum Beispiel alle geladenen Packages angegeben werden. Außerdem ist es essentiell, den Datensatz in einer vereinfachten Kurzform zur Verfügung zu stellen. Dabei ist es optimal, eine minimale Version des Problems zu abstrahieren. Angenommen, du stößt beim Rechnen mit einem eigenen Datensatz auf ein Problem. Allerdings hat niemand anderes im Internet diesen Datensatz. Nun kann ein möglichst kleiner Datensatz innerhalb von R erstellt werden, mit dem man auch auf dasselbe Problem oder auf denselben Fehler stößt. Da das jedoch vor allem am Anfang nicht ganz einfach ist, wird erst in Kapitel 5.3 erläutert, wie man selbst Datensätze erstellen kann. Eine weitere Möglichkeit ist die Funktion dput(), die einen bestehenden Datensatz in eine für andere durch Copy &amp; Paste einlesbare Struktur bringt. Das geht natürlich nicht, wenn sensible Daten enthalten sind. Gehen wir das Ganze einmal Schritt für Schritt durch. Angenommen, wir haben irgendeine Fehlermeldung bezüglich des big_five Datensatzes. Genauer gesagt möchten wir einfach nur die Spalten Extraversion und Neurotizismus in einem Streudiagramm gemeinsam abbilden. Da immer nur die minimale Struktur zur Verfügung gestellt werden soll, um das Beispiel möglichst einfach und übersichtlich zu halten, wählen wir also nur diese beiden Spalten (select()) der ersten 10 Personen (slice_head(n = 10)) aus. Die Funktion dput() erledigt dann den Rest. big_five %&gt;% select(Extraversion, Geschlecht) %&gt;% slice_head(n = 10) %&gt;% dput() structure(list(Extraversion = c(3, 3.1, 3.4, 3.3, 3, 2.8, 3.5, 3.5, 3, 3.1), Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;)), row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;)) Die Ausgabe dieser Funktion musst du im Detail nicht verstehen. Allerdings kannst du diese Ausgabe in der Form kopieren und mit anderen Teilen, welche den Datensatz dadurch wiederum bei sich in R einlesen können. Anschließend ist es jedem möglich, auf die Spalten Extraversion und Neurotizismus der ersten 10 Personen zuzugreifen, ohne den big_five Datensatz geladen zu haben. Nun kannst du den fehlerhaften Code auf das Datenbeispiel anwenden und beschreiben, was nicht funktioniert hat. Häufig kommt man von ganz alleine auf die Lösung, während man Schritt für Schritt die durchgeführten Zeilen Code auseinander nimmt, um das Problem zu replizieren. Bevor du allerdings die Frage an jemand anderes richtest, solltest du diese häufigen Fehlerquellen bereits ausgeschlossen haben: Tippfehler Package nicht installiert Package nicht explizit geladen Package ist zu alt Klammer zu viel oder zu wenig RStudio nicht über ein R Projekt sondern über ein R Skript geöffnet Anführungszeichen vergessen oder an falsche Stelle gesetzt Alte Ergebnisse, die du nicht mehr auf dem Schirm hast, kommen dir in die Quere Komma statt Punkt bei Dezimalzahlen Numerische Spalten im Datensatz wurden als Character eingelesen (häufig bei Excel Dokumenten) Character Spalten sind fälschlicher Weise als Faktoren dargestellt Das Kapitel soll nicht etwa dazu dienen, dich vor dem Fragenstellen abzuschrecken. Stattdessen soll es dafür sorgen, dir eine möglichst positive Erfahrung beim Fragenstellen zu bereiten. Auch wirst du merken, wie schnell du lernst, die Probleme selbst zu lösen. Auch wenn dies manchmal erst kurz vor dem Fragen beim Vorbereiten des reproduzierbaren Beispiels der Fall ist. 2.7 Historische Relikte Dieses Kapitel richtet sich vor allem an diejenigen, die bereits vor diesem Buch Kontakt mit R gehabt haben. Da R seit über 25 Jahren existiert, gibt es dementsprechend auch in die Jahre gekommene Mittel und Wege, die mehr Probleme bereiten, als sie lösen. Keine der hier vorgestellten Funktionen solltest du jemals benutzen müssen: Die Funktionen attach() und detach() werden benutzt, um einen Datensatz unsichtbar in die Umgebung zu laden. Die in diesem Buch besprochenen Funktionen sind allerdings alle auf die Arbeit direkt am Datensatz ausgelegt. gc() steht für Garbage Collection (engl. für Müllsammlung) und soll Speicherplatz nach vielen Berechnungen wieder freiräumen. Allerdings führt R diese Funktion im Hintergrund automatisch aus, weswegen dieser manuelle Funktionsaufruf redundant ist. Die Funktion rm(list = ls()) verspricht eine saubere neue R Session zu kreieren, weswegen man diese häufig zu Beginn von R Skripten findet. Tatsächlich löscht dieser Befehl nicht alles, was zu schwierig identifizierbaren Fehlern führen kann. Spätestens seit es R Projekte gibt, ist dieser Befehl überflüssig. setwd() teilt R mit, wo genau auf dem Computer sich der einzulesende Datensatz befindet. Auch das ist durch R Projekte und das here Package überflüssig geworden (siehe Kapitel 3.3). Darüber hinaus solltest du es auch in Teil I bis III dieses Buches (bis auf wenige Ausnahmen) dringend vermeiden, einzelne Spalten des Datensatzes separat abzuspeichern. Sämtliche vorgestellte Funktionen arbeiten am besten direkt am Datensatz. Wenn du irgendwann im Rahmen des vierten Teils dieses Buches ein tieferes Verständnis von R gesammelt hast, kannst du dich gerne darin versuchen. Die oben genannten Funktionen solltest du hingegen auch dann auf keinen Fall verwenden. "],["project.html", "Kapitel 3 Projektorientierung 3.1 Das Problem 3.2 R Projekte erstellen 3.3 Das here Package", " Kapitel 3 Projektorientierung 3.1 Das Problem Erinnere dich an das letzte Mal zurück, als du eine Datei irgendwo hochgeladen hast. Normalerweise öffnet sich dann ein kleines Dialogfenster, durch das du bis zu deiner Datei navigieren kannst. Das hat den Hintergrund, dass dein Betriebssystem den genauen Ort der Datei – den so genannten Pfad – wissen muss, um diese finden und hochladen zu können. Das gleiche kannst du beobachten, wenn du gefragt wirst, wo genau du auf deinem Computer eine heruntergeladene Datei speichern möchtest. In R war das Einlesen von Datensätzen genau deswegen lange ein Problem. Dabei gibt es vor allem vier zentrale Probleme hervorzuheben: Wenn jemand auf einem anderen Computer die Rechnung deines R Skripts nachvollziehen möchte, ist der Pfad auf seinem oder ihrem Computer anders. Der Dateipfad ist oft lang, weil die Zieldatei in vielen verschachtelten Unterordnern liegt. Häufig verschiebt man den Ordner mit der Zeit an einen anderen Ort. Zwischen den Betriebssystemen ist die Art, den Pfad darzustellen, unterschiedlich. Zum Ersten Punkt kommt hinzu, dass die wenigsten überhaupt wissen, was genau ein Pfad ist und wie man diesen korrekt angeben müsste. Punkt zwei und drei ist vor allem hinsichtlich Reproduzierbarkeit ein Problem. Auch wenn du deine Analysen an KollegInnen oder BetreuerInnen schicken möchtest, wird der Code ohne Anpassungen nicht funktionieren. Glücklicherweise gibt es mittlerweile R Projekte, so dass du dich niemals mit Pfaden und daraus resultierenden fehlenden Reproduzierbarkeit auseinander setzten musst. Durch Projekte kann eine Funktion zum Einlesen des Datensatzes unabhängig vom Ort des Ordners sehen, wo sich die Datei befindet. Die einzigen beiden Voraussetzungen sind, dass der Datensatz im selben Ordner oder Unterordner wie die R Projektdatei liegt und dass man das here Package lädt. 3.2 R Projekte erstellen Nehmen wir einmal an, auf unserem Desktop liegt ein Ordner namens Beispiel. Wie in Abbildung 3.1 ersichtlich, befinden sich in diesem Ordner drei Dateien und ein Unterordner namens Daten. Abbildung 3.1: Beispielhafte Ordnerstruktur mit R Skript, Projektdatei und Datensatz. Auswertung.R ist unser R Skript, Beispiel.Rproj unsere Projektdatei und video.xlsx der Datensatz, den wir zur Auswertung einlesen wollen. Die Projektdatei muss manuell vor dem Einlesen erstellt werden. Dafür benötigt man nur ein paar Klicks. Oben rechts befindet sich ein Reiter namens Project: (None), wenn kein Projekt geöffnet ist und ansonsten der Projektname (zum Beispiel das Projekt Beispiel). Öffne zuerst das Dropdown Menu. Abbildung 3.2: Erster Schritt beim Erstellen eines neuen Projektes. Uns interessieren zum einen New Project... und Open Project... und zum anderen sind weiter unten andere Projekte aufgelistet, die vorher geöffnet wurden (hier 07_Buch und remp). Dadurch kann man mit einem einfachen Klick zwischen den eigenen Projekten wechseln. Dieses Feature erleichtert die Arbeit ungemein, da man auf dem Computer nicht mehr diverse Ordner nach den richtigen Dateien durchsuchen muss. Zum Erstellen eines neuen Projekts klicke auf New Project..., wodurch ein neues Fenster erscheint. Abbildung 3.3: Zweiter Schritt beim Erstellen eines neuen Projektes. Wir entscheiden uns exemplarisch für die Option Existing Directory. Das bedeutet, unser Ordner Namens Beispiel existiert bereits auf dem Desktop. Ob dieser Ordner leer oder bereits mit anderen Dateien gefüllt ist, ist nicht weiter von Bedeutung. Abbildung 3.4: Letzter Schritt zur Erstellung eines neuen Projektes. Mit einem Klick auf Create Project wird nun eine Projektdatei mit der Endung .Rproj in den gewählten Ordner gespeichert. Projekte bieten übrigens bezüglich Reproduzierbarkeit einen weiteren Bonus. Bei jedem Start von RStudio wird eine neue und in sich abgeschlossene R Umgebung geladen. So kann garantiert werden, dass der Code genau so auch auf anderen Computern ausgeführt werden kann. Beachte, beim Öffnen eines R Projects in dem Ordner nicht auf das R Skript (hier Auswertung.R) sondern auf die Projektdatei zu klicken und erst im Anschluss das R Skript zu öffnen. Für alle zukünftigen Öffnungen kannst du in Zukunft einfach RStudio öffnen und in dem eingangs beschrieben Dropdown Menü rechts oben das Projekt auswählen. Damit R die Position der Projektdatei auch findet, brauchen wir nun außerdem das here Package. Beim Arbeiten in einer Cloud wie Dropbox kann es zu einer Fehlermeldung kommen, die besagt, dass RStudio nicht auf die Datei zugreifen kann. Um das zu umgehen, muss die Synchronisierung der Cloud für die Dauer des Arbeitens mit R angehalten werden. 3.3 Das here Package Die Magie passiert, wenn du nun das here Package lädst. Das Package findet sofort den relativen Pfad zu deiner Projektdatei heraus. Was bedeutet das? Während man früher beispielsweise auf Windows mit C:\\Users\\J-PhN\\Desktop\\Beispiel den absoluten Pfad zum Ordner eingeben musste, findet das here Package den Ordner Beispiel mit der Projektdatei unabhängig von der Lage des Ordners. In der Praxis sieht das beim Laden so aus: &gt; library(here) here() starts at C:/Users/J-PhN/Desktop/Beispiel Würden wir den Ordner verschieben, hätte das keine Auswirkungen auf unseren Code. Das Package würde wieder zum Projektordner finden. Der erste Schritt ist also immer das Erstellen eines R Projekts und das Laden des here Packages am Anfang jedes neuen Skripts, mit dem man einen Datensatz einlesen möchte. "],["vars.html", "Kapitel 4 Wichtiges Grundlagenwissen 4.1 Variablen speichern und verwenden 4.2 Datentypen 4.3 Struktur von Daten 4.4 Der Dollar-Operator", " Kapitel 4 Wichtiges Grundlagenwissen 4.1 Variablen speichern und verwenden Ein zentrales Konzept in R ist das Speichern von Variablen mithilfe des Zuweisungspfeils. Dies ist vor allem für all jene ungewöhnlich, die das Erstellen von Variablen aus anderen Programmiersprachen mit dem Gleichheitszeichen kennengelernt haben. Wenn man das Ergebnis der durchgeführten Operation nicht speichert, ist es sofort weg und muss erneut ausgeführt werden. Würde man nun 2 + 2 [1] 4 rechnen, gibt R zwar 4 zurück, allerdings kann man später nicht mehr auf diese 4 zurückgreifen. Wenn man beispielsweise einen Datensatz einliest, ohne diesen mit dem Zuweisungspfeil zu speichern, kann man auf diesen im weiteren Verlauf nicht zugreifen. Mithilfe des Zuweisungspfeils wird die Variable in die lokale Environment gespeichert. Wir erinnern uns, die Environment ist in der Standardeinstellung nach Installation von RStudio im Fenster oben rechts. Möchten wir beispielsweise die Rechenoperation von vorhin namens rechnung speichern, würde man wie folgt vorgehen: rechnung &lt;- 2 + 2 Im Nachfolgenden könnte man nun diese Variable jederzeit wieder aufrufen. rechnung [1] 4 Auch ist es nun möglich, weitere Rechenoperationen mit dem vorherigen Ergebnis auszuführen. Zum Beispiel könnte man unser Ergebnis namens Rechnung mit 4 Multiplizieren. rechnung * 4 [1] 16 Variablen kann man grundsätzlich fast so benennen, wie man möchte. Man darf nur nicht mit einer Zahl anfangen oder nach einem Punkt direkt eine Zahl als Namen wählen wie bei .2VariablenName (falsch). Auf Umlaute sollte im Zusammenhang mit Programmiersprachen ebenfalls immer verzichtet werden. Das liegt an verschiedenen Zeichenkodierungen, auf die an dieser Stelle nicht weiter eingegangen werden soll. In den Variablen können sämtliche Datenstrukturen (siehe Kapitel 11) verstaut werden. Für den Moment reicht es für uns zu wissen, dass wir Datensätze und Zwischenergebnisse in den Variablen abspeichern müssen, um weiter darauf zugreifen zu können. Variablen können einfach überschrieben werden, indem man der Variable einen anderen Wert zuweist. Gerade in der Datenvorbereitung kann es schon einmal verlockend sein, die Änderungen unter demselben Variablennamen zu speichern. Wenn man allerdings eine unbeabsichtigte Änderung abspeichert, kann dies nicht rückgängig gemacht werden. Der Datensatz muss als Resultat erneut eingelesen werden. Es sei also vor allem am Anfang Vorsicht geboten. Auf der anderen Seite sollte man auch nicht jeden einzelnen Schritt in der Datenvorbereitung mit einem bedeutungslosen Namen versehen. Nicht das am Ende rechnung1, rechnung2, rechnung3 und rechnung4 existieren ohne jegliche Information im Namen, welche Variante nun welche Änderung enthält. Aussagekräftige Namen helfen anderen, deinen Code zu verstehen. Wenn du dir nun denkst, ohnehin nicht mit anderen zusammenarbeiten zu werden, lass dir folgendes gesagt sein: der andere bist in den meisten Fällen du selbst einige Wochen oder Monate später. Ohne Dokumentation und vernünftige Namensgebung sieht dein R Skript Monate später schnell so aus, als hätte es irgendein Fremder geschrieben. Dein zukünftiges Ich wird dir dankbar sein. Behalte immer im Hinterkopf, dass der Zuweisungspfeil zwar die Variable speichert, aber keinen direkten Output in der Konsole ausgibt. Oft erweckt das den Eindruck, als wäre nichts passiert. Es hilft dann, den Namen der Variable wie zuvor gezeigt erneut aufzurufen. Wenn man etwas im Code kommentieren möchte, muss eine führende Raute hinzugefügt werden. Das eignet sich nicht nur für kurze Beschreibungen, sondern auch als Gliederung eines langen R Skriptes. Beispielsweise könnte man so den Abschnitt der Berechnung vom Erstellen von Visualisierungen optisch trennen. Die Anzahl der Rauten spielt dabei keine Rolle. # Berechnung I # Dieser Code summiert 2 und 4 2 + 4 [1] 6 4.2 Datentypen Grundsätzlich gibt es in R vier verschiedene Grunddatentypen: Integer, Double, Character und Logical (siehe Abbildung 4.1). Dabei lassen sich Integer und Double zum Datentyp Numeric zusammenfassen, da die Unterscheidung dieser beiden in R selten von Bedeutung ist. Schauen wir uns die Datentypen nun etwas genauer an. Numeric (kurz: &lt;num&gt;) beschreibt numerische Werte, also Zahlen. Integer (kurz: &lt;int&gt;) sind ganze Zahlen und Doubles (kurz: &lt;dbl&gt;) Dezimalzahlen. Beachte dabei, dass Dezimalzahlen in R mit Punkten und nicht mit Kommata dargestellt werden. Abbildung 4.1: Schematische Übersicht über die wichtigsten Datentypen in R. 4.2.1 Zahlen, Buchstaben und logische Abfragen Beispiele für Numerics wären zum Beispiel Alter, Gehalt oder der Blutdruck. Wenn wir später Datentypen aus einem echten Datensatz ansehen, wirst du schnell merken, dass beinahe alle Zahlen als Double deklariert werden. Das liegt an der Eigenheit von R, ein L hinter die Zahl setzen zu müssen, wenn es sich um eine ganze Zahl hat. Dies hat allerdings keinerlei Auswirkung auf die in diesem Buch vorgestellten Funktionen. Double: 3.14 42 Integer: 42L Character (&lt;chr&gt;) ist der Datentyp, der Text enthalten kann – also einzelne Buchstaben, Zeichen, Wörter oder ganze Sätze. Dabei muss der Text immer in Anführungszeichen stehen. Solange die Anführungszeichen verwendet werden, kann mit Ausnahme vom Backslash (\\) alles geschrieben werden. Beispiele für Characters wären zum Beispiel die Blutgruppe, das Herkunftsland oder Allergien. &quot;Hallo Welt&quot; Logical oder auch logische Datentypen sind etwas abstrakter und kommen in Datensätzen seltener vor. Sie werden dann benötigt, wenn wir aufgrund von bestimmten Bedingungen manche Operationen durchführen möchten und andere wiederum nicht. Beispiele dafür wären die Auswahl derjenigen Personen, die über 50 Jahre alt sind oder eine neue Spalte namens Geschlecht zu erstellen, die nur bei weiblichen Personen eine 1 und ansonsten eine 0 einträgt. Dabei wird nämlich geprüft, ob die Aussage (zum Beispiel Person A ist älter als 50) wahr oder falsch ist. Das heißt, es gibt dabei zwei Zustände: TRUE oder FALSE (immer in Großbuchstaben). Eine Bedingung kann entweder zutreffen oder eben nicht. Es gibt verschiedene Funktionen, die TRUE oder FALSE zurückgeben, auf die wir im Verlaufe des Buches noch stoßen werden. Grundsätzlich gibt es dabei nur wenige verschiedene Grundoperatoren, die man kennen sollte. Um zu schauen, ob zwei Werte gleich sind, benutzen wir ein doppeltes Gleichheitszeichen (==). 2 == 2 [1] TRUE Gleiches Prinzip gilt für größer gleich (&gt;=) und kleiner gleich (&lt;=). Für größer (&gt;) oder kleiner (&lt;) reicht hingegen das einzelne mathematische Zeichen. Möchten wir nun logische Operationen kombinieren, verwenden wir UND (&amp;) oder ODER (|). Bei UND müssen beide Aussagen wahr sein, 1 &lt; 2 &amp; 1 == 2 [1] FALSE bei ODER hingegen nur mindestens eine. Man würde es wie folgt lesen: Entweder ist 1 kleiner 2 ODER 1 ist gleich 2. Da die erste Aussage wahr ist, wird TRUE zurückgegeben. 1 &lt; 2 | 1 == 2 [1] TRUE So können beliebig viele logischen Operationen miteinander kombiniert werden. Ein besonderer Fall logischer Datentypen ist NA (Akronym für Not Available), also die Bezeichnung für einen fehlenden Wert. Wir können einen Wert oder eine Variable auf seinen Datentyp überprüfen. Zurückgegeben wird uns nur TRUE oder FALSE. Die für uns interessanten Funktionen hierfür heißen is.numeric(), is.character(), is.logical(), is.factor(), is.Date(), is.POSIXct() und is.na(). Natürlich gibt es auch is.double() und is.integer(), allerdings genügt uns für die Anwendungen in diesem Buch is.numeric(). Möchte man generell herausfinden, mit welchem Datentyp man es zu tun hat, verwendet man typeof(). typeof(&quot;Kontrollgruppe&quot;) [1] &quot;character&quot; Da es unpraktisch ist, bei diversen Spalten eines Datensatzes einzeln den Typ abzufragen, wird dieser in tibbles (siehe Kapitel 11.3) – dem Datensatzformat, welches wir innerhalb von R konsistent im gesamten Buch verwenden – direkt unter dem Spaltennamen angezeigt. big5 # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # … with 196 more rows Da dort nur die Spalten angezeigt werden, die auf den Bildschirm passen, bietet glimpse() eine übersichtlichere Möglichkeit, einen schnellen Überblick über sämtliche Datentypen zu erhalten. glimpse(big5) Rows: 200 Columns: 7 $ Alter &lt;dbl&gt; 36, 30, 23, 54, 24, 14, 32, 20, 29, 17, 30, 15, 14, 23, 27, 15, 1964, 2… $ Geschlecht &lt;chr&gt; &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;… $ Extraversion &lt;dbl&gt; 3.0, 3.1, 3.4, 3.3, 3.0, 2.8, 3.5, 3.5, 3.0, 3.1, 3.2, 3.5, 3.0, 3.2, 2… $ Neurotizismus &lt;dbl&gt; 1.9, 3.4, 2.4, 4.2, 2.8, 3.5, 3.1, 2.6, 3.7, 3.6, 3.6, 2.8, 3.8, 2.0, 3… $ O1 &lt;dbl&gt; 5, 5, 3, 2, 5, 5, 3, 2, 4, 4, 5, 4, 2, 5, 4, 2, 5, 3, 5, 5, 3, 4, 2, 3,… $ O2 &lt;dbl&gt; 1, 3, 3, 5, 1, 1, 1, 1, 1, 3, 2, 3, 3, 1, 2, 3, 1, 2, 4, 1, 1, 1, 4, 3,… $ O3 &lt;dbl&gt; 5, 5, 5, 3, 5, 5, 5, 3, 5, 4, 5, 4, 3, 5, 4, 5, 5, 5, 4, 4, 1, 5, 3, 2,… Möchten wir nun mehrere Werte eines Datentypens aneinanderreihen, um sie zum Beispiel in eine Spalte eines Datensatzes zu schreiben, können wir c() (Abkürzung für Combine, engl. für kombinieren) verwenden. Möchtest du beispielsweise die Werte 1, 4, 5, und 10 kombinieren, erstellt dir c() einen entsprechenden Vektor (siehe Kapitel 11.1). Die Feinheiten und Merkmale von Vektoren brauchen dich an dieser Stelle nicht zu interessieren. Allerdings brauchen wir die Funktion c() des Öfteren, um Werte aneinander zu reihen. vec &lt;- c(1, 4, 5, 10) Wenn du verschiedene Datentypen innerhalb von c() miteinander kombinierst, werden die Datentypen ineinander umgewandelt. Dabei gilt Character &gt; Integer &gt; Logical. Wenn Zahlen mit Buchstaben kombiniert werden, wird also alles zu Buchstaben, auch wenn eigentlich Zahlen gemeint sind. Es gibt diverse Datentypen, die auf diesen vier Grundtypen aufbauen. Zwei wichtige, Faktoren und Zeitdaten, werden wir uns im Folgenden noch anschauen. Es kann passieren, dass Zahlen von R als Buchstaben interpretiert werden. Das kommt vor allem beim Einlesen von schlecht formatierten Datensätzen vor. Wenn eine Berechnung nicht so funktioniert, wie sie sollte, lohnt es sich, die Datentypen der jeweiligen Spalten zu überprüfen. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 4.2.2 Faktoren Wenn wir wissen, wie viele Ausprägungen Characters (Zeichenketten) annehmen können, verwenden wir Faktoren. Ein illustratives Beispiel hierfür wäre die Aufteilung in Experimental- und Kontrollgruppen. Bereits bei der Versuchsplanung weißt du, wie viele Gruppen du haben möchtest (beispielsweise eine Experimental- und zwei Kontrollgruppen), damit du eine entsprechende Stichprobenplanung durchführen kannst. Schauen wir uns das ganze mal etwas konkreter an. Angenommen, die Information über die Bedingung (Experimental, Kontrolle) befindet sich in der Variable namens Bedingung als Characters. bedingung &lt;- c(&quot;exp&quot;, &quot;kont1&quot;, &quot;exp&quot;, &quot;exp&quot;, &quot;kont2&quot;, &quot;kont1&quot;) Mit der Funktion factor() können nun Faktoren daraus gemacht werden. Zusätzlich sollte man das optionale levels Argument verwenden, um die Reihenfolge der Faktorstufen festzulegen. factor(bedingung, levels = c(&quot;exp&quot;, &quot;kont1&quot;, &quot;kont2&quot;)) [1] exp kont1 exp exp kont2 kont1 Levels: exp kont1 kont2 Das levels Argument ist außerdem wichtig, wenn wir nicht alle Faktorstufen beobachtet haben. Wenn beispielsweise eine der Gruppen in der Erhebung nicht vorkam, möchten wir eine Häufigkeit von 0 angezeigt bekommen. Würden wir die Faktorstufen nicht explizit definieren, würde die fehlende Gruppe, in den im weiteren Verlauf des Buches verwendeten Funktionen, schlichtweg nicht angezeigt werden. bedingung &lt;- c(&quot;exp&quot;, &quot;kont1&quot;, &quot;exp&quot;, &quot;exp&quot;, &quot;kont1&quot;) factor(bedingung, levels = c(&quot;exp&quot;, &quot;kont1&quot;, &quot;kont2&quot;)) [1] exp kont1 exp exp kont1 Levels: exp kont1 kont2 Die Ausgabe der Funktion zeigt uns, dass die Stufe kont2 vorliegen könnte und innerhalb des Faktors gespeichert ist. Wenn die als Faktor zu kodierende Spalte numerisch ist, können mit dem Argument labels die Namen der verschiedenen Ausprägungsgrade spezifiziert werden. Ein häufiges Beispiel hierfür wäre die Variable Geschlecht mit drei Ausprägungsgeraden. geschlecht &lt;- c(1, 1, 2, 3, 1, 3) Die Umkodierung geht mit dem labels Argument intuitiv, sofern man auf die Reihenfolge der Labels achtet. factor(geschlecht, labels = c(&quot;m&quot;, &quot;f&quot;, &quot;d&quot;)) [1] m m f d m d Levels: m f d Grundsätzlich sollte man Faktoren nur bei Bedarf erstellen (zum Beispiel unmittelbar vor der ANOVA oder vor Erstellen einer Abbildung), da Faktoren nicht mit allen Funktionen erwartungsgemäß harmonieren. Dies hängt damit zusammen, dass Faktoren als Zahlen gespeichert werden. So erhalten wir für den Faktor fact1 als class() den Datentyp des Faktors, fct1 &lt;- factor(bedingung, levels = c(&quot;exp&quot;, &quot;kont1&quot;, &quot;kont2&quot;)) class(fct1) [1] &quot;factor&quot; während uns typeof() den Datentyp Integer zurückgibt. typeof(fct1) [1] &quot;integer&quot; Tatsächlich behandelt R Faktoren als Integer (Zahlen), was zu überraschenden Outputs führen kann. Verwende Faktoren also am besten nur dann, wenn du sie wirklich brauchst. Beispielsweise zum Rechnen inferenzstatistischer Verfahren oder unmittelbar vor dem Erstellen von Visualisierungen. Wie man mit Faktoren konkret umgehen kann, wird in Kapitel 6.10 erklärt. 4.2.3 Zeitdaten Wie bereits in Abbildung 4.1 illustriert, interessieren uns die Datentypen Date und POSIXct. Letzteres ist im Regelfall ein unerwünschtes Format, welches häufig beim Einlesen von Excel Dokumenten entsteht. Der Datentyp POSIXct ist ein Akronym für Portable Operating System Inferface calendar time (in der Ausgabe von tibbles und der Funktion glimpse() mit dttm für Datetime abgekürzt). Enthalten ist das Datum (Jahr.Monat.Tag) und die Uhrzeit. Die Uhrzeit ist allerdings in den wenigstens Forschungskontexten von Interesse. date_time [1] &quot;2024-01-09 08:30:00&quot; In das gewünschte Date Format können wir mit as.Date() umwandeln. as.Date(date_time) [1] &quot;2024-01-09&quot; Hier haben wir nur Informationen über das Jahr, den Monat und den Tag. Wenn wir ein Datum ausgeben, sieht es zunächst wie eine Buchstabenfolge aus. date_past [1] &quot;2002-02-15&quot; Wir können uns aber mit is.Date() vergewissern, dass wir den Datentype Date vorliegen haben. is.Date(date_past) [1] TRUE Ähnlich wie bei Faktoren müssen wir auch hier zum Verständnis zwischen der Klasse des Datums class(date_past) [1] &quot;Date&quot; und dem Datentyp unterscheiden. typeof(date_past) [1] &quot;double&quot; Der zugrundeliegende Datentyp des Datums ist eine Dezimalzahl (Double). Intern speichert R das Datum als Anzahl von Tagen seit einem bestimmten Datum ab. Meistens ist dieses Datum der 1. Januar 1900. Von diesem Datum bis zum 15. Februar 2002 sind 37300 Tage vergangen. Überprüfen können wir das durch Subtrahieren der beiden Daten. date_past - ymd(&quot;1900-01-01&quot;) Time difference of 37300 days Dieses Format ist eine sogenannte difftime (engl. für Zeitdifferenz). Um diesen Wert beispielsweise in die vergangenen Jahre umzuwandeln, können wir durch die Funktion dyears() teilen. Als Argument können die Anzahl der Jahre übergeben werden. (date_past - ymd(&quot;1900-01-01&quot;)) / dyears(1) [1] 102.1218 Die Monate würde man mit dmonths() und die Tage mit ddays() erhalten. Es passiert tatsächlich öfter als man denkt, dass das Datum nicht als Datum sondern als Zahl angezeigt wird. Dann können wir unter Angabe des Startdatums (origin), die Zahl wieder in ein Datum umwandeln. as.Date(37300, origin = &quot;1900-01-01&quot;) [1] &quot;2002-02-15&quot; Die Gründe hierfür können vielfältig sein. Beispielsweise wenn man bei unbekanntem genauen Datum \"UN.06.2022\" in eine Zelle einträgt. Beim Einlesen kann die Spalte dann nicht als Datum eingelesen werden. Dies zu korrigieren, kann unter Umständen sehr mühsam sein. Einige Methoden zum konkreten Umgang mit diesen und weiteren Problemstellungen wird in Kapitel 6.11 erläutert. 4.2.4 Datentypen konvertieren Datentypen können auch umgewandelt werden. Die Namen sind ähnlich wie beim Abfragen des Datentypes, nur dass der Präfix hier nicht is, sondern as ist. Um den vorhin erstellten Vektor vec in den Typ Character umzuwandeln, würde man dementsprechend as.character(vec) [1] &quot;1&quot; &quot;4&quot; &quot;5&quot; &quot;10&quot; verwenden. Wie man sieht, stehen nun sämtliche Zahlen in Anführungszeichen, weswegen man zum Beispiel nicht mehr eine Zahl ohne Weiteres addieren könnte. as.numeric() as.character() as.factor() as.Date() ymd() dmy() Beachte hier, dass nur bei Date ein Großbuchstabe verwendet wird. Wenn leere Character in Numerics umgewandelt werden, generiert R automatisch fehlende Werte (NAs). 4.3 Struktur von Daten Bis auf wenige Ausnahmen, setzen sämtliche im Buch verwendete Funktionen eine bestimmte Struktur der Daten voraus: Jede Zeile bezieht sich auf eine Beobachtung (z.B. jede Zeile enthält die Werte von einem Proband oder einer Patientin). Jede Spalte enthält eine Variable (z.B. Alter, Familienstand oder Überlebenszeit). Jede Zelle beinhaltet einen einzigen Wert, der genau einer Variable einer Beobachtung zuzuordnen ist. Wenn diese Voraussetzungen erfüllt sind, spricht man auch von einem tidy (engl. für aufgeräumten) Datensatz. Achte also am besten bereits beim Erheben der Daten auf eine richtige Struktur, da sonst die Funktionen, die wir in Kapitel 6 kennenlernen werden, nicht ohne Weiteres anwendbar sind. Als ein Praxisbeispiel betrachten wir den big_five Datensatz. Darin entspricht jede Zeile einem Proband oder einer Probandin, der oder die einen Fragebogen zu den Big5 Persönlichkeitsfaktoren beantwortet haben. Die Spalten sind dabei eine Mischung aus demographischen Variablen (z.B. Alter) und den Werten der eigentlichen Fragen (z.B. O1 für die erste Frage der Dimension Offenheit für neue Erfahrungen). big5 # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # … with 196 more rows 4.4 Der Dollar-Operator Manche Statistikfunktionen benötigen jedoch nicht den gesamten Datensatz im vorherigen Kapitel beschriebenen Format, sondern nur eine Spalte daraus. Diese Spalte können wir mit dem sogenannten Dollar-Operator aus dem Datensatz extrahieren. Das praktische an dem Dollar-Operator im Vergleich zu anderen Methoden zum Extrahieren der Spalte, ist die Möglichkeit der automatischen Vervollständigung durch RStudio. Angenommen, wir geben den Namen des Datensatzes big_five gefolgt von einem Dollar-Zeichen an, werden uns alle enthaltenen Spalten in einem Dropdown-Menu angezeigt. Dies ist vor allem bei langen Namen der jeweiligen Spalte äußerst praktisch und schützt uns vor Tippfehlern. Hier wählen wir exemplarisch die Spalte Alter aus. big5$Alter Nun würden uns alle 200 Alterswerte in einer Reihe (oder präziser: als Vektor) zurückgegeben werden. Um das ganze übersichtlich zu halten, lassen wir uns mit head() das Alter der ersten 6 Personen ausgeben. head(big5$Alter) [1] 36 30 23 54 24 14 "],["io.html", "Kapitel 5 Datensätze 5.1 Einlesen externer Dateien 5.2 Datensätze aus Packages laden 5.3 Datensätze in R erstellen 5.4 Speichern und Konvertieren", " Kapitel 5 Datensätze 5.1 Einlesen externer Dateien Weißt du was Projekte sind und kannst diese innerhalb von RStudio erstellen? Wenn nicht, lies dir das Konzept der Projektorientierung genau durch (siehe Kapitel 3.3). Ansonsten kann R die Datei, die deinen Datensatz enthält nicht finden und somit auch nicht einlesen. Es muss also immer zunächst ein neues R Projekt erstellt werden. Der Datensatz muss sich dabei im gleichen Ordner wie die Projektdatei befinden. Im Regelfall möchte man den Datensatz nicht direkt in R erstellen, sondern einen bereits existierenden Datensatz zur Auswertung einlesen. Datensätze können dabei in verschiedenen Formaten vorliegen. Dies ist vor allem abhängig davon, mit welchen Programmen Unternehmen, Universitäten oder KollegInnen zur Datenerhebung arbeiten. Einige Beispiele für Dateientypen, in denen Daten häufig gespeichert werden, sind: R (.RData | .rda | .rds) Excel (.xlsx | .xls) SPSS (.sav) Stata (.dta) Comma separated values (.csv) Tabular separated values (.tsv) Sämtliche Datentypen können wir mit import() aus dem rio Package einlesen. Wir laden also zunächst das Package. library(rio) Um auch exotischere Dateiformate einlesen zu können, solltest du einmal den Befehl install_formats() ausführen. Es ist generell empfohlen, diese Funktion einmalig auszuführen, da sonst jedes Mal beim Laden des Packages eine entsprechende Meldung angezeigt wird. Beim Einlesen erkennt die Funktion import() die Art der Datei anhand der Endung und übernimmt hinter den Kulissen alles Weitere. Damit der Datensatz als sogenannter tibble eingelesen wird, solltest du zusätzlich das Argument setclass = \"tbl\" setzen. Weshalb wir die Sonderform der tibbles verwenden und was dies genau ist, wird im Verlaufe des Buches klar. Für den Moment musst du dir beim setclass Argument allerdings noch nichts denken, es aber trotzdem verwenden. Würden wir nur die import() Funktion aufrufen, würde der Datensatz zwar eingelesen, aber sofort wieder verschwinden, da dieser in R nicht abgespeichert wäre. Daher müssen wir den eingelesenen Datensatz, so wie in Kapitel Kapitel 4.1 beschrieben, einer Variable zuordnen. Der Name dieser Variable ist dabei nicht wichtig. Falls mit mehr als einem Datensatz gearbeitet wird, sollte ein aussagekräftigerer Name als der hier verwendete Name daten verwendet werden. Der Name des Datensatzes muss dabei in Anführungszeichen gesetzt werden. In unserem Beispiel heißt das Excel Dokument big_five.xlsx. daten &lt;- import(&quot;big_five.xlsx&quot;, setclass = &quot;tbl&quot;) Die Datei (hier das Excel Dokument) muss sich innerhalb desselben Ordners wie die Projektdatei befinden. Wenn die Datei in einem Unterordner ist, Abbildung 5.1: Beispielshafte Ordnerstruktur mit Unterordner für Datensätze, R Skript und Projektdatei. muss man den relativen Pfad – also den Weg bis zur Datei innerhalb der Unterordner (hier namens Daten) des Ordners (hier namens Beispiel) – zusätzlich der R Funktion mitteilen. Dabei nutzen wir die Funktion here() aus dem gleichnamigen Package. Dort können wir als separate Argumente den gesamten relativen Pfad eintragen. In diesem Beispiel liegt innerhalb des Ordners Daten der Datensatz indonesisch.xlsx. daten &lt;- import( file = here(&quot;Daten&quot;, &quot;indonesisch.xlsx&quot;), setclass = &quot;tbl&quot; ) Manchmal liegen Daten jedoch nicht in einer sondern in vielen verschiedenen Dateien vor. Das ist vor allem häufig bei biophysiologischen Messung wie beim Eye Tracking der Fall, bei denen die erhobenen Daten pro Person abgespeichert werden. Da wir nicht 20 Mal import() kopieren möchten (da Copy &amp; Paste sehr fehleranfällig ist), gibt es die Funktion import_list(). Angenommen, im Ordner Daten wären unsere 20 Excel Dokumente, in denen jeweils die Daten pro Person liegen. Dann könnte man zuerst mit dir() die Dateinamen herausfinden, um diese dann mit import_list() einzulesen. Mit dem Zusatzargument rbind = TRUE können wir die Datensätze direkt zusammenfügen. Voraussetzung dafür ist natürlich, dass die Datensätze dieselben Spalten haben. Da die Dateien in einem Unterordner liegen, müssen wir import_list() zusätzlich mit der Funktion here() mitteilen, wo sich die Datensätze befinden. dateien &lt;- dir(&quot;Daten&quot;, pattern = &quot;.xlsx$&quot;) daten &lt;- import_list( file = here(&quot;Daten&quot;, dateien), setclass = &quot;tbl&quot;, rbind = TRUE ) Das Dollar-Zeichen signalisiert in diesem Fall nur, dass wir am Ende des Dateinamens, die Endung .xlsx (also ein Excel Dokument) erwarten. Dasselbe funktioniert übrigens für den Fall, verschiedene Excel Sheets innerhalb eines Excel Workbooks zu haben. Mit import_list() können alle auf einen Schlag eingelesen werden. Die in diesem Kapitel kennengelernten Funktionen können wir praktisch für alle Dateienarten verwendet werden. Falls also anders als in unserem Beispiel die Daten nicht in einem Excel Dokument sondern einer Datei mit der Endung .csv enthalten sind, müssen wir lediglich die Endung verändern (z.B. big_five.csv). 5.2 Datensätze aus Packages laden Zum Bearbeiten dieses Buches brauchst du jedoch keine externen Datensätze, sondern nur jene, die im remp Package enthalten sind. Nach laden des Packages hast du grundsätzlich erst Zugriff auf alle enthaltenden Datensätze, wenn du extra die data() Funktion aufrufst. Zum Einlesen des big_five Datensatzes, muss demnach zum Beispiel library(remp) data(big_five) geschrieben werden. So kannst du das Gelernte sofort begleitend in RStudio nachvollziehen und ausprobieren. Sollte das Package vorher nicht geladen sein, kann alternativ auch nur data(big_five, package = &quot;remp&quot;) verwendet werden. Da die Syntax länger ist und beim Üben das remp Package sowieso geladen werden sollte, ist jedoch der zuvor dargestellte Weg empfohlen. 5.3 Datensätze in R erstellen Direkt innerhalb von R Datensätze zu erstellen, ergibt nur in wenigen Anwendungsfällen wirklich Sinn. Der womöglich Wichtigste ist das Erstellen eines minimalen reproduzierbaren Beispiels, falls man auf einen Fehler stößt, den man selbst nicht lösen kann. Für größere Datensätze sollte man die Daten jedoch besser in Datenformaten wie .csv oder .xlsx kreieren. Möchte man einen Datensatz erstellen, muss man lediglich der Funktion tibble() Werte übergeben. In diesem Beispiel speichern wir den neuen Datensatz mit den zwei Spalten Extraversion und Geschlecht als my_tbl. Die Funktion c() (engl. für combine) kombiniert die einzelnen Werte eines Datentyps und kettet selbige aneinander. my_tbl &lt;- tibble( Extraversion = c(1.2, 2.7, 1.5, 4.8), Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;) ) my_tbl # A tibble: 4 × 2 Extraversion Geschlecht &lt;dbl&gt; &lt;chr&gt; 1 1.2 m 2 2.7 f 3 1.5 f 4 4.8 m 5.4 Speichern und Konvertieren Das Speichern von Datensätzen funktioniert durch das rio Package ähnlich intuitiv wie das Importieren von Dateien. Anstelle von import() benutzen wir dafür stattdessen export(). Das erste Argument der Funktion ist der Datensatzname und das zweite Argument ist der gewünschte Dateiname. Der Dateientyp wird durch die gewählte Endung festgelegt. Möchte man beispielsweise den fertig aufbereiteten Datensatz video_clean als csv Datei abspeichern, würde man export(big_five_clean, &quot;big_five_clean.csv&quot;) schreiben. Manchmal ist es nützlich, den Datensatz unabhängig vom Einlesen umzuwandeln. Das kann zum Beispiel der Fall sein, wenn deine Kollegen dir eine SPSS Datei schicken (.sav) und du selbst kein SPSS hast, aber trotzdem einen Blick in die Daten werfen möchtest. Die Entwickler des rio Packages haben auch daran gedacht und die Funktion convert() geschrieben. Als erstes Argument übergibst du der Funktion den ursprünglichen Dateinamen (mit Endung) und als zweites denselben oder einen anderen Dateinamen mit der Endung des gewünschten Dateientypen. Sinnvoll wäre in diesem Kontext das Umwandeln in ein Excel Dokument, da dieses mit MS oder Libre Office problemlos geöffnet werden kann. convert(&quot;big_five.sav&quot;, &quot;big_five.xlsx&quot;) Die neue Datei wird sowohl bei export() als auch bei convert() in deinem Projektverzeichnis gespeichert. Besonders wichtig ist auch das Abspeichern von kategorealen Variablen als Buchstaben oder Wörter (Datentyp Character) und nicht als Zahlen. Nach spätestens einem Jahr kann kein Mensch mehr nachvollziehen, ob beispielsweise eine 1 nun für weiblich und 0 für männlich oder eine 1 für männlich und eine 0 für weiblich verwendet wurde. # A tibble: 4 × 2 Extraversion Geschlecht &lt;dbl&gt; &lt;dbl&gt; 1 1.2 0 2 2.7 1 3 1.5 1 4 4.8 0 Stattdessen sollte man die Zahlen in Character umwandeln. Wie das geht, wird in Kapitel 6.4.3 gezeigt. # A tibble: 4 × 2 Extraversion Geschlecht &lt;dbl&gt; &lt;chr&gt; 1 1.2 m 2 2.7 f 3 1.5 f 4 4.8 m Stelle vor dem Abspeichern des Datensatzes immer sicher, dass alle Variablen unmissverständlich kodiert sind. Kategorien sollten immer explizit benannt werden. "],["datenvorbereitung.html", "Kapitel 6 Datenvorbereitung 6.1 Einführung 6.2 Spalten auswählen, umbenennen und umsortieren 6.3 Zeilen auswählen und umsortieren 6.4 Spalteninhalte verändern 6.5 Umgang mit fehlenden Werten und Duplikaten 6.6 Breites und langes Datenformat 6.7 Spalten trennen 6.8 Datensätze zusammenführen 6.9 Buchstaben und Wörter bearbeiten 6.10 Faktoren verändern 6.11 Mit Zeitdaten arbeiten 6.12 Binäre Antwortmatrix erstellen", " Kapitel 6 Datenvorbereitung 6.1 Einführung Die Datenvorbereitung oder auch Datenaufbereitung ist im Regelfall der mit Abstand aufwendigste Teil. Selten hat man nach der Datenerhebung bereits einen perfekt formatierten Datensatz, den man statistisch auswerten kann. Manche gehen sogar so weit, der Datenvorbereitung einen Anteil von über 90% der gesamten Bearbeitungszeit zuzuschreiben. Mit den Funktionen des tidyverse ist dies heutzutage glücklicherweise leicht zu bewerkstelligen. Das tidyverse ist ein Sammelsurium an Packages, die die gleiche Philosophie teilen. Dabei steht der Name tidy universe, also eine Art aufgeräumtes Universum. Beim Laden des tidyverse werden acht Packages gemeinsam bereitgestellt. Damit spart man sich im Prinzip nur das einzelne Aufrufen der acht Packages. Man könnte stattdessen auch jedes Package einzeln laden. Ausgeführt in R sieht das wie folgt aus. Unter Conflicts werden Funktionen genannt, die den selben Namen wie base R Funktionen haben und die von hier an überschrieben werden. library(tidyverse) -- Attaching packages ------------------ tidyverse 1.3.0 -- v ggplot2 3.3.2 v purrr 0.3.4 v tibble 3.0.4 v dplyr 1.0.2 v tidyr 1.1.2 v stringr 1.4.0 v readr 1.4.0 v forcats 0.5.0 -- Conflicts --------------------- tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() ggplot2 bietet ein mächtiges Framework zum Erstellen von Visualisierungen (siehe Kapitel 8). tibble erweitert das klassische Format eines Datensatzes (früher data.frame) (siehe Kapitel 11.3). tidyr stellt vor allem zwei zentrale Funktionen zum Wechsel zwischen langem und breiten Dateiformat dar (siehe Kapitel 6.6). readr bietet verschiedene Funktionen zum Einlesen. Wir werden im Rahmen dieses Buches nur indirekt durch das rio Package darauf zurückgreifen (siehe Kapitel 5). purrr für verschiedene Funktionen für iterative Prozesse (siehe Kapitel 12). dplyr stellt diverse Funktionen zur Datenvorbereitung vor und stellt den Hauptteil dieses Kapitel dar. stringr zur Veränderung von Charactern, also sogenannten Buchstabenfolgen (siehe Kapitel 6.9). Dabei iststring ein anderes Wort für Characters. forcats zur Manipulation von Faktoren (siehe Kapitel 6.10). Der Name kommt von for categoricals und bietet folglich Funktionen für kategorische Variablen. Nicht mit dem tidyverse geladen, aber dennoch gut kombinierbar sind außerdem lubridate zur Manipulation von Zeitdaten (siehe Kapitel 6.11). broom für eine Formatierung der statistischen Ergebnisse, mit der man weiterrechnen kann (siehe Kapitel 9.8) Die in diesem Kapitel eingeführten Funktionen zur Datenaufbereitung sind in sich konsistent. Man muss das Prinzip also nur einmal verstehen, um sämtliche Funktionen anwenden zu können. Dabei sind diese durch die ausdrucksstarke Namensgebung beinahe schon selbsterklärend. Schauen wir uns mal eine typische Aneinanderreihung von Befehlen an: big5 %&gt;% select(Geschlecht, Extraversion) %&gt;% filter(Geschlecht == &quot;m&quot;) %&gt;% mutate(Extraversion_lg = log(Extraversion)) # A tibble: 82 × 3 Geschlecht Extraversion Extraversion_lg &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 m 3 1.10 2 m 3.4 1.22 3 m 3.3 1.19 4 m 3.5 1.25 # … with 78 more rows Gelesen würde es wie folgt: Man nehme den Datensatz big5 UND DANN wähle die Spalten Geschlecht, Extraversion UND DANN filtere die Spalten, in denen Geschlecht gleich “m” (für männlich) ist UND DANN mutiere oder verändere die (neue) Spalte Extraversion_lg durch die logarithmierten Werte der Extraversion Die anderen Funktionen sind ähnlich intuitiv und nahe an der englischen Sprache benannt. Besonders ist an dieser Stelle das kryptische Symbol Prozent-Größer als-Prozent (%&gt;%). Dieses Symbol können wir nur nach Laden des tidyverse (oder genauer magrittr) verwenden. Es macht dabei nichts anderes als die Übergabe oder das Weiterreichen des modifizierten Datensatzes an die nächste Funktion. Dies ist nur möglich, da das erste Argument der hier behandelten Funktionen immer der Datensatzname ist. Daher können wir uns hier den Namen des Datensatzes sparen. In dem obigen Beispiel werden zuerst zwei der Spalten ausgewählt. Dann wird das Ergebnis dieses Befehls – also der Datensatz mit den zwei Spalten – im nächsten Schritt der Funktion filter() übergeben. Dieses Verbindungssymbol %&gt;% wird Pipe genannt. Es kann mit dem Shortcut strg + shift + M beziehungsweise cmd + shift + M direkt erstellt werden. Die Verwendung der Pipe hat zwei große Vorteile: Die Verschachtelung mehrerer Funktionen ineinander wird verhindert. Wir müssen nicht jedes Ergebnis der verschiedenen Funktionen einzeln zwischenspeichern. Trotzdem müssen wir das Ergebnis dieser aneinandergeketteten Funktion natürlich irgendwann mit dem Zuweisungspfeil speichern. daten &lt;- big5 %&gt;% select(Geschlecht, Extraversion) %&gt;% filter(Geschlecht == &quot;m&quot;) %&gt;% mutate(Extraversion_lg = log(Extraversion)) Beachte, dass sämtliche Änderungen, die du am Datensatz vollziehst, erst gespeichert werden, wenn du sie mit dem Zuweisungspfeil (siehe Kapitel 4.1) einer Variable zuweist. Wenn dieser Variablenname bereits vergeben ist (z.B. der bisherige Datensatzname) wird dieser überschrieben. Um das rückgängig zu machen, muss der Datensatz dann einfach wieder neu eingelesen werden. Es empfiehlt sich bei einschneidenden Änderungen einen neuen Variablennamen zu verwenden. Ein zentrales Konzept ist die sogenannte Pipe (%&gt;%), die verschiedenste Funktionsaufrufe aneinanderbinden kann. Dabei wird der Datensatz an die nächste Funktion weitergeben. Für die in diesem Buch verwendete Pipe (%&gt;%) muss das tidyverse geladen sein. Seit der R Version 4.0.0 gibt es auch im normalen R eine vergleichbare Pipe, die sich in Funktionalität etwas unterscheidet. Da die im tidyverse enthaltene Pipe deutlich verbreiteter und auf die Packages abgestimmt ist, verwenden wir konsistent nur diese im Verlauf des Buches. 6.2 Spalten auswählen, umbenennen und umsortieren Die Funktionen in diesem Kapitel beschäftigen sich mit der Auswahl, Umbenennung und Umordnung von Spalten. Wir haben die Funktion select() bereits im vorherigen Kapitel kennengelernt. Es können also beliebig viele Spalten ausgewählt werden. Dies ist vor allem sehr nützlich, wenn der Datensatz sehr groß ist und man übersichtlich nur die Spalten haben möchte, die zur Auswertung verwendet werden. Zur Auswahl einer Spalte muss nur der Name (ohne Anführungszeichen) übergeben werden. Man kann auch direkt in der Funktion die Spalte umbenennen. Dabei muss auf der linken Seite des Gleichheitszeichen der neue Name stehen. big5 %&gt;% select(Extraversion, Neuro = Neurotizismus) # A tibble: 200 × 2 Extraversion Neuro &lt;dbl&gt; &lt;dbl&gt; 1 3 1.9 2 3.1 3.4 3 3.4 2.4 4 3.3 4.2 # … with 196 more rows schreiben. Zur Auswahl der Spalten von Extraversion bis O2 verwendet man einen Doppelpunkt. big5 %&gt;% select(Extraversion:O2) Möchte man nur die Spalte Geschlecht entfernen und den Rest ausgeben lassen, erreicht man dies mit einem Minus vor dem Spaltennamen. Bei mehreren zu entfernenden Spalten müsste man diese in Klammern rahmen (z.B. -(Extraversion:O2)). big5 %&gt;% select(-Geschlecht) Darüber hinaus können wir so genannte Helferfunktionen verwenden. Diese können nur in Kombination mit einer anderen Funktion verwendet werden. Ein nützliches Beispiel ist where(). So können beispielsweise alle numerischen Spalten ausgewählt werden. big5 %&gt;% select(where(is.numeric)) Eine weitere nützliche Helferfunktion ist starts_with(). So könnte man in diesem Fall beispielsweise alle Fragen zum Persönlichkeitsfaktor Offenheit auswählen, da diese alle mit dem Buchstaben O beginnen. big5 %&gt;% select(starts_with(&quot;O&quot;)) Außerdem nützlich sind ends_with() und contains(). Wenn man hingegen die Spalten nur umbenennen und dabei den gesamten Datensatz behalten möchte, muss man rename() verwenden. Die Syntax des Umbenennens bleibt dabei gleich. big5 %&gt;% rename(Sex = Geschlecht) # A tibble: 200 × 7 Alter Sex Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # … with 196 more rows Während beide Funktionen Spalten umbenennen können, gibt select() nur die ausgewählten Spalten und rename() hingegen alle Spalten zurück. Außerdem können Funktionen zur Umbenennung von Spalten verwendet werden. Dafür müssen wir rename_with() einfach nur die Funktion (ohne Klammern) übergeben. In diesem Beispiel werden alle Buchstaben der Spaltennamen in Großbuchstaben verändert. big5 %&gt;% rename_with(toupper) # A tibble: 200 × 7 ALTER GESCHLECHT EXTRAVERSION NEUROTIZISMUS O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # … with 196 more rows Gerade bei sehr großen Datensätzen mit vielen Spalten ist die Funktion relocate() äußerst nützlich. Eine neue Spalte wird zum Beispiel immer ans Ende des Datensatzes angefügt. Um diese trotzdem betrachten zu können, übergeben wir den Spaltennamen einfach unserer Funktion. big5 %&gt;% relocate(O1) # A tibble: 200 × 7 O1 Alter Geschlecht Extraversion Neurotizismus O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 36 m 3 1.9 1 5 2 5 30 f 3.1 3.4 3 5 3 3 23 m 3.4 2.4 3 5 4 2 54 m 3.3 4.2 5 3 # … with 196 more rows Wenn die Spalte nicht direkt am Anfang, sondern nach einer bestimmten anderen Spalte eingeordnet werden soll, können wir dies mit dem .after Argument festlegen. Hier würde die Spalte O1 hinter der Spalte Alter ausgegeben werden. big5 %&gt;% relocate(O1, .after = Alter) Auch hier können wir wieder Helferfunktionen wie where() verwenden. Man könnte beispielsweise alle numerischen Spalten hinter alle Character Spalten anfügen. big5 %&gt;% relocate(where(is.numeric), .after = where(is.character)) Eine weitere nützliche Funktion bei sehr breiten Datensätzen mit vielen Spalten ist names(). So können wir auf einem Blick alle Spaltennamen ausgegeben bekommen. big5 %&gt;% names() [1] &quot;Alter&quot; &quot;Geschlecht&quot; &quot;Extraversion&quot; &quot;Neurotizismus&quot; &quot;O1&quot; [6] &quot;O2&quot; &quot;O3&quot; Von Zeilennamen (rownames()) sollte hingegen grundsätzlich Abstand genommen werden. Falls der Datensatz Zeilennamen enthält, die tatsächlich von Bedeutung sind, sollte man diese mit der Funktion rownames_to_column(\"Spaltenname\") aus dem tibble Package in eine eigene Spalte befördern. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.3 Zeilen auswählen und umsortieren Anders als im vorherigen Kapitel beschäftigen sich diese beiden Funktionen mit der Auswahl und Umordnung von Zeilen. Der Funktion filter() muss dabei ein logischer Ausdruck übergeben werden. Das Ergebnis der Abfrage muss also immer TRUE oder FALSE zurückgeben können (siehe Kapitel 4.2). Zur Auswahl aller männlichen Probanden würde man Geschlecht == \"m\" schreiben. Beachte an dieser Stelle das doppelte Gleichheitszeichen. big5 %&gt;% filter(Geschlecht == &quot;m&quot;) # A tibble: 82 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 23 m 3.4 2.4 3 3 5 3 54 m 3.3 4.2 2 5 3 4 32 m 3.5 3.1 3 1 5 # … with 78 more rows Um Zeilen neu anzuordnen, benutzt man arrange(). Wenn die Zeilen nach aufsteigendem Alter sortiert werden sollen, muss man lediglich den Spaltennamen übergeben. big5 %&gt;% arrange(Alter) Für eine absteigende Anordnung kann man sich die Funktion desc() zur Hilfe nehmen. big5 %&gt;% arrange(desc(Alter)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1995 f 2.5 3.7 4 1 4 2 1964 f 3.2 2.3 5 1 5 3 60 f 3 2.7 5 1 5 4 59 m 2.7 2.3 5 1 5 # … with 196 more rows So sehen wir hier beispielsweise zwei falsch eingetragene Alterswerte. Hier haben zwei Probanden nicht das Alter sondern das jeweilige Geburtsjahr in den Datensatz eingetragen. Das müsste man vor einer Auswertung natürlich noch einsprechend korrigieren. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.4 Spalteninhalte verändern 6.4.1 Einzelne Spalten Die Funktion mutate() ist eine sehr divers einsetzbare Funktion zum Verändern bestehender oder Hinzufügen neuer Spalten. Dabei wird der neue oder bereits bestehende Spaltennamen auf die linke Seite des Gleichheitszeichens geschrieben. Auf der rechten Seite kann so ziemlich alles stehen, solange die Funktion eine Spalte zurückgibt, die genauso lang ist wie der Datensatz. Man könnte hier zum Beispiel nicht den Mittelwert berechnen, da dabei nur ein Wert zurückgeben werden würde. Mit der Funktion log() logarithmieren wir hingegen jeden einzelnen der Extraversionswerte, sodass wir 200 Werte erhalten – also genau so viele, wie wir Zeilen haben. big5 %&gt;% mutate(Extraversion_lg = log(Extraversion)) Innerhalb eines mutate() Aufrufes können auch gleich mehrere Spalten neu erstellt oder verändert werden. Die verschiedenen Spalten müssen dabei lediglich mit einem Komma getrennt werden. Hier berechnen wir beispielsweise die logarithmische mittlere Ausprägung von Extraversion und Neurotizismus. Der Abstand der öffnenden Klammer oben und schließenden Klammer unten ist aus funktioneller Sicht nicht relevant. Es ist allerdings im Sinne der Lesbarkeit bei vielen Argumenten sinnvoller, die Befehle auf mehrere Zeilen aufzuteilen. Um unser Ergebnis zu betrachten, ordnen wir noch unsere neuen mit lg endenden Spalten nach vorne an. big5 %&gt;% mutate( Extraversion_lg = log(Extraversion), Neurotizismus_lg = log(Neurotizismus) ) %&gt;% relocate(ends_with(&quot;lg&quot;)) # A tibble: 200 × 9 Extraversion_lg Neurotizismus_lg Alter Geschlecht Extraversion Neurotizismus O1 O2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.10 0.642 36 m 3 1.9 5 1 2 1.13 1.22 30 f 3.1 3.4 5 3 3 1.22 0.875 23 m 3.4 2.4 3 3 4 1.19 1.44 54 m 3.3 4.2 2 5 # … with 196 more rows, and 1 more variable: O3 &lt;dbl&gt; 6.4.2 Mehrere Spalten Bei zwei Spalten ist das obige Beispiel eine praktische Möglichkeit zum Logarithmieren. Wenn du eine Funktion über mehrere Spalten anwenden möchten, solltest du hingegen zusätzlich auf across() (engl. für herüber) zurückgreifen. Wir wollen schließlich eine Funktion über mehrere Spalten anwenden. Die Auswahl der Spalten erfolgt dabei innerhalb von across() genau wie bei select() (siehe Kapitel 6.2). Man kann beispielsweise den Doppelpunkt zur Auswahl eines Bereichs verwenden. An dieser Stelle müssen wir einmal genau hingucken, wo welche Klammer aufhört und endet. Denn die Funktion log() ist noch immer innerhalb von across(). big5 %&gt;% mutate(across(Extraversion:Neurotizismus, log)) Dies wird noch etwas klarer, wenn wir einmal exemplarisch die Namen der jeweiligen Argumente von across() auftragen. Mit .cols wählen wir die Spalten aus und dem .fns Argument übergibt man die anzuwendende Funktion. Außerdem können wir an dieser Stelle noch das .names Argument verwenden. Wie Dir vielleicht bereits aufgefallen ist, werden sonst die bisherigen Werte der ausgewählten Spalten nur überschrieben und keine neuen erstellt. Mit .names können wir die neuen Spaltennamen festlegen. Beachte dabei die geschweiften Klammern um .cols innerhalb des .names Argumentes, die notwendig sind, um auf den jeweiligen Namen der Spalte zurückzugreifen. Diesem Spaltennamen wird dann der Suffix _lg angehangen. big5 %&gt;% mutate(across( .cols = Extraversion:Neurotizismus, .fns = log, .names = &quot;{.col}_lg&quot;) ) Die auf mehrere Spalten anzuwendende Funktion muss innerhalb von across() übergeben werden. Falls ein Fehler auftritt, ist dieser in der Regel auf falsche Positionierung der Klammern zurückzuführen. Der übersichtshalber lassen wir diese Argumente allerdings für einfachere Anwendungsfälle im Verlaufe des Buches weg. Ein weiterer Unterschied besteht in der manuellen Auswahl einzelner Spalten. Hier müssen wir die einzelnen Spalten im Gegensatz zur Anwendung bei select() innerhalb von c() schreiben. big5 %&gt;% mutate(across(c(Extraversion, Neurotizismus), log)) Auch hier können wir die bereits in Kapitel 6.2 besprochenen Helferfunktionen wie where() verwenden. big5 %&gt;% mutate(across(where(is.numeric), log)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.58 m 1.10 0.642 1.61 0 1.61 2 3.40 f 1.13 1.22 1.61 1.10 1.61 3 3.14 m 1.22 0.875 1.10 1.10 1.61 4 3.99 m 1.19 1.44 0.693 1.61 1.10 # … with 196 more rows 6.4.3 Helferfunktionen Wenn wir beispielsweise eine bestehende Spalte auf bestimmte Art und Weise verändern wollen, wenn eine Bedingung zutrifft, erreichen wir dies mit der Funktion if_else(). Wir haben in Kapitel 6.3 gesehen, dass zwei Probanden ihr Geburtsjahr anstelle des Alters in Jahren angegeben haben. Wenn unsere Bedingung (condition) zutrifft, also das Alter in Jahren größer als 120 ist, soll das Jahr der Erhebung (2020) Minus das Alter gerechnet werden. Ansonsten (else beziehungsweise false) wird nur das unveränderte Alter zurückgegeben. Anschließend überprüfen wir noch unsere Berechnung, indem wir das Alter wieder absteigend anordnen. big5 %&gt;% mutate(Alter = if_else( condition = Alter &gt; 120, true = 2020 - Alter, false = Alter) ) %&gt;% arrange(desc(Alter)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 60 f 3 2.7 5 1 5 2 59 m 2.7 2.3 5 1 5 3 59 m 2.8 1.6 4 2 5 4 58 m 2.3 2.9 5 1 4 # … with 196 more rows TEXT Beispiel änder 0 und 1 in “m” und “f”. Achtung vor Faktoren, da diese nicht mit allen Funktionen funktioneiren. Bei mehr als zwei Bedingungen können wir stattdessen die Funktion case_when() verwenden. Auf der linken Seite der Tilde (~) ist dabei immer die Bedingung angegeben. Auf der rechten Seite hingegen ist die Ausgabe, wenn die zugehörige Bedingung auf der linken Seite zutreffen sollte. Allen Werten, auf die keine der explizit genannten Bedingungen zutrifft, wird NA (Akronym für Not Available, engl. für nicht vorhanden) zugewiesen. Dies kann man anpassen, indem man am Ende noch TRUE als Bedingung hinzufügt. In diesem Beispiel müssen alle, die bisher keiner Bedingung zugeordnet sind, der ältesten Altersgruppe angehören. Am Ende ordnen wir unsere neu erstellte Spalte zum Kontrollieren unserer Berechnung noch nach vorne. big5 %&gt;% mutate(Gruppe = case_when( Alter &lt;= 25 ~ &quot;Jungspund&quot;, Alter &gt; 25 &amp; Alter &lt;= 45 ~ &quot;Mittel&quot;, between(Alter, 46, 65) ~ &quot;Erfahren&quot;, TRUE ~ &quot;Weise&quot;) ) %&gt;% relocate(Gruppe) # A tibble: 200 × 8 Gruppe Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Mittel 36 m 3 1.9 5 1 5 2 Mittel 30 f 3.1 3.4 5 3 5 3 Jungspund 23 m 3.4 2.4 3 3 5 4 Erfahren 54 m 3.3 4.2 2 5 3 # … with 196 more rows Die Helferfunktion between() ist eine übersichtliche Alternative zum kombinierten logischen Begriff eine Zeile darüber. Wichtig ist an dieser Stelle, dass der Datentyp auf der linken Seite immer logisch sein muss. Auf der rechten Seite der Tilde muss es immer der gleiche Datentyp sein. Wenn wir also wie hier den Datentyp Character haben, muss bei allen diesen Zuweisungen auf der rechten Seite der Datentyp übereinstimmen. Ein weiterer praktischer Anwendungsfall ist die Umkodierung von von einzelnen Fragen. Angenommen, wir messen auf einer Skala von 1 (trifft gar nicht zu) bis 5 (trifft vollkommen zu) die Ausprägung der Offenheit für neue Erfahrungen. Um Verzerrungen zu vermeiden, sind in einem derartigen Fragebogen immer einige Items verneint gestellt. Normalerweise würde beispielsweise fragen, ob man gerne neue Sportarten ausprobiert. Würden wir allerdings fragen, ob man nicht gerne neue Sportarten ausprobiert, trifft unsere Skala natürlich nicht mehr zu. Jetzt wäre 5 (trifft gar nicht zu) und 1 (trifft vollkommen zu). Stellen wir uns vor, dies würde für das Item O1 zutreffen. Zum Vergleich erstellen wir eine neue Spalte namens O1_new, welche die umkodierten Werte enthält. big5 %&gt;% select(O1) %&gt;% mutate(O1_new = case_when( O1 == 1 ~ 5, O1 == 2 ~ 4, O1 == 3 ~ 3, O1 == 4 ~ 2, O1 == 5 ~ 1) ) # A tibble: 200 × 2 O1 O1_new &lt;dbl&gt; &lt;dbl&gt; 1 5 1 2 5 1 3 3 3 4 2 4 # … with 196 more rows Immer wenn die Spalte O1 den Werte 1 hat, wird eine 5 daraus gemacht und immer wenn eine 2 angekreuzt wurde, diese mit einer 4 ersetzt. Die 3 können wir so belassen und die 4 und 5 wandeln wir auf die gleiche Art und Weise um. Beachte auch hier, dass wir auf der linken Seite immer eine logische Abfrage und rechts den selben Datentyp (hier Double) vorliegen haben. all_of() erklären und contains() und %in% und starts_with() und ends_with() und where() Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.4.4 Eigene Funktionen erstellen Obwohl direkt in R oder in zusätzlichen Packages bereits eine Vielzahl von Funktionen enthalten sind, braucht man doch immer wieder eigene Funktionen für spezifische Anwendungsfälle. Dies versuchen wir anhand einer Funktion zu illustrieren, die den Logarithmus einer der Funktion übergebenen Zahl mit zwei summiert. Diese Funktion sei new_log() genannt. Eine Funktion wird mit function() erstellt. Innerhalb der runden Klammern können wir mit einem Komma getrennt beliebig viele Argumente festlegen. An dieser Stelle nehmen wir nur x. Der Name dieses Arguments ist grundsätzlich egal, solange er wie hier in dem Beispiel sowohl innerhalb von function() als auch in log() miteinander übereinstimmt. Die eigentliche Berechnung findet innerhalb der geschweiften Klammern statt. Es ist wichtig, dass wir einmal vor Benutzung diese Funktion durch Ausführen (strg + enter) lokal speichern. new_log &lt;- function(x) { log(x) + 2 } Eigene Funktionen müssen genau wie Packages nach Neustart von R immer wieder neu geladen werden. Dies erreicht man beispielsweise durch einfaches Ausführen des obigen Befehls. Es gibt in der Hinsicht also keinen Unterschied zum Speichern gewöhnlicher Variablen. Eine Funktion mit zwei Argumenten, wenn wir beispielsweise zusätzlich noch die Höhe der Zahl innerhalb der Funktion anpassen möchten, könnte wie folgt aussehen. new_log &lt;- function(x, zahl = 2) { log(x) + zahl } Nun könnte man als zweites Argument die zu addierende Zahl modifizieren. Innerhalb der runden Klammern steht zahl = 2, um den Standardwert von zahl zu definieren. Würden wir das Argument dann beim Anwenden weglassen, würde 2 addiert werden. Nur wenn man die Zahl verändern wollen würde, müsste man das der Funktion new_log() das Argument zahl übergeben. Erst einmal erstellt, können die eigenen Funktionen nun wie im vorherigen Kapitel bereits gelernt direkt in mutate() angewendet werden. big5 %&gt;% mutate(across(Extraversion:Neurotizismus, new_log)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3.10 2.64 5 1 5 2 30 f 3.13 3.22 5 3 5 3 23 m 3.22 2.88 3 3 5 4 54 m 3.19 3.44 2 5 3 # … with 196 more rows Eine besondere Art, diese eigenen Funktionen zu definieren, sind sogenannte Lambda Funktionen. Diese sind anonyme Funktionen, die keinen Funktionsnamen erhalten und daher auch nur an der definierten Stelle verwendet werden können. Dabei sind zwei Sachen hervorzuheben. Auf der einen Seite muss man immer eine Tilde (~) führend hinzufügen. Auf der anderen Seite ändert sich auch der Name des Arguments innerhalb von log(). Schließlich haben wir hier keine Namen der Argumente (im Beispiel oben waren das x und zahl) festgelegt. Das zu übergebende Argument ist an dieser Stelle die jeweilige Spalte des Datensatzes. Diese wird unabhängig vom Kontext mit .x festgelegt (beachte den führenden Punkt). big5 %&gt;% mutate(across(Extraversion:Neurotizismus, ~ log(.x) + 2)) TEXT weiteres sinnvolles Beispiel mit scales. Da hier nicht nur numerisch zurückgegeben wird, muss man erst umwandeln. big5 %&gt;% mutate(across( .cols = Extraversion:Neurotizismus, .fns = ~ as.numeric(scale(.x)), .names = &quot;{.col}_z&quot;) ) Lambda Funktionen sind eine praktische Möglichkeit, schnell eigene wenig komplexe Funktionen zu erstellen, die man nur an einer Stelle benötigt. So spart man sich das eigenständige Erstellen einer neuen Funktion. Für komplexere Anwendungen ist jedoch das Erstellen einer eigenen Funktion mit function() {} der übersichtlichere und damit empfohlene Weg. Mithilfe der Lambda Funktionen könnten wir jetzt auf einen Schlag nicht nur ein Item umkodieren, sondern so viele wie wir wollen. Wir erinnern uns, ein Item könnten wir mithilfe von case_when() umkodieren. big5 %&gt;% select(O1) %&gt;% mutate(O1_new = case_when( O1 == 1 ~ 5, O1 == 2 ~ 4, O1 == 3 ~ 3, O1 == 4 ~ 2, O1 == 5 ~ 1) ) Möchten wir nun auf einen Schlag die Spalten o1, O4 und O6 umkodieren, könnten wir dies wie gewohnt mit across() erreichen. Was sich nun durch die Lambda Funktion ändern, ist zum einen die Tilde und zum anderen ersetzt unser Platzhalter Argument .x den Spaltennamen aus dem vorherigen Beispiel. big5 %&gt;% mutate(across(c(O1, O3), ~ case_when( .x == 1 ~ 5, .x == 2 ~ 4, .x == 3 ~ 3, .x == 4 ~ 2, .x == 5 ~ 1) )) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 1 1 1 2 30 f 3.1 3.4 1 3 1 3 23 m 3.4 2.4 3 3 1 4 54 m 3.3 4.2 4 5 3 # … with 196 more rows Beachte an dieser Stelle, dass die schließende Klammer von across() erst hinter dem vollständigen Funktionsaufruf von case_when() geschrieben werden muss. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.4.5 Zeilenweise berechnen In Kapitel 6.4 haben wir bereits kennengelernt, wie man durch das Anwenden von Funktionen auf eine oder mehre Spalten eben diese verändert. Allerdings kann man mit dem bisherigen Wissen noch keine Berechnung in Abhängigkeit von den jeweiligen Zeilen durchführen. Dies ist allerdings gar kein seltener Anwendungsfall. Zum Beispiel möchte man die Mittelwerte bestimmter Spalten pro Person ausrechnen. Dafür gibt es die Funktionen rowwise() und c_across(), die miteinander kombiniert werden müssen. Wir möchten an dieser Stelle den Mittelwert pro Person für Offenheit berechnen. Dieser ergibt sich aus drei einzelnen Fragen zur Offenheit (O1, O2, O3). Zuerst müssen wir die Funktion rowwise() aufrufen. Schließlich müssen wir R erst einmal signalisieren, dass wir die folgende Funktion nun pro Zeile (oder zeilenweise) anwenden möchten. Innerhalb von mutate() müssen unsere drei Fragen zur Offenheit nun der Funktion c_across() übergeben werden. Beachte das Präfix c_ an dieser Stelle. Zur Kontrolle holen wir uns die neu erstellte Spalte namens Offenheit wieder an den Anfang des Datensatzes. big5 %&gt;% rowwise() %&gt;% mutate(Offenheit = mean(c_across(O1:O3))) %&gt;% ungroup() %&gt;% relocate(Offenheit) # A tibble: 200 × 8 Offenheit Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.67 36 m 3 1.9 5 1 5 2 4.33 30 f 3.1 3.4 5 3 5 3 3.67 23 m 3.4 2.4 3 3 5 4 3.33 54 m 3.3 4.2 2 5 3 # … with 196 more rows Hinter die Spaltenauswahl mithilfe von c_across() können wir mit einem Komma getrennt wie gewohnt weitere Argumente der jeweiligen Funktion übergeben. Hier sei exemplarisch die Entfernung fehlender Werte mit na.rm = TRUE illustriert. big5 %&gt;% rowwise() %&gt;% mutate(Offenheit = mean(c_across(O1:O3), na.rm = TRUE)) %&gt;% ungroup() Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.5 Umgang mit fehlenden Werten und Duplikaten colSums(is.na(df)) distinct() und arrange() library(MICE) md.pattern(df, rotate.names = TRUE) Zur besseren Illustration der verschiedenen Möglichkeiten verwenden wir an dieser Stelle einen kleinen selbst erstellten Datensatz namens df. df &lt;- tibble( Alter = c(34, NA, 45, 999), Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, NA), Extraversion = c(4, 3, 999, 2) ) Enthalten sind zum einen fehlende Werte als NA und zum anderen als 999 kodiert. NA ist dabei ein besonderer Datentyp, der für Not Available (engl. für nicht verfügbar) steht. Die Kodierung als 999 ist typisch für SPSS Nutzer, da dort kein extra Datentyp für fehlende Werte vorhanden ist. Wir sind also daran interessiert, diese 999 oder andere nicht passende Werte in NAs sowie umgekehrt NAs in bestimmte Zahlen umzuwandeln. Zum Umwandeln von Werten in NA können wir die Funktion na_if() aus dem dplyr (tidyverse) Package verwenden. Die Syntax ist dabei denkbar intuitiv. Wenn beispielsweise in der Spalte Alter die Zahl 999 vorkommt, soll stattdessen NA geschrieben werden. Das ganze müssen wir natürlich innerhalb von mutate() verwenden (siehe Kapitel 6.4). df %&gt;% mutate(Alter = na_if(Alter, 999)) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f 999 4 NA &lt;NA&gt; 2 Das selbe können wir natürlich auch gleich auf mehrere Spalten anwenden. df %&gt;% mutate(across(Alter:Geschlecht, ~ na_if(.x, 999))) Alternativ kann mit der Helferfunktion everything() auch der ganze Datensatz ausgewählt werden. df %&gt;% mutate(across(everything(), ~ na_if(.x, 999))) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f NA 4 NA &lt;NA&gt; 2 Zur Umwandlung von NAs in beispielsweise die Zahl 999, können wir replace_na() aus selbigen Package benutzen. Wenn in der Spalte Alter ein NA steht, soll dieses mit 999 ersetzt werden. df %&gt;% mutate(Alter = replace_na(Alter, 999)) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 999 f 3 3 45 f 999 4 999 &lt;NA&gt; 2 Eine Besonderheit stellt die Zuweisung von NAs innerhalb von if_else() oder case_when() dar. So können NAs erstellt werden, wenn eine bestimmte Bedingung zutrifft. Wir haben bereits in Kapitel 6.4 gelernt, dass die Datentypen auf der rechten Seite des Gleichheitszeichens beziehungsweise auf der rechten Seite der Tilde übereinstimmen müssen. Für die unterschiedlichen Datentypen gibt es jeweils einen eigenen NA Typ. Numerisch: NA_real_ Character: NA_char_ Logisch: NA df %&gt;% mutate(Alter = if_else( condition = Alter &gt; 120, true = NA_real_, false = Alter) ) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f 999 4 NA &lt;NA&gt; 2 Im vorherigen Kapitel haben wir bereits summarise() mit den verschiedenen Funktionen der deskriptiven Lagemaße kennengelernt. Diese haben im Regelfall ein Argument namens na.rm (Akronym für not available remove), welches die fehlenden Werte der entsprechenden Spalte direkt entfernt. Genauer gesagt verwenden diese Funktionen an dieser Stelle die sogenannten Pairwise complete observations. big5_mod %&gt;% summarise( Min = min(Alter, na.rm = TRUE), Mean = mean(Alter, na.rm = TRUE) ) Eine weitere Möglichkeit ist das Entfernen von Zeilen, die fehlende Werte enthalten. Dies erreichen wir mit drop_na() aus dem dplyr Package. Allerdings entfernt diese Funktion alle Zeilen mit NAs. Wenn du also zwei Spalten auswerten möchtest und in einer dritten für den Moment irrelevanten Spalte ist ein fehlender Wert, würde die entsprechende Zeile trotzdem entfernt werden. Hier ist also Vorsicht geboten, um keine Informationen zu verlieren. df %&gt;% drop_na() # A tibble: 2 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 45 f 999 Alternativ können wir mithilfe von filter() und der logischen Abfrage is.na() das Entfernen von NAs auch auf eine bestimmte Spalte begrenzen (hier Geschlecht). df %&gt;% filter(!is.na(Geschlecht)) # A tibble: 3 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 m 4 2 NA f 3 3 45 f 999 Da distinct immer die erste der doppelten Zeilen behaelt, kann man so die Erstdiagnose drin behalten daten1 %&gt;% arrange(Name, Vorname, Erstdiagnose) %&gt;% print(n = 30) daten1 %&gt;% distinct(Name, Vorname, .keep_all = TRUE) Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.6 Breites und langes Datenformat Grundsätzlich kann man ein breites von einem sogenannten langen Datenformat unterscheiden. Im breiten Datensatz ist jede Spalte eine Variable, jede Zeile eine Beobachtung und jede Zelle ein Wert. Für die meisten Fälle ist das unser gewünschtes Datenformat. In Abbildung 6.1 ist ein einfaches Beispiel für einen breiten Datensatz mit drei Personen und zwei Variablen. Abbildung 6.1: Breites Datenformat mit drei Personen und drei Variablen. Für das Erstellen mehrfaktorieller Abbildungen und hierarchischer statistischer Modellierung benötigen wir allerdings das lange Datenformat. In Abbildung 6.2 ist das lange Äquivalent zum gerade besprochenen breiten Datenbeispiel. Abbildung 6.2: Langes Datenformat mit Persönlichkeitsfaktor als Innersubjektfaktor. Schauen wir uns nun an, wie wir dies intuitiv mithilfe des tidyr Packages erreichen können. Mit pivot_longer() (engl. für Drehpunkt länger) können wir einen breiten ins lange Datenformat bringen. Auf der anderen Seite nutzen wir pivot_wider() für die Transformation vom langen ins breite Datenformat. Erster Funktion findet deutlich häufiger Anwendung. Für das Umformatieren ins lange Datenformat ist es essentiell wichtig, einen eindeutigen Personenidentifikator im breiten Datensatz zu haben. Ansonsten wird es nicht funktionieren. Hier entscheiden wir uns einfach für die Zeilennummer, die wir mit der Funktion row_number() in die Spalte VPN (Akronym für Versuchspersonennummer) schreiben. wide_big5 &lt;- big5 %&gt;% mutate(VPN = row_number()) %&gt;% select(VPN, Geschlecht, Extraversion, Neurotizismus) wide_big5 # A tibble: 200 × 4 VPN Geschlecht Extraversion Neurotizismus &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 m 3 1.9 2 2 f 3.1 3.4 3 3 m 3.4 2.4 4 4 m 3.3 4.2 # … with 196 more rows Nun müssen wir in der Funktion pivot_longer() nur noch die gewünschten Spalten auswählen. Im obigen Beispiel wären das Extraversion und Neurotizismus. Beachte, dass wie bei across() auch hier die Spalten bei einzelner Auswahl der Funktion innerhalb von c() übergeben werden müssen. Nun müssen wir noch definieren, wie unsere beiden neuen Spalten heißen sollen. Schließlich kommen in eine Spalte unsere Werte (values_to) und in eine andere Spalte die Spaltennamen (names_to), welche hier die Persönlichkeitsfaktoren sind. Hier entscheiden wir uns für die Namen \"Auspraegung\" und \"Faktor\". Die Namen müssen hier unbedingt in Anführungszeichen geschrieben werden, da die Spalten noch nicht existieren. Das Ergebnis speichern wir an dieser Stelle als long_big5 ab. long_big5 &lt;- wide_big5 %&gt;% pivot_longer( cols = c(Extraversion, Neurotizismus), values_to = &quot;Auspraegung&quot;, names_to = &quot;Faktor&quot; ) long_big5 # A tibble: 400 × 4 VPN Geschlecht Faktor Auspraegung &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 m Extraversion 3 2 1 m Neurotizismus 1.9 3 2 f Extraversion 3.1 4 2 f Neurotizismus 3.4 # … with 396 more rows Zur Auswahl der Spalten können die selben Helferfunktionen verwendet werden wie in Kapitel 6.2 beschrieben (z.B. starts_with(), ends_with() oder everything()). Umgekehrt können wir mithilfe von pivot_wider() den Datensatz long_big5 wieder ins breite Datenformat bringen. Dafür müssen wir hier nur festlegen, aus welcher Spalte die Werte (values_from) und woher die Spaltennamen (names_from) kommen sollen. Hier benötigen wir keine Anführungszeichen, da die Spalten bereits in unserem Datensatz enthalten sind. long_big5 %&gt;% pivot_wider( values_from = Auspraegung, names_from = Faktor ) # A tibble: 200 × 4 VPN Geschlecht Extraversion Neurotizismus &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 m 3 1.9 2 2 f 3.1 3.4 3 3 m 3.4 2.4 4 4 m 3.3 4.2 # … with 196 more rows Als grobe Daumenregel kann man sich merken, dass man nicht vorhandene Spalten mit Anführungszeichen übergeben muss. Auf bereits im Datensatz vorhandene Spalten kann man hingegen im Regelfall ohne Anführungszeichen zugreifen. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.7 Spalten trennen Mit der Funktion separate() können Spalten getrennt werden. Dies ist vor allem praktisch, wenn der Datensatz im langen Format vorliegt. Exemplarisch sei hier der im remp Package enthaltene Datensatz big5_zeit enthalten. big5_zeit # A tibble: 5 × 5 VPN Extrav_T1 Extrav_T2 NeurotFA NeurotFB &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 3.2 3.3 2.8 3.2 2 2 1.7 1.5 4.1 3.2 3 3 2.8 2.7 3.2 2.8 4 4 4.7 4.2 1.7 2.4 # … with 1 more row Um das hier bestehende Problem klarer zu machen, wandeln wir diesen erst einmal in ein langes Datenformat um. zeit1 &lt;- big5_zeit %&gt;% select(-NeurotFA, -NeurotFB) %&gt;% pivot_longer( cols = Extrav_T1:Extrav_T2, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) zeit1 # A tibble: 10 × 3 VPN Faktor Auspraegung &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Extrav_T1 3.2 2 1 Extrav_T2 3.3 3 2 Extrav_T1 1.7 4 2 Extrav_T2 1.5 # … with 6 more rows Nun sehen wir, dass die Spalte Faktor zwei Informationen enthält. Einmal den Persönlichkeitsfaktor (Extrav) und einmal den entsprechenden Messzeitpunkt (T1, T2). Nun können wir der Funktion separate() die zu trennende Spalte übergeben. Außerdem müssen wir noch spezifizieren, in welche verschiedene Spalten wir hier trennen müssen (hier Faktor und Zeitpunkt). Getrennt sind die beiden Informationen durch einen Unterstrich (_). Dies können wir durch das sep (Akronym für Separator) festlegen. Hätten wir in der Spalte Faktor mehr als zwei Informationen durch mehrere Unterstriche getrennt, müssten wir dem Argument into entsprechend drei Spaltennamen übergeben. zeit1 %&gt;% separate( col = Faktor, into = c(&quot;Faktor&quot;, &quot;Zeitpunkt&quot;), sep = &quot;_&quot; ) # A tibble: 10 × 4 VPN Faktor Zeitpunkt Auspraegung &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Extrav T1 3.2 2 1 Extrav T2 3.3 3 2 Extrav T1 1.7 4 2 Extrav T2 1.5 # … with 6 more rows Falls die Spalten nicht eindeutig durch einen Unterstrich, sondern beispielsweise durch Großschreibung voneinander getrennt werden sollen, kann man mit sep auch die Trennstelle (hier 6) festlegen. big5_zeit %&gt;% select(-Extrav_T1, -Extrav_T2) %&gt;% pivot_longer( cols = NeurotFA:NeurotFB, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) %&gt;% separate( col = Faktor, into = c(&quot;Faktor&quot;, &quot;Zeitpunkt&quot;), sep = 6 ) 6.8 Datensätze zusammenführen Eine ziemlich häufige und dabei leider oft schwierige Herausforderung ist das Zusammenführen von Datensätzen. Dies hat man beispielsweise bei den meisten biologischen Messungen, in denen man pro Person viele Messwerte erhält. Meist müssten dann die einzelnen Datensätze der verschiedenen Personen aneinander gebunden werden. Eine Möglichkeit zur Lösung dieses speziellen Szenarios haben wir bereits in Kapitel 5.1 kennengelernt. Dort haben wir mehrere Datensätze direkt mit der Funktion zum Einlesen zeilenweise (row bind) aneinander gebunden. import_list(dateien, rbind = TRUE) Ein weiteres sehr häufiges Szenario ist das Zusammenfügen beim Erheben zu verschiedenen Messzeitpunkten. Mit potentiellen Problemen, die einem dabei begegnen können und deren Lösungen werden wir uns im restlichen Teil dieses Kapitels detailliert auseinandersetzen. Natürlich gibt es noch diverse andere Anwendungsfälle, in denen ein Zusammenführen mehrerer Datensätze gewünscht ist. Dieses Kapitel sollte das grundlegende Verständnis vermitteln, mit jeder zukünftigen Problematik dieses Kontextes fertig zu werden. Zum Verstehen der verschiedenen Funktionen verwenden wir die im remp Package enthaltenden Datensätze namens A, B, C, D und E. In A bis D wurde bei drei Personen zu zwei Messzeitpunkten (T1, T2) jeweils die Konzentration von Low Density Lipoprotein (LDL) gemessen. Dieses Lipoprotein ist hauptsächlich für den Transport von Cholesterin zur Leber verantwortlich. Ein erhöhter Spiegel erhöht maßgeblich das Risiko eine Herz-Kreislauf Erkrankungen. Zwischen den beiden Messzeitpunkten wurden Maßnahmen wie Sport, Ernährungsumstellung und Statine (Medikamente) angewandt, sodass sich die Konzentration am zweiten Messzeitpunkt verringert hat. Die Spaltennamen von A und B sind dieselben, während sich die von C und D unterscheiden. Datensatz E enthält Daten von Person Nummer 3 sowie zwei weiteren Personen mit der ID 4 und 6. Schauen wir uns kurz Datensatz A A # A tibble: 3 × 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 und Datensatz B an. B # A tibble: 3 × 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 156 T2 2 2 102 T2 3 3 111 T2 Wie zu Beginn des Kapitels beschrieben, könnten wir aufgrund derselben Spaltennamen die beiden Datensätze zeilenweise zusammenbinden. Wir verwenden hier allerdings nicht die in R enthaltene rbind() Funktion sondern bind_rows() aus dem dplyr Package (enthalten im tidyverse). Zum Aneinanderbinden müssen wir diese beiden Datensätze einfach nacheinander der Funktion übergeben. Es können pro Aufruf immer nur zwei Datensätze aneinander gebunden werden. bind_rows(A, B) # A tibble: 6 × 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 4 1 156 T2 # … with 2 more rows Wir sehen, dass unser neue Datensatz nun 6 anstelle der vorherigen 3 Zeilen hat. Jede Person ist also jetzt doppelt enthalten. Mit der im Kapitel 6.6 kennengelernten Funktion pivot_wider() können wir dann den Datensatz wieder in ein breites Datenformat bringen. bind_rows(A, B) %&gt;% pivot_wider( values_from = LDL, names_from = Zeitpunkt, names_glue = &quot;LDL_{.name}&quot; ) # A tibble: 3 × 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 156 2 2 118 102 3 3 128 111 Das ist grundsätzlich eine sehr einfache und elegante Lösung. Wichtig hierbei ist die zwingende Notwendigkeit einer Spalte mit eindeutiger Zuordnung der Beobachtungen (zum Beispiel ID). Ansonsten können wir pivot_wider() nicht verwenden. Bei komplexeren Datensätzen stößt dieser Ansatz allerdings schnell an seine Grenzen. Sobald eine Zeile doppelt vorkommt, würde es beispielsweise schon nicht mehr funktionieren. Wenn die Spaltennamen nicht dieselben sind, können wir nicht mehr zeilenweise binden. Man könnte natürlich die Spalten in diesem trivialen Beispiel einfach umbenennen. Da man selten nur drei Spalten im gesamten Datensatz hat, nutzen wir diese Gelegenheit, um uns das spaltenweise Zusammenfügen anzuschauen. Zuerst werfen wir aber noch einen Blick in den Datensatz C C # A tibble: 3 × 3 ID LDL_T1 Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 und dann in D D # A tibble: 3 × 3 ID LDL_T2 Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 156 T2 2 2 102 T2 3 3 111 T2 Syntaktisch ändert sich im Vergleich zu vorher nichts. Nur das Suffix heißt nun anders. Wir verwenden also bind_cols(), indem wir einfach die beiden Datensätze als Argumente übergeben. df &lt;- bind_cols(C, D) New names: • `ID` -&gt; `ID...1` • `Zeitpunkt` -&gt; `Zeitpunkt...3` • `ID` -&gt; `ID...4` • `Zeitpunkt` -&gt; `Zeitpunkt...6` df # A tibble: 3 × 6 ID...1 LDL_T1 Zeitpunkt...3 ID...4 LDL_T2 Zeitpunkt...6 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 1 156 T2 2 2 118 T1 2 102 T2 3 3 128 T1 3 111 T2 Hier sehen wir einen Grund für die Benutzen der dplyr anstelle der Basisfunktion (z.B. cbind()) in Aktion. Die Funktion bind_cols() ändert automatisch doppelte Spaltennamen um. Schließlich liegen in beiden Datensätzen eine Spalte namens ID und Zeitpunkt vor. Jetzt kann man natürlich die Spalten der ID und beiden LDL Konzentrationen manuell auswählen (siehe Kapitel 6.2). df %&gt;% select(ID = `ID...1`, LDL_T1, LDL_T2) # A tibble: 3 × 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 156 2 2 118 102 3 3 128 111 Beachte an dieser Stelle die Notwendigkeit die Backticks vor und nach dem Spaltennamen ID…1, da wir keinen normalen Namen vorliegen haben. Bei komplexeren Datensätzen ist dieses spaltenweise Zusammenfügen jedoch sehr fehleranfällig. Daher sind in dplyr auch noch sogenannte Joining Funktionen enthalten. Diese kommen aus der Welt der Datenbanken, beziehungsweise aus der dort verbreiteten Sprache SQL. Um das Prinzip des Joinings etwas genauer zu betrachten, nehmen wir uns an dieser Stelle noch den E Datensatz zur Hand. E # A tibble: 3 × 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 111 T2 2 4 88 T2 3 6 93 T2 Wenn wir diesen mit Datensatz A zusammenführen möchte, könnten wir dies wieder eine Kombination aus bind_rows() und pivot_wider() erreichen. bind_rows(A, E) %&gt;% pivot_wider( values_from = LDL, names_from = Zeitpunkt, names_glue = &quot;LDL_{.name}&quot; ) # A tibble: 5 × 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 NA 2 2 118 NA 3 3 128 111 4 4 NA 88 # … with 1 more row Dies ist wie gesagt für komplexere Datensätze selten fehlerfrei machbar. Eine erste Alternative bietet daher die Funktion full_join(). Diese integriert alle Werte aus beiden Datensätze. full_join(A, E) Joining, by = c(&quot;ID&quot;, &quot;LDL&quot;, &quot;Zeitpunkt&quot;) # A tibble: 6 × 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 3 3 128 T1 4 3 111 T2 # … with 2 more rows Wir erhalten die Nachricht Joining, by = c(\"ID\", \"LDL\", \"Zeitpunkt\"), welche uns angibt, nach welchen Variablen gejoint wurde (hier alle). Wir können dies auch explizit mit dem by Argument spezifizieren. So kann man full_join() zwingen, nur die ID in die Zeilen zu schreiben und die restlichen Werte mit NAs (fehlenden Werten) aufzufüllen. full_join(A, E, by = &quot;ID&quot;) # A tibble: 5 × 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 NA &lt;NA&gt; 2 2 118 T1 NA &lt;NA&gt; 3 3 128 T1 111 T2 4 4 NA &lt;NA&gt; 88 T2 # … with 1 more row Diese Ausgabe können wir noch etwas verschönern, indem wir das Suffix T1 und T2 anstelle von x und y verwenden. Außerdem wählen wir nur die ID und LDL Spalten aus und lassen uns alle fünf Zeilen ausgeben. full_join(A, E, by = &quot;ID&quot;, suffix = c(&quot;_T1&quot;, &quot;_T2&quot;)) %&gt;% select(ID, starts_with(&quot;LDL&quot;)) %&gt;% print(n = 5) # A tibble: 5 × 3 ID LDL_T1 LDL_T2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 180 NA 2 2 118 NA 3 3 128 111 4 4 NA 88 5 6 NA 93 Wenn hingegen wie bei Datensatz A und B nur dieselben Personen zwei mal gefragt wurden, beide Datensätze also in dem Sinne vollständig sind, entstehen keine fehlenden Werte in Form von NA. full_join(A, B, by = &quot;ID&quot;) # A tibble: 3 × 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 156 T2 2 2 118 T1 102 T2 3 3 128 T1 111 T2 Neben dem full_join() gibt es noch andere Joins, die je nach Anwendungsfall angemessener sind. Der left_join() integriert nur die Werte vom zweiten (rechten) Datensatz in den ersten (linken) Datensatz, die im ersten (hier A) enthalten sind. left_join(A, E, by = &quot;ID&quot;) # A tibble: 3 × 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 NA &lt;NA&gt; 2 2 118 T1 NA &lt;NA&gt; 3 3 128 T1 111 T2 Möchte man nur die Werte aus dem ersten (linken) Datensatz A in den zweiten (rechten) Datensatz E integrieren, die im rechten Datensatz enthalten sind, kann die right_join() Funktion verwendet werden. Beachte, dass bei beiden Funktion hier explizit nach der ID zusammengeführt wird. Wenn wir also über das Vorhandensein von etwas in einem Datensatz reden, ist in Falle unserer Beispiele damit immer ID gemeint. right_join(A, E, by = &quot;ID&quot;) # A tibble: 3 × 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 128 T1 111 T2 2 4 NA &lt;NA&gt; 88 T2 3 6 NA &lt;NA&gt; 93 T2 Wenn wir nur diejenigen Werte integrieren möchten, die in beiden Datensätzen enthalten sind, verwenden wir inner_join(). Da hier nur die Person 3 in beiden Datensätzen vorhanden ist, wird auch nur diese vollständig zusammengeführt und ausgegeben. inner_join(A, E, by = &quot;ID&quot;) # A tibble: 1 × 5 ID LDL.x Zeitpunkt.x LDL.y Zeitpunkt.y &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 128 T1 111 T2 Abschließend gibt es noch zwei Funktion, die nicht direkt zusammenführen, sondern nur eine Bedingung prüfen und davon abhängig den ersten (linken) Datensatz zurückgeben. Die Funktion semi_join() gibt nur jene Werte aus dem ersten Datensatz zurück, welche im ersten (linken) und zweiten (rechten) vorkommen. semi_join(A, E, by = &quot;ID&quot;) # A tibble: 1 × 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 3 128 T1 Die Funktion anti_join() hingegen gibt nur die Werte aus dem ersten (linken) Datensatz zurück, die nicht im zweiten (rechten) Datensatz enthalten sind. anti_join(A, E, by = &quot;ID&quot;) # A tibble: 2 × 3 ID LDL Zeitpunkt &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 180 T1 2 2 118 T1 Bei sehr einfachen Szenarien kann man auf die Kombination aus bind_rows() und pivot_wider() zurückgreifen. Die Datensätze spaltenweise mithilfe von bind_cols() zusammenzuführen ist hingegen immer eine fehleranfällige und daher riskante Idee. Die Lösung ist in den meisten komplexeren Fällen ein sogenannter Join. Welche Art von Join man im eigenen Anwendungsfall letzten Endes verwendet, hängt immer vom Kontext ab und kann nicht generell beantwortet werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.9 Buchstaben und Wörter bearbeiten Oft muss man entweder die Spaltennamen oder die Inhalte verschiedener Spalten, die Characters enthalten in irgendeiner Form anpassen. In diesem Kapitel schauen wir uns an, wie man mit Funktionen aus stringr (enthalten im tidyverse) Character ersetzt (str_replace()), extrahiert (str_extract()) und entdeckt (str_detect()). Das Präfix str steht dabei für String – einem anderen Wort für Character. Ein häufiges Ärgernis im Kontext von Programmiersprachen sind Umlaute, da verschiedene Zeichenkodierungen diese intern verschieden übersetzen, so dass auf anderen Betriebssystemen zu Kauderwelsch kommen kann. Schauen wir uns im Folgenden an, wie man Umlaute ersetzt. Es sei der Satz char &lt;- &quot;Österreich hat 28610 schräge Berge&quot; gegeben. Möchte man nun das eine ä mit ae ersetzen, verwendet man str_replace(). str_replace( string = char, pattern = &quot;ä&quot;, replacement = &quot;ae&quot; ) [1] &quot;Österreich hat 28610 schraege Berge&quot; Dabei muss der zu ersetzende Buchstabe immer zuerst eingegeben werden. Nun haben wir aber immer noch ein ö im Satz enthalten. Für mehr als eine Anpassung verwendet man str_replace_all(). Die Syntax ist leider etwas kontra intuitiv, wenn man bereits select() und rename() kennengelernt hat. Hier ist der alte Name auf der linken Seite der jeweiligen Gleichung. str_replace_all( string = char, c(&quot;ä&quot; = &quot;ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;) ) [1] &quot;Oesterreich hat 28610 schraege Berge&quot; Aber wie geht man vor, wenn Spaltennamen Umlaute enthalten? Um dem auf den Grund zu gehen, erstellen wir uns einen neuen tibble (11.3), der die Preise für drei verschiedene Sägen in Österreich enthält. umlaut &lt;- tibble( Säge = c(&quot;Häxler&quot;, &quot;Sünde3000&quot;, &quot;Lölf4&quot;), Österreich = c(10.45, 4.60, 9.70) ) umlaut # A tibble: 3 × 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 2 Sünde3000 4.6 3 Lölf4 9.7 Die Namen enthalten jeweils einen Umlaut. Beachte, dass ein Umlaut groß geschrieben ist und die Funktionen case sensitive sind. Das bedeutet, dass wir mit den Befehlen oben nur kleine Buchstaben ersetzen und nicht ihre großen Äquivalente. An dieser Stelle verwenden wir eine im Kapitel 6.4.4 bereits eingeführte so genannte Lambda Funktion. umlaut %&gt;% mutate(across(where(is.character), ~ str_replace_all( string = .x, c(&quot;ä&quot; = &quot;ae&quot;, &quot;ö&quot; = &quot;oe&quot;, &quot;ü&quot; = &quot;ue&quot;, &quot;Ä&quot; = &quot;Ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;, &quot;Ü&quot; = &quot;Ue&quot;) ))) # A tibble: 3 × 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Haexler 10.4 2 Suende3000 4.6 3 Loelf4 9.7 Möchten wir alle Spaltennamen von Umlauten befreien, könnten wir dies mithilfe von rename_with() und str_replace_all() erreichen. umlaut %&gt;% rename_with(~ str_replace_all( string = .x, c(&quot;ä&quot; = &quot;ae&quot;, &quot;ö&quot; = &quot;oe&quot;, &quot;ü&quot; = &quot;ue&quot;, &quot;Ä&quot; = &quot;Ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;, &quot;Ü&quot; = &quot;Ue&quot;) )) # A tibble: 3 × 2 Saege Oesterreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 2 Sünde3000 4.6 3 Lölf4 9.7 Für das Extrahieren von Buchstaben oder Zahlen können wir str_extract() verwenden. Wir nehmen wieder unseren Beispielsatz von oben, der als char gespeichert ist. Wir könnten beispielsweise die Zahl herausfiltern, indem wir einen sogenannten Regex verwenden (Akronym für Regular Expression). Diese sind grundsätzlich sehr komplex und benötigen zur vernünftigen Einführung ein eigenes Buch. In der Praxis muss man in der Regel nur Online nach dem gewünschten Regex suchen, ohne selbst jeden Kniff und jedes Detail zu kennen. Um eine Zahl mit mehr als einer Ziffer herauszuholen, könnte man nach \"\\\\d+\" suchen. Der erste Backslash ist nur nötig, weil wir mit R arbeiten. Danach folgt ein weiterer Backslash sowie ein d (für digit, engl. für Ziffer) und ein Plus zur Signalisierung, dass es sich hier auch um eine Zahl mit mehreren Ziffern handeln kann. str_extract(char, &quot;\\\\d+&quot;) [1] &quot;28610&quot; Die Funktion str_detect() entdeckt bestimmte Buchstaben, Wörter oder ganze Regex. Dabei gibt die Funktion einen logischen Wert aus (TRUE, FALSE), wenn das Gesuchte gefunden oder nicht gefunden wurde. Das ist daher praktisch, da man diese Funktion für logische Bedingungen innerhalb von if_else() oder case_when() verwenden kann. str_detect(char, &quot;\\\\d+&quot;) [1] TRUE df %&gt;% filter(str_detect(a, &quot;canal&quot;)) df %&gt;% rename_with( .fn = ~ str_extract(., &quot;([^\\\\s]+)&quot;), .cols = starts_with(&quot;Q&quot;) ) In der Praxis müssen die Funktionen aus dem stringr Package in der Regel in Kombination mit mutate() verwendet werden. Einen weiteren Anwendungsfall stellt der Umbruch langer Achsenbeschriftung bei der Erstellung von Visualisierungen mithilfe von str_wrap() dar. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.10 Faktoren verändern Falls du nicht mehr genau im Kopf hast, was Faktoren sind, schaue Dir noch einmal Kapitel 4.2.2 an. Faktoren sind vor allem zur automatischen Erstellung von Dummy Variablen für Regressionsmodelle (siehe Kapitel 9.6.1), für das richtige Abbilden und das Ändern der Reihenfolge für das Erstellen von Visualisierungen (siehe Kapitel 8.6) sowie für andere inferenzstatistische Verfahren nützlich. Im Folgenden schauen wir uns Beispiele an, wie man mit einer Funktion aus dem forcats Package (mit dem tidyverse geladen) Faktoren umbenennen (fct_recode()) und deren Reihenfolge ändern (fct_relevel(), fct_reorder()) kann. Dafür verwenden wir die Spalte Gruppe aus dem big5_mod Datensatz mit den Faktorstufen (oder Leveln) Jungspund, Mittel, und Weise. big5_mod %&gt;% relocate(Gruppe) # A tibble: 200 × 6 Gruppe Alter Geschlecht Extraversion Neurotizismus ID &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 Mittel 36 m 3 1.9 1 2 Jung 30 f 3.1 3.4 2 3 Jung 23 m 3.4 2.4 3 4 Weise 54 m 3.3 4.2 4 # … with 196 more rows Um die Veränderungen der Faktorstufen besser darstellen zu können, ziehen wir uns die Spalte Gruppe aus dem Datensatz heraus (siehe Kapitel 5.3). Zuvor müssen wir die Spalte allerdings noch zum Datentyp Faktor umwandeln. faktoren &lt;- big5_mod %&gt;% mutate(Gruppe = as.factor(Gruppe)) %&gt;% pull(Gruppe) Zum Anzeigen der Faktorstufen, verwenden wir levels. faktoren %&gt;% levels() [1] &quot;Jung&quot; &quot;Mittel&quot; &quot;Weise&quot; Dass die Reihenfolge schon der Altersreihenfolge entspricht, liegt nur daran, dass wir die Faktorstufen oben genau spezifiziert haben. Ansonsten können durchaus unerwartete Reihenfolgen der Faktorstufen auftreten. Es lohnt sich also in jedem Fall ein Blick in die Faktorstufen, bevor man sie verwendet. Möchte man einzelne Faktoren umbenennen, verwendet man fct_recode(). faktoren %&gt;% fct_recode(Alt = &quot;Weise&quot;) %&gt;% levels() [1] &quot;Jung&quot; &quot;Mittel&quot; &quot;Alt&quot; Möchten wir eine bestimmte Faktorstufe an erster Stelle haben, verwenden wir fct_relevel() mit der gewünschten Stufe als Character. faktoren %&gt;% fct_relevel(&quot;Mittel&quot;) %&gt;% levels() [1] &quot;Mittel&quot; &quot;Jung&quot; &quot;Weise&quot; Zum Ändern der gesamten Reihenfolge kann man beliebig viele weitere Faktorstufen der Funktion übergeben. faktoren %&gt;% fct_relevel(&quot;Weise&quot;, &quot;Mittel&quot;, &quot;Jung&quot;) %&gt;% levels() [1] &quot;Weise&quot; &quot;Mittel&quot; &quot;Jung&quot; Wenn die Reihenfolge der Faktoren in absteigender (.desc = TRUE) oder aufsteigender (.desc = FALSE) Reihenfolge z.B. in Abhängigkeit des Mittelwertes einer anderen Spalte (wie dem Ausmaß an Extraversion) sortiert werden soll, verwendet man fct_reorder(). extraversion &lt;- big5_mod %&gt;% pull(Extraversion) faktoren %&gt;% fct_reorder(extraversion, .fun = mean, .desc = TRUE) %&gt;% levels() [1] &quot;Mittel&quot; &quot;Jung&quot; &quot;Weise&quot; Dabei kann man mit dem Argument .fun die gewünschte Funktion zur Auswertung der zweiten Variable (hier Extraversion) verwenden. In den am Anfang des Kapitels beschriebenen Anwendungsfällen wird es noch klarer werden, weshalb die Notwendigkeit von Faktoren besteht und inwiefern Dir die Funktionen aus dem forcats Package konkret das Leben erleichtern. big5_mod %&gt;% mutate(Gruppe = fct_relevel(Gruppe, &quot;Jung&quot;)) # A tibble: 200 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # … with 196 more rows big5_mod %&gt;% mutate(Gruppe = fct_reorder( Gruppe, Alter, .fun = mean, .desc = FALSE) ) # A tibble: 200 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # … with 196 more rows TEXT Vorsicht, wenn eine Faktorauspraegung nicht vorhanden. Dann wichtig, dass man mit factor und levels explizit definiert, was vorliegen sollte. Faktoren sollten erst unmittelbar vor Verwendung erstellt und verändert werden. Es kann im Umgang von Faktoren zu seltsamen Fehlermeldungen kommen, da diese innerhalb von R als Integer (Zahl) und nicht als Character (Buchstabenfolge) behandelt werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 6.11 Mit Zeitdaten arbeiten library(lubridate) In Kapitel 4.2.3 wurden zwei verschiedene Zeitdatentypen bereits eingeführt. Wir schauen uns an dieser Stelle noch einige nützliche Funktionen aus dem lubridate Package an. Möchte man ein Datum auseinander nehmen, kann man die Funktionen year(), month() und day() verwenden. daten %&gt;% mutate(across(where(is.POSIXct), as.Date)) daten %&gt;% mutate( Birthday = ymd(Birthday), Age = 2022 - year(Birthday) ) daten1 %&gt;% mutate( Alter_Diagnose = (ymd(Erstdiagnose) - ymd(Geburtsdatum)) / dyears(1), OS_zeit = ymd(OS_datum) - ymd(Erstdiagnose), ) 6.12 Binäre Antwortmatrix erstellen Wenn man Daten im Rahmen von Fragebögen erhebt, kriegt man meist nicht zurück, ob ein Item (eine Aufgabe oder Frage) richtig oder falsch beantwortet wurde. Stattdessen werden nur die abgegeben Antworten im Datensatz gespeichert. Mit data_binary() aus dem remp Package kann man den Datensatz in die gewünschte binäre Antwortmatrix umwandeln. Sprich, man erhält die Information für jedes Item pro Person, ob das Item richtig (1) oder falsch (0) beantwortet wurde. Wenn wir einen Beispielsdatensatz mit drei Items und vier Antwortoptionen haben, die vier Personen beantwortet haben, df2 &lt;- big5 %&gt;% select(O1:O3) df2 # A tibble: 200 × 3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 1 5 2 5 3 5 3 3 3 5 4 2 5 3 # … with 196 more rows bei Item 1 die Antwort 3 richtig ist, bei Item 2 die Antwort 2 und bei Item 3 die Antwort 4, würde man die Antworten in einer Variable speichern. df2 %&gt;% data_binary(answers = c(4, 1, 5)) # A tibble: 200 × 3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 1 1 2 0 0 1 3 0 0 1 4 0 0 0 # … with 196 more rows Nun benutzen wir die Funktion konsistent zu den bisher gelernten in Kombination mit der Pipe. big5 %&gt;% select(Geschlecht, O1:O3) %&gt;% data_binary(answers = tibble(&quot;f&quot;, 4, 1, 5)) # A tibble: 200 × 4 Geschlecht O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 0 1 1 2 1 0 0 1 3 0 0 0 1 4 0 0 0 0 # … with 196 more rows "],["descr.html", "Kapitel 7 Deskriptive Statistik 7.1 Verschiedene Lagemaße 7.2 Häufigkeiten und Kontingenztafeln", " Kapitel 7 Deskriptive Statistik 7.1 Verschiedene Lagemaße Deskriptive Statistiken sind wichtig, um einen ersten Überblick über die Daten zu erhalten. Wir werden an dieser Stelle den big5_mod Datensatz verwenden. big5_mod # A tibble: 200 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # … with 196 more rows Für die Berechnung von Mittelwert (mean()) und Standardabweichung (sd()) verwenden wir die im tidyverse enthaltene Funktion summarise(). Das tidyverse wird in Kapitel 6 eingeführt. Auf der linken Seite des Gleichheitszeichens stehen auch hier wieder die Namen der neu erstellten Spalten. big5_mod %&gt;% summarise( mean = mean(Extraversion), sd = sd(Extraversion) ) # A tibble: 1 × 2 mean sd &lt;dbl&gt; &lt;dbl&gt; 1 3.08 0.347 Zum Gruppieren der Variablen wird group_by() verwendet. big5_mod %&gt;% group_by(Geschlecht) %&gt;% summarise( mean = mean(Extraversion), sd = sd(Extraversion) ) # A tibble: 2 × 3 Geschlecht mean sd &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 f 3.05 0.358 2 m 3.11 0.328 Dabei können beliebig viele Spalten der Funktion übergeben werden. So könnte man beispielsweise nicht nur nach Geschlecht, sondern auch nach der Altersgruppe gruppieren. big5_mod %&gt;% group_by(Geschlecht, Gruppe) %&gt;% summarise( mean = mean(Extraversion), sd = sd(Extraversion) ) # A tibble: 6 × 4 # Groups: Geschlecht [2] Geschlecht Gruppe mean sd &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 f Jung 3.07 0.373 2 f Mittel 3.07 0.299 3 f Weise 2.83 0.269 4 m Jung 3.12 0.324 # … with 2 more rows Neben dem Mittelwert und der Standardabweichung gibt es noch diverse weitere Funktionen: n() für die Anzahl an Beobachtungen, min() und max() für Minimum und Maximum, var() für die Varianz, sqrt() für die Quadratwurzel, median() für den Median und quantile() zur Berechnung der jeweiligen Quantile. Für die Berechnung des Standardfehlers teilen wir direkt innerhalb des Funktionsaufrufes die Standardabweichung durch die Wurzel aus der Anzahl der Personen. big5_mod %&gt;% summarise( N = n(), Min = min(Alter), Mean = mean(Alter), Median = median(Alter), Max = max(Alter), SD = sd(Alter), SE = SD / sqrt(N) ) Grundsätzlich kann jede Funktion summarise() übergeben werden, die einen einzelnen Wert berechnet. Somit unterscheidet sich die Anwendung maßgeblich vom bereits kennengelernten mutate(). Dort musste die Ausgabe immer eine Reihe von Werten umfassen, die der Anzahl der Zeilen im Datensatz entspricht. Auch hier können wir mehrere Spalten gleichzeitig mithilfe von across() auswerten (siehe Kapitel 6.4.2). Die Syntax ändert sich in dem Fall im Vergleich zu vorher. Hier müssen wir die verschiedenen Funktionen mit entsprechendem Namen innerhalb einer Liste übergeben. Listen als solche werden erst später eingeführt und müssen uns an dieser Stelle nicht weiter interessieren (siehe Kapitel (lists)). big5_mod %&gt;% summarise(across( .cols = Extraversion:Neurotizismus, .fns = list(mean = mean, sd = sd)) ) # A tibble: 1 × 4 Extraversion_mean Extraversion_sd Neurotizismus_mean Neurotizismus_sd &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.08 0.347 3.13 0.682 Bei vielen Lagemaßen kann es schon einmal recht viel zu schreiben sein. Wenn man keine Lust hat, dies jedes Mal manuell abzutippen, kann man auch direkt vereinfachte Funktionen zur deskriptiven Statistik verwenden. Das remp Package bietet die Funktion descriptive() an. Dieser muss man kein weiteres Argument übergeben. Es wird dann die Anzahl, das Minimum, das erste Quartil, der Mittelwert, der Median, das zweite Quartil, die Standardabweichung und der Standardfehler für sämtliche numerische Spalten zurückgegeben. Alle anderen Datentypen werden von dieser Funktion ignoriert. big5_mod %&gt;% descriptive() # A tibble: 4 × 10 Variable N Min Q1 Mean Median Q3 Max SD SE &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Alter 200 13 18.8 26.5 23 31.2 60 11.4 0.8 2 Extraversion 200 2.3 2.8 3.08 3 3.3 4.3 0.35 0.02 3 ID 200 1 50.8 100. 100. 150. 200 57.9 4.09 4 Neurotizismus 200 1.4 2.7 3.13 3.1 3.6 4.6 0.68 0.05 Auch hier können wir die Berechnungen auf die selbe Art und Weise gruppieren. big5_mod %&gt;% group_by(Geschlecht) %&gt;% descriptive() Alternativen für einen schnellen Überblick bieten beispielsweise auch das skimr Package mit der Funktion skim() oder describe() aus dem psych Package. Beide können auch nicht-numerische Spalten auswerten und erstere gibt zu jeder Spalte sogar ein kleines Histogramm aus. Wie dir vielleicht bereits aufgefallen ist, sieht die Ausgabe von descriptive() anders aus als die von summarise(). Während erstere die Variablen untereinander in unterschiedliche Zeilen übersichtlich auflistet, fügt summarise() die Ergebnisse spaltenweise hinzu. Wenn wir den selben Output wie in descriptive() erreichen möchten, müssen wir zuerst den Datensatz in ein langes Format bringen (siehe Kapitel 6.6). Nun gruppieren wir nach der neuen Spalte namens Variable. Anschließend können wir wie gewohnt mit summarise() die deskriptiven Statistiken berechnen. Nichts anderes macht die Funktion descriptive() intern. big5_mod %&gt;% pivot_longer( cols = c(Alter, Extraversion, Neurotizismus), names_to = &quot;Variable&quot;, values_to = &quot;Wert&quot; ) %&gt;% group_by(Variable) %&gt;% summarise( Q1 = quantile(Wert, 0.25), Mean = mean(Wert), Q3 = quantile(Wert, 0.75), Schiefe = skewness(Wert), Kurtosis = kurtosis(Wert) ) # A tibble: 3 × 6 Variable Q1 Mean Q3 Schiefe Kurtosis &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Alter 18.8 26.5 31.2 1.34 3.97 2 Extraversion 2.8 3.08 3.3 0.761 3.95 3 Neurotizismus 2.7 3.13 3.6 -0.132 2.56 Beachte an dieser Stelle, dass für die externen Funktionen skewness() und kurtosis() das moments Package installiert und geladen sein muss. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 7.2 Häufigkeiten und Kontingenztafeln Eine Möglichkeit an Häufigkeiten zu kommen, haben wir mit n() innerhalb von summarise() bereits kennengelernt. Eine alternative Möglichkeit ist die count() Funktion aus selbigem Package. Hier können wir uns group_by() sparen und stattdessen die gruppierenden Spalten direkt in count() schreiben. big5_mod %&gt;% count(Geschlecht, Gruppe) # A tibble: 6 × 3 Geschlecht Gruppe n &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 f Jung 89 2 f Mittel 20 3 f Weise 9 4 m Jung 58 # … with 2 more rows Für die Erstellung von Kontingenztafeln benötigen, die wir mit statistischen Tests auswerten können, benötigen wir allerdings eine andere Funktion namens table(). Dieser müssen wir für eine klassischen Vierfeldertafel zwei Argumente in Form von einzelnen Spalten (oder Vektoren) übergeben. Wir müssen die Spalten also, wie in Kapitel 4.4 bereits besprochen, aus dem Datensatz extrahieren. Wenn wir also untersuchen möchten, es Unterschiede in der Häufigkeit extrovertierter Menschen zwischen den Geschlechtern gibt, wählen wir erst die Spalte Extraversion des Datensatzes big5_mod und anschließend. Beachte die logische Abfrage. Wenn Extraversion größer 3 ist, schreibe 1 und ansonsten 0. table(big5_mod$Extraversion &gt; 3, big5_mod$Geschlecht) f m FALSE 66 43 TRUE 52 39 Manchmal ist es interessant, rein deskriptiv nach einer weiteren Kategorie aufzuteilen. Diese können in gleicher Manier der Funktion table() übergeben werden. table(big5_mod$Extraversion &gt; 3, big5_mod$Geschlecht, big5_mod$Neurotizismus &gt; 3.5) , , = FALSE f m FALSE 47 39 TRUE 33 23 , , = TRUE f m FALSE 19 4 TRUE 19 16 Möchte man die Kontingenztafel für jede Spalte ausgeben lassen, kann table() ohne Klammern in der Funktion map() genutzt werden. Diese wendet table() auf jeden Spalte der Reihe nach an und speichert diese in Form einer Liste ab. big5_mod %&gt;% map(table) Der Übersicht halber schalten wir hier noch ein select() davor und wählen Geschlecht und die Altersgruppe aus. Wir könnten aber auf gleiche Art und Weise sämtlich Spalten in einer Kontingenztafel ausgeben lassen. big5_mod %&gt;% select(Geschlecht, Gruppe) %&gt;% map(table) $Geschlecht f m 118 82 $Gruppe Jung Mittel Weise 147 39 14 Diese Art des Berechnens nennt man auch iterativ, da in jeder Iteration die Berechnung auf eine neue Spalte angewandt wird. Diese Art der Berechnung wird in Kapitel 12 genauer erläutert. Vom Prinzip her ist die Anwendung von map() in diesem Fall wie das Nutzen von across() innerhalb von mutate() (siehe Kapitel 6.4.2). Allerdings muss die Ausgabe in mutate() oder auch in summarise() das gleiche Format für alle Spalten haben. Wenn wir Kontingenztafeln mit unterschiedlich vielen Kategorien in den jeweiligen Spalten erstellen, ist diese Bedingung nicht erfüllt. Daher müssen wir auf map() zurückgreifen. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). "],["visual.html", "Kapitel 8 Visualisierungen 8.1 Einführung 8.2 Histogramm und Dichte 8.3 Streudiagramm 8.4 Boxplot 8.5 Violin Plot 8.6 Balkendiagramm 8.7 Liniendiagramm 8.8 Quantil-Quantil Plot 8.9 Mehrfaktorielle Abbildungen 8.10 Anpassen des Aussehens 8.11 Anordnen mehrerer Graphen 8.12 Speichern von Graphen 8.13 Exemplarische Erweiterungen 8.14 Anwendungsbeispiel", " Kapitel 8 Visualisierungen 8.1 Einführung Für die sämtliche Visualisierungen werden wir das ggplot Package aus dem tidyverse verwenden. ggplot steht dabei für grammar of graphics. Genau wie in der Datenvorbereitung geht es also auch hier darum, das zugrundeliegende Prinzip einmal zu verstehen, um es dann auf verschiedenste Abbildungstypen anzuwenden. Wir werden wie bei der deskriptiven Statistik (siehe Kapitel 7) auch hier mit der leicht modifizierten Variante des Big Five Datensatzes arbeiten. big5_mod # A tibble: 200 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # … with 196 more rows In Abbildung 8.1 sind die grundsätzlichen Komponenten links und je ein entsprechendes Beispiel rechts abgebildet. Jeder ggplot besteht aus verschiedenen Layern, die untereinander gelegt werden. Um eine Abbildung zu erstellen, muss man auf jeden Fall Data, Aestetics und Geometries verwenden. Die Layer namens Scales und Theme sind optional und passen lediglich das Erscheinungsbild an. Wir werden uns in Kapitel 8.2 bis X) die jeweiligen Data, Aestetics und Geometries anschauen. Erst in Kapitel 8.10 werden die Anpassungsmöglichkeiten mithilfe der Scales und Theme Layer umfangreich eingeführt. Abbildung 8.1: Vereinfachte Anordnung der Layer im Rahmen der Grammar of Graphics mit Beispielen. Es ist wichtig zu verstehen, dass die Layer untereinander gelegt werden und man am Ende von unten auf die kreierte Abbildung schaut. Wenn man beispielsweise mehrere Geometries hintereinander in einen Plot einbaut, kann der zuletzt hinzugefügte den zuvor hinzugefügten (teilweise) überdecken. Bleiben wir bei dem Beispiel der Erstellung eines Histogramms aus Abbildung 8.1. ggplot(data = big5_mod, mapping = aes(x = Extraversion)) + geom_histogram() Innerhalb der Funktion ggplot() wird dem data Argument der Datensatz big5_mod übergeben. Aus diesem Datensatz möchten wir die Spalte Extraversion auf der x-Achse abgebildet haben. Die ersten beiden Layer sind somit bereits implementiert. Der Aestetics Layer wird durch das mapping Argument hinzugefügt. Allerdings können wir die Spalte nicht einfach mit x = Extraversion hinzufügen, sondern benötigen die zusätzliche Funktion aes(), der wir unsere Spalte übergeben. Dabei steht aes() für aesthetics (engl. für Ästhetik). Neben der Spalte, die auf der x-Achse abgebildet werden soll, kann auf die selbe Art und Weise die y-Achse definiert werden. Auch Argumente zur Veränderung des Aussehens wie color (Außenfarbe) oder fill (Füllfarbe) können hier innerhalb von aes() der Funktion ggplot übergeben werden. Nun haben wir oben bereits gesehen, dass das Histogramm erst mit der Funktion geom_histogram() hinzugefügt wird. Das Präfix geom ist dabei für jeden Geometry Layer der selbe. Für ein einfaches Histogramm muss an dieser Stelle nichts weiteres getan werden. Wichtig ist an dieser Stelle noch das +, welches sämtliche Layer zusammenbindet und untereinander hinzufügt. Anders als in Kapitel 6 wird hier also nicht die Pipe (%&gt;%), sondern ein Plus-Zeichen verwendet. Anders als sonst können Funktionen aus ggplot2 nicht mit einer Pipe aneinander gebunden werden. Das hat ausschließlich historische Gründe, da zu der Zeit der Erstellung von ggplot2 die Pipe noch nicht existiert hat. Dies wird sich in Zukunft auch nicht mehr ändern. Schauen wir uns die einzelnen Befehle einmal genauer an. Die erste Zeile kreiert erst einmal nur den Plot an sich und kein Histogramm. Die beiden Argumente data und mapping schreiben wir von nun an nicht mehr extra dazu, da wir die Reihenfolge dieser Argumente im Verlaufe des Buches ändern. ggplot(big5_mod, aes(x = Extraversion)) Wie zuvor kurz erwähnt, beginnt die Funktion zur eigentlichen Visualisierung der Daten mit geom_ und endet mit dem Namen der Abbildungsart (hier Histogramm). ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram() Das standardmäßige Aussehen mit dem grauen Hintergrund ist ein ggplot in der Form nicht publikationsreif. In Kapitel 8.10 werden diverse Anpassungsmöglichkeiten dieses flexiblen und sehr umfangreichen Packages erläutert werden. Zuvor werden wir allerdings erst einmal die üblichsten Abbildungsarten nacheinander Schritt für Schritt durchgehen. 8.2 Histogramm und Dichte Das Histogramm wurde exemplarisch bereits in der Einführung beschrieben. Wir erinnern uns, in dem Fall bedarf es nur der Zuweisung der Variable für die x-Achse, da auf der y-Achse die Häufigkeitsverteilung dargestellt wird. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram() Für ein schlichteres Aussehen fügen wir in Abbildung 8.2 (b) noch eine schwarze Rahmenfarbe mit dem color Argument und eine weiße Füllungsfarbe mit fill hinzu. Histogramme sind maßgeblich von der gewählten Breite der Balken abhängig. Bei zu wenigen Balken können Informationen der Verteilung verloren gehen, bei zu vielen hingegen irrelevante Trends erscheinen. Dieses kann entweder direkt mit der Anzahl der Balken (bins Argument) oder mit der Breite (binwidth Argument) verändert werden. Wir werden hier binwidth verwenden, da man dieser auch eine Funktion übergeben kann. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = 0.2 ) Abbildung 8.2: Histogramme Es gibt verschiedene Arten, eine möglichst optimale binwidth herauszufinden. Exemplarisch sei hier die Freedman-Diaconis Regel gezeigt. Wie bereits in Kapitel 6.4.4 eingeführt, erstellen wir dafür eine Funktion namens opt_bin(). Diese neu erstellte Funktion muss man erst einmal speichern, in dem man den obigen Befehl ausführt. Nun können wir opt_bin direkt in geom_histogram() einfügen (siehe Abbildung 8.3 (a)). ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = function(x) (max(x) - min(x)) / nclass.FD(x) ) Eine weitere Möglichkeit, die Verteilung der Extraversion in unserer Population darzustellen, ist die Wahrscheinlichkeitsdichte. Dazu müssen wir lediglich den Suffix des Geometry Layers zu density (engl. für Dichte) verändern (siehe Abbildung 8.3 (b)). ggplot(big5_mod, aes(x = Extraversion)) + geom_density() Möchten wir nun das Histogramm gemeinsam mit der Wahrscheinlichkeitsdichte abbilden, müssen wir erst einmal beide auf die selbe Skala bringen. Hier möchten wir exemplarisch die Häufigkeiten des Histogramms ebenfalls als Dichte ausgedrückt haben. Dafür bedarf es eines kleinen Tricks. Dazu muss der Histogrammfunktion ebenfalls mit aes() ein Wert für die y-Achse übergeben werden. Mit ..density.. wird die Dichte berechnet. Die beiden Punkte vor und hinter der Dichte signalisieren der Funktion, dass etwas berechnet werden soll. Anschließend muss lediglich mit einem weiteren Plus-Zeichen die Dichtefunktion hinzugefügt werden. Für eine ansprechendere optische Darstellung fügen wir noch eine graue Füllfarbe hinzu und machen diese mit alpha leicht durchsichtig. Das Ergebnis kann in Abbildung 8.3 (c) betrachtet werden. ggplot(big5_mod, aes(x = Extraversion, height = stat(density))) + geom_histogram( mapping = aes(y = stat(density)), binwidth = 0.2, color = &quot;black&quot;, fill = &quot;white&quot; ) + geom_density( fill = &quot;grey&quot;, alpha = 0.7 ) Abbildung 8.3: Histogramm und Dichte Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.3 Streudiagramm Um ein Streudiagramm zu erstellen, müssen wir neben der x-Achse nun ebenfalls die y-Achse festlegen. Ansonsten ändert sich nur der Geometry Layer zu geom_point(). Exemplarisch sei hier die mittlere Extraversion gegen das Lebensalter aufgetragen (siehe Abbildung 8.4 (a)). Auf weitere Parameter wie das Ändern der Farbe (color) oder Form (shape) verzichten wir an dieser Stelle. ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point() Nun sieht dieses Streudiagramm etwas seltsam aus, weil die zugrunde liegenden Daten ordinal und nicht metrisch skaliert waren. Die Extraversion wurde mit einem Fragebogen ermittelt und daraus dann der Mittelwert gebildet. Dadurch ist natürlich kein richtiges metrisches Skalenniveau gegeben, weswegen die Punkte hier in Reih und Glied erscheinen. Bei größeren Datensätzen kann es passieren, dass die Punkte sich in einem derartigen Szenario überlappen. Um das zu verhindern, kann die Position zu jitter verändert werden. Dies bewirkt eine leichte zufällige Variation jedes Datenpunktes (siehe Abbildung 8.4 (b)). ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) Mit geom_smooth() kann man eine am besten passende Linie durch die Punkte ziehen. Wir entscheiden uns an der Stelle für eine lineare Regressionsgrade (method = lm). Für alternative Methoden sei auf die Dokumentation der Funktion verwiesen. Außerdem färben wir die Grade schwarz und fügen ein 95% Konfidenzintervall mit se = TRUE hinzu. Damit der Regressionsgrade nicht nur in dem Bereich, in dem Daten beobachtet wurden, abgebildet wird, kann zusätzlich das fullrange Argument auf TRUE gesetzt werden. Um den Effekt dieser Funktion zu illustrieren, greifen wir an dieser Stelle etwas voraus und definieren mit xlim(c(2, 5)) die untere Grenze der x-Achse mit 2 und die obere mit 5 (siehe Abbildung 8.4 (c)). ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) + geom_smooth( color = &quot;black&quot;, method = lm, se = TRUE, fullrange = TRUE ) + xlim(c(2, 5)) Abbildung 8.4: Streudiagramm und Regressionsgrade Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.4 Boxplot Würde man nur einen Boxplot für die mittlere Ausprägung von Extraversion erstellen wollen, könnte man dies genau wie in den beiden zuvor besprochenen Kapiteln nur durch das Auswechseln des Geomitry Layers mit geom_boxplot() erreichen. Dies ist allerdings eine seltene Situation. Meistens möchte man auf der x-Achse mehrere Variablen miteinander vergleichen. Wir möchten an dieser Stelle zum Beispiel Extraversion und Neurotizismus miteinander vergleichen. Um dies zu erreichen, müssen wir den Datensatz vom breiten ins lange Datenformat transformieren. Wenn Dir das lange Datenformat kein Begriff ist, schaue Dir noch einmal das Kapitel 6.6 genauer an. Dort wird die hier verwendete Funktion pivot_longer() ausführlich eingeführt. big5_long &lt;- big5_mod %&gt;% pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) big5_long # A tibble: 400 × 6 Alter Geschlecht Gruppe ID Faktor Auspraegung &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 36 m Mittel 1 Extraversion 3 2 36 m Mittel 1 Neurotizismus 1.9 3 30 f Jung 2 Extraversion 3.1 4 30 f Jung 2 Neurotizismus 3.4 # … with 396 more rows Nun sind unsere Persönlichkeitsfaktoren in der Spalte Faktor und die Werte aus den Spalten in Auspraegung. Einen Boxplot erstellt man mit geom_boxplot(). Auf der x-Achse möchten wir die Persönlichkeitsfaktoren und auf der y-Achse die mittleren Ausprägungen darstellen (siehe Abbildung 8.5 (a)). ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_boxplot() Um zusätzlich Fehlerbalken zu erhalten müssen wir diese mit stat_boxplot() berechnen und ausgeben lassen. Als Argument muss das geom Argument auf \"errorbar\" (engl. für Fehlerbalken) gesetzt werden. Die Breite des Fehlerbalkens kann durch das optionale Argument width kontrolliert werden. Zum Ausblenden der Ausreißer setzt man innerhalb von geom_boxplot() die outlier.shape auf NA (Akronym für Not Available). Das Ergebnis ist in Abbildung 8.5 (b) illustriert. ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + stat_boxplot(geom = &quot;errorbar&quot;, width = 0.4) + geom_boxplot(outlier.shape = NA) Abbildung 8.5: Boxplots Wichtig ist dabei die Reihenfolge der Funktionsaufrufe. Wie bereits erwähnt, werden die verschiedenen Layer untereinander gezeichnet. Würden wir also zunächst geom_boxplot() und erst anschließend stat_boxplot() zum ggplot hinzufügen, würde die Linie des Fehlerbalkens über den Boxplot gezeichnet werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.5 Violin Plot Für die Violin Plots nutzen wir, genau wie zuvor bei den Boxplots (siehe Kapitel 8.4), den Datensatz im langen Datenformat. Dieser heißt big5_long und wurde mithilfe von pivot_longer() erstellt. Es ändert sich nichts außer das Geometry Layer, welches nun geom_violin() heißt (siehe Abbildung 8.6 (a)). ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_violin() Optional kann zusätzlich das Argument trim auf FALSE gesetzt werden, um das Abschneiden der Enden des Violin Plots zu verhindern. Mit dem Argument können wir explizit Quantile (hier 25%, 50%, 75% ) einzeichnen lassen. Das Ergebnis ist in Abbildung 8.6 (b) zu sehen. ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75) ) Abbildung 8.6: Violin Plots Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.6 Balkendiagramm Bei Balkendiagrammen möchte man in der Regel mehrere Merkmale miteinander vergleichen. Deswegen müssen wir, genau wie auch bereits bei den Boxplots und Violin Plots, den Datensatz erst in ein langes Format bringen. In Kapitel 6.6 wird erläutert, wie das im Detail funktioniert. big5_long &lt;- big5_mod %&gt;% pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) Nun möchte man in Balkendiagrammen häufig Mittelwerte oder andere konkrete Einzelwerte miteinander vergleichen. Deswegen müssen wir vor der graphischen Darstellung erst die Mittelwerte berechnen. Zusätzlich berechnen wir die Standardabweichung zur späteren Erstellung der Fehlerbalken. Wie das funktioniert, wurde bereits in Kapitel 7 eingeführt. big5_means &lt;- big5_long %&gt;% group_by(Faktor) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means # A tibble: 2 × 3 Faktor Mean SD &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion 3.08 0.347 2 Neurotizismus 3.13 0.682 Nun kann man mit der Funktion geom_col() genau diese Mittelwerte abbilden. Auf der x-Achse sind demnach wie zuvor auch die Persönlichkeitsfaktoren und auf der y-Achse die Mittelwerte aufgetragen (siehe Abbildung 8.7 (a)). ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col() Verwechsle geom_col() nicht mit geom_bar(). Die erste Funktion stellt genau das dar, was man ihr übergibt (z.B. Mittelwerte). Letztere Funktion hingegen erstellt Balken mit einer Höhe, die proportional zur Anzahl der Fälle in der jeder Gruppe ist. Dies findet in der Wissenschaft eher seltener Anwendung, weswegen man wahrscheinlich meist mit geom_col() besser bedient ist. Um das Balkendiagramm zu verschönern, können wir auch hier die Füllfarbe (fill) und die Rahmenfarbe (color) entsprechend anpassen. Zusätzlich bilden wir mit geom_errorbar() die Standardabweichung (SD) ab, indem wir das Minimum des Fehlerbalken als Mittelwert minus Standardabweichung und das Maximum als Mittelwert plus Standardabweichung festlegen. Außerdem kann die Breite der Fehlerbalken mit width verändert werden. Beachte an dieser Stelle, dass die Grenzen der Fehlerbalken (ymin und ymax) im Gegensatz zur Fehlerbalkenbreite innerhalb der Funktion aes() definiert werden müssen. Das Ergebnis ist in Abbildung 8.7 (b) dargestellt. Mehrfaktorielle Balkendiagramme werden erst in Kapitel 8.9 eingeführt. ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col(fill = &quot;white&quot;, color = &quot;black&quot;) + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4) Abbildung 8.7: Balkendiagramme Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.7 Liniendiagramm Bei Mittelwertvergleichen in Form von Liniendiagrammen ändert sich im Vergleich zu den Balkendiagrammen nur wenig. Auch hier verwenden wir wieder den Datensatz big5_means, der unsere Mittelwerte und Standardabweichungen für Extraversion und Neurotizismus enthält. Zum Erstellen der Verbindungslinie zwischen den beiden Persönlichkeitsfaktoren muss das group Argument auf 1 gesetzt werden. Nun müssen wir noch die Linie mit geom_line(), die Mittelwerte als Punkte mit geom_point() und die Fehlerbalken mit geom_errorbar() erstellen. Auch hier ändert sich nichts im Vergleich zu den Balkendiagrammen im vorherigen Kapitel. Das Ergebnis ist in Abbildung 8.8 (a) illustriert. ggplot(big5_means, aes(x = Faktor, y = Mean, group = 1)) + geom_line() + geom_point() + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2) Ein weiteres klassisches Beispiel eines Liniendiagramms ist die Abbildung von Zeitreihen. Dafür schauen wir uns den Kurs der Bitcoin Aktie an, welcher im remp Package enthalten ist. bitcoin # A tibble: 731 × 2 Date Price &lt;date&gt; &lt;dbl&gt; 1 2019-01-01 3844. 2 2019-01-02 3943. 3 2019-01-03 3837. 4 2019-01-04 3858. # … with 727 more rows Auf der x-Achse soll das Datum stehen und auf der y-Achse der Preis bei geschlossener Börse in USD. Die Zeitreihe wird wie zuvor mit geom_line() visualisiert. Wichtig ist hierbei, dass das Datum vom Datentyp date ist (siehe Kapitel 6.11). Zusätzlich können wir, wie beim Streudiagramm in Kapitel 8.3, mit stat_smooth() eine am besten passendste Kurve zur Kursbeschreibung hinzufügen. Abschließend greifen wir an dieser Stelle etwas vor und verändern noch die Benennung der x-Achse mithilfe von scale_x_date(). Dabei gibt es verschiedene Möglichkeiten der Anzeige, die jeweils mit einem Prozentzeichen angeführt werden müssen. Hier zeigen wir den abgekürzten Monatsnamen (%b) und das entsprechende Jahr (%Y). Das Ergebnis ist in Abbildung 8.8 (b) illustriert. ggplot(bitcoin, aes(x = Date, y = Price)) + geom_line() + stat_smooth(color = &quot;black&quot;) + scale_x_date(date_labels = &quot;%b %Y&quot;) Eine weitere Anwendung finden Liniendiagramme bei Scree Plots zur Auswahl der Anzahl der Faktoren für explorative Faktorenanalysen. Dafür benötigen wir den kompletten Big Five Rohdatensatz mit den einzelnen Fragen zu den Persönlichkeitsfaktoren namens big_five_comp. Mit der im remp enthaltenen Funktion data_eigen() können praktisch die entsprechenden Eigenvalues berechnet werden, die wir im Scree Plot abbilden wollen. big5_scree &lt;- big_five_comp %&gt;% select(-Geschlecht) %&gt;% data_eigen() big5_scree # A tibble: 51 × 2 Eigenvalues Dimension &lt;dbl&gt; &lt;int&gt; 1 8.25 1 2 4.59 2 3 3.62 3 4 3.57 4 # … with 47 more rows Es ändert sich im Prinzip nichts im Vergleich zum vorherigen Beispiel. Auf der x-Achse haben wir unsere verschiedenen Dimensionen und auf der y-Achse die Eigenvalues. geom_point() und geom_line() modifizieren wir selbsterklärend zusätzlich optisch leicht. Neu ist an dieser Stelle die Funktion geom_hline() (für horizontal line), welche eine horizontale Linie beim Schnittpunkt mit der y-Achse von 1 einzeichnet (siehe Abbildung 8.8 (b)). ggplot(big5_scree, aes(x = Dimension, y = Eigenvalues)) + geom_point(shape = 19, size = 2) + geom_line(size = 0.6) + geom_hline( aes(yintercept = 1), size = 0.8, linetype = &quot;longdash&quot; ) Abbildung 8.8: Abbildung von Liniendiagrammen als (a) Mittelwertsvergleich (b) Zeitreihe und (c) Scree Plot Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.8 Quantil-Quantil Plot Um mithilfe eines Q-Q Plots die Quantile zweier Verteilungen zu überprüfen, können wir geom_qq() und geom_qq_line() verwenden. Ein häufiger Anwendungsfall ist die graphische Überprüfung einer Variable auf Normalverteilung. Daher ist dies auch die Standardeinstellung innerhalb der Funktionen. Die interessierende Variable (hier Alter) muss dem sample Argument übergeben werden (siehe Abbildung 8.9). model &lt;- lm(Extraversion ~ Geschlecht + Alter, data = big5) resid_df &lt;- augment(model) ggplot(resid_df, aes(sample = .resid)) + geom_qq() + geom_qq_line() Abbildung 8.9: Q-Q Plots Möchte man die Verteilung einer Spalte mit einer anderen Verteilung vergleichen, können mit dem distribution Argument die Quantile einer anderen Verteilung wie qbinom (Binomialverteilung) oder qt (t-Verteilung) ohne Anführungszeichen festgelegt werden. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.9 Mehrfaktorielle Abbildungen In den bisherigen Kapiteln haben wir bislang nur einfaktorielle Abbildungen besprochen. Wir haben also einfach eine Variable auf der x-Achse gegen eine Variable auf der y-Achse aufgetragen. Was aber, wenn man nach mehreren Faktoren gruppieren möchte? Zum Beispiel könnte man die Farbe der Balkendiagramme je nach Geschlecht verändern. Da wir nun mehr als einen Faktor haben, sprechen wir von zweifaktoriell. Die Anzahl der Ausprägungsgrade, also ob zwei oder fünf Geschlechter erhoben hat, spielt dabei keine Rolle. Diese zweifaktoriellen Abbildungen können mit selbsterklärenden Gruppierungselementen wie color, fill und linetype implementiert werden. Wenn man drei- oder vierfaktorielle Abbildungen kreieren möchte, muss man auf Facets zurückgreifen. Im Sinne der Barrierefreiheit bezüglich Farbenblindheit, stellen wir die Standardfarben auf die Viridis Farbenpalette um, die genau für diesen Zweck kreiert wurden. Der Suffix d steht für discrete und c für continouus. Diese vier Befehle stellen global (also für die gesamte R Session) die Standardfarben um. Dabei handelt es sich um dieselbe Farbpalette wie in diesem Buch. Weitere Informationen über die Anpassung der Farbpaletten finden sich in Kapitel 8.10. 8.9.1 Gruppierungsargumente Gruppierungsargumente können grundsätzlich erst einmal fast alles innerhalb von aes() sein. Je nach gewünschter Abbildung kann color, fill, linetype aber auch size und shape beispielsweise verwendet werden. Im Vergleich zu vorher übergeben wir diesen Argumenten nun kein Charakter (wie \"black\"), sondern das gruppierende Argument (z.B. Geschlecht). Zusätzlich benötigen wir dann nur noch ein position Argument, welches spezifiziert, wie die Gruppen zueinander stehen. Eine häufige Wahl hierfür ist die Funktion position_dodge(0.95) (engl. für ausweichen), welches die Gruppen direkt nebeneinander darstellt. Die Zahl in der Klammer steht für den genauen Abstand zwischen den Balken oder Linien (je nach Funktion). Alternativ könnte man auch position = \"stack\" für eine aufeinander gestapelte Ansicht pro Kategorie verwenden. Wenn man stattdessen position = \"fill\" verwendet, werden diese übereinander gestapelten Anteile auf 1 standardisiert, sodass man die Verhältnisse besser vergleichen kann. Beim Erstellen eines gruppierten Balkendiagramms, müssen wir nun nur noch den Datensatz mit einem zusätzlichen Faktor (hier Geschlecht) mit group_by() gruppieren. big5_means2 &lt;- big5_long %&gt;% group_by(Faktor, Geschlecht) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means2 # A tibble: 4 × 4 # Groups: Faktor [2] Faktor Geschlecht Mean SD &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f 3.05 0.358 2 Extraversion m 3.11 0.328 3 Neurotizismus f 3.25 0.633 4 Neurotizismus m 2.96 0.718 Jetzt setzen wir noch fill = Geschlecht für die Füllfarbe und fügen entsprechend wie bereits besprochen die Positionsargumente jeder Funktion hinzu (siehe Abbildung 8.10). ggplot(big5_means2, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95), color = &quot;black&quot;, alpha = 0.8) + geom_errorbar( aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) Abbildung 8.10: Zweifaktorielle Balkendiagramme Ein anderes Beispiel wären gruppierte Liniendiagramme. Grundsätzlich ändern wir an dieser Stelle nur fill zu linetype und müssen zusätzlich das Gruppenargument Geschlecht den einzelnen Funktionen übergeben, da sonst keine Linien zwischen den Gruppen gezeichnet werden würden (siehe Abbildung 8.11). ggplot(big5_means2, aes(x = Faktor, y = Mean, linetype = Geschlecht)) + geom_line( aes(group = Geschlecht), position = position_dodge(0.2) ) + geom_point(position = position_dodge(0.2)) + geom_errorbar( aes(group = Geschlecht, ymin = Mean - SD, ymax = Mean + SD), width = 0.2, position = position_dodge(0.2) ) Abbildung 8.11: Zweifaktorielle Liniendiagramme Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.9.2 Facetten als weitere Dimensionen Bei dreifaktoriellen Abbildungen müssen wir erst einmal einen weiteren Faktor (hier Gruppe) der Funktion group_by() übergeben. big5_means3 &lt;- big5_long %&gt;% group_by(Faktor, Geschlecht, Gruppe) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means3 # A tibble: 12 × 5 # Groups: Faktor, Geschlecht [4] Faktor Geschlecht Gruppe Mean SD &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f Jung 3.07 0.373 2 Extraversion f Mittel 3.07 0.299 3 Extraversion f Weise 2.83 0.269 4 Extraversion m Jung 3.12 0.324 # … with 8 more rows Zum Darstellen unsere drei Altersgruppen können wir nun mit facet_wrap() auswählen, ob die Gruppen zeilenweise (facet_wrap(Gruppe ~)) oder wie in dem Beispiel spaltenweise (facet_wrap(~ Gruppe)) dargestellt werden sollen (siehe Abbildung 8.12). ggplot(big5_means3, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95), color = &quot;black&quot;, alpha = 0.8) + geom_errorbar( aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) + facet_wrap(~ Gruppe) Abbildung 8.12: Dreifaktorielle Abbildung. Anstelle von der Tilde, unterschiedliche y Achse und Qualten p + facet_wrap(vars(Variable), scales = &quot;free_y&quot;, ncol = 2) Das Prinzip bleibt auch bei vierfaktoriellen Abbildungen das gleiche. Wir erfinden dafür noch eine zusätzliche Spalte mit zwei Messzeitpunkten. Die Funktion rep() (repeat, engl. für wiederholen) wiederholt dabei die beiden Messzeitpunkte T1 und T2 jeweils 200 mal, da wir 400 Zeilen in unserem Datensatz haben. Das ist natürlich in der Praxis nicht notwendig, wenn wir bereits eine Spalte haben, nach der wir zusätzlich gruppieren möchten. Ansonsten fügen wir wie zuvor diesen weiteren Faktor zu group_by() hinzu. big5_means4 &lt;- big5_long %&gt;% mutate(Zeitpunkt = rep(c(&quot;T1&quot;, &quot;T2&quot;), each = 200)) %&gt;% group_by(Faktor, Geschlecht, Gruppe, Zeitpunkt) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means4 # A tibble: 24 × 6 # Groups: Faktor, Geschlecht, Gruppe [12] Faktor Geschlecht Gruppe Zeitpunkt Mean SD &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f Jung T1 3.08 0.391 2 Extraversion f Jung T2 3.06 0.362 3 Extraversion f Mittel T1 2.93 0.121 4 Extraversion f Mittel T2 3.13 0.336 # … with 20 more rows Anschließend müssen wir nur noch facet_wrap() zu facet_grid() tauschen, da letztere Funktion sowohl zeilenweise (links der Tilde) als auch spaltenweise (rechts der Tilde) zeitgleich gruppieren kann (siehe Abbildung 8.13). ggplot(big5_means4, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95), color = &quot;black&quot;, alpha = 0.8) + geom_errorbar( aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) + facet_grid(Zeitpunkt ~ Gruppe) p + facet_grid(rows = vars(Variable), cols = vars(Gruppe)) Abbildung 8.13: Vierfaktorielle Abbildungen Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 8.10 Anpassen des Aussehens Wir werden uns anhand der Abbildung aus Kapitel 8.9.1 die verschiedenen Anpassungsmöglichkeiten anschauen. big5_means2 &lt;- big5_long %&gt;% group_by(Faktor, Geschlecht) %&gt;% summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) Unsere Abbildung p &lt;- ggplot(big5_means2, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col( position = position_dodge(0.95), color = &quot;black&quot;, alpha = 0.8 ) + geom_errorbar( mapping = aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2, position = position_dodge(0.95) ) Anpassen p + scale_fill_viridis_d( begin = 0.27, end = 0.72, option = &quot;C&quot;, name = &quot;Geschlecht (Auswahl)&quot;, labels = c(&quot;Weiblich&quot;, &quot;Männlich&quot;) ) + scale_x_discrete( limits = c(&quot;Neurotizismus&quot;, &quot;Extraversion&quot;) ) + scale_y_continuous( expand = c(0, 0), limits = c(0, 5.2), breaks = 0:5 ) + labs( x = &quot;Persönlichkeitsfaktor&quot;, y = &quot;Mittlere Ausprägung&quot; ) + theme_classic( base_size = 14, base_family = &quot;sans&quot; ) + theme( legend.position = c(0.4, 0.9), axis.title.y = element_text(vjust = 2), axis.title.x = element_text(vjust = 0.5), axis.text = element_text(size = 13) ) Wie wir dazu kommen in den Kapiteln. 8.10.1 Farben Bleiben wir zunächst beim Datensatz. Wenn wir die Bezeichnungen innerhalb unserer Legende (also der Spalte Geschlecht) umbenennen wollen, nutzen wir fct_recode(). Zum Verändern der Reihenfolge kann fct_relevel() verwendet werden. Die Funktionen zur Veränderungen von Faktoren wurden bereits in Kapitel 6.10 eingeführt. Nun erstellen wir zunächst die Abbildung und speichern diese als plot. Grundsätzlich könnten wir alle Anpassungen auch einfach mithilfe einer sehr langen Aneinanderkettung von Befehlen erledigen. Der übersichtshalber speichern wir an dieser Stelle Zwischenergebnisse immer wieder ab. Das Farbschema kann durch verschiedene Farbpaletten adaptiert werden. Häufig sind im wissenschaftlichen Kontext jedoch nur Graustufen erwünscht, welche wir mit scale_fill_grey() erstellen können. Wichtig ist an dieser Stelle das start und end Argument der Farbpalette, da die Grautöne sonst zu dunkel für das Erkennen von Fehlerbalken sind. Hätten wir in unserem ggplot nicht die Füllfarbe (fill), sondern die Rahmenfarbe (color) verändert, würde man entsprechend scale_color_grey() verwenden. a &lt;- p + scale_fill_grey(start = 0.3, end = 0.7) b &lt;- p + scale_fill_manual(values = c(&quot;#A50F15&quot;, &quot;#FC9272&quot;)) c &lt;- p + scale_fill_brewer(palette = &quot;Blues&quot;) library(scales) library(RColorBrewer) brewer.pal(9, &quot;Reds&quot;) %&gt;% show_col() 8.10.2 Thema Alternativ könnten wir wie in Kapitel 8.9 bereits erwähnt, eine Farbpalette manuell einstellen, die auch bei Farbenblindheit eine Differenzierung der Farben erlaubt. Dafür würde man beispielsweise scale_fill_viridis_d(start = 0.27, end = 0.72, opt = \"C\") benutzen. Das ist die Farbpalette, die in diesem Buch verwendet wird. Die Option bezieht sich damit auf eine der fünf in Viridis enthaltenen Farbpaletten. Für genaue Informationen über die einzelnen Farbpaletten sei auf die offizielle Dokumentation verwiesen. Auch der graue Standardhintergrund und die fehlende Visualisierung der Achsen ist in der Regel in der Wissenschaft nicht erwünscht. Ein gutes minimales Thema ist theme_classic(), welchem wir die grundlegende Größe aller Textelemente übergeben können. a &lt;- p + theme_classic(base_size = 14, base_family = &quot;sans&quot;) b &lt;- p + theme_minimal() c &lt;- p + theme_bw() ggthemes Package 8.10.3 Achsen p + scale_x_discrete( limits = c(&quot;Neurotizismus&quot;, &quot;Extraversion&quot;), labels = c(&quot;Extraversion&quot; = &quot;Extra&quot;, &quot;Neurotizismus&quot; = &quot;Neuro&quot;) ) + scale_y_continuous( expand = c(0, 0), limits = c(0, 5.2), breaks = 0:5 ) + labs( x = &quot;Persönlichkeitsfaktor&quot;, y = &quot;Mittlere Ausprägung&quot; ) + theme( axis.title.y = element_text(vjust = 2), axis.title.x = element_text(vjust = 0.5), axis.text = element_text(size = 13) ) p + coord_cartesian(xlim = c(0, 8)) p + scale_x_continuous( labels = c(&quot;25&quot;, &quot;50&quot;, &quot;75&quot;, &quot;100&quot;, &quot;125&quot;, &quot;150&quot;), ) p + scale_x_discrete(guide = guide_axis( n.dodge = 2, check.overlap = TRUE) ) p + scale_x_discrete( labels = ~ str_wrap(.x, width = 10) ) theme( axis.title.x = element_text(hjust = 1, vjust = 0.2), axis.text.y = element_text(hjust = 0, color = &quot;black&quot;), axis.text.x = element_text( angle = 45, hjust = 1, size = 12 ) ) Genauere Anpassungen des Themas sind mit der theme Funktion möglich. Hier können wir jedes kleinste Detail manuell anpassen. Hervorzuheben ist an dieser Stelle die zwingende Notwendigkeit der element_text() Funktion, der die Argumente wie size übergeben werden müssen. Nützlich ist bei langen Achsenbeschriftungen auch angle (engl. für Winkel) in Kombination mit hjust (Akronym für horizontal adjustment). Strip steht für die Überschriften bei den Facets für mehrfaktorielle Abbildungen (siehe Kapitel 8.9.2). Mithilfe von element_blanck() könnte man den Hintergrund der Überschriftsboxen entfernen. Mit legend.position kann man die Koordinaten der Legende festlegen, mit legend.direction die Ausrichtung. Wenn man letzteres Argument weglässt, wird die Legende vertikal ausgegeben. Nun kommen wir im nächsten Schritt zu den Achsen. Mit dem expand Argument entfernt man den extra Raum zwischen y-Achse und x-Achse, wodurch die Balken nicht mehr schweben. Mithilfe von labs() (Akronym für labels) kann die Achsen- sowie die Legendbeschriftung angepasst werden. ggtitle() könnte der Abbildung einen Titel hinzufügen. Da dies häufig jedoch nicht erwünscht ist, nutzen wir das an dieser Stelle nur, um einen etwas größeren Platz über der y-Achse zu verschaffen. Alternativ zur Festlegung der Grenzen mit limits innerhalb von scale_y_continouus() könnte man ebenfalls die Funktion lims(), xlims() oder ylims() verwenden. Bei vielen Breaks oder welchen mit ganz bestimmten Abständen könnte man ebenfalls seq() verwenden. seq(from = 0, to = 4000, by = 500) [1] 0 500 1000 1500 2000 2500 3000 3500 4000 Abschließen schauen wir uns noch an, wie man mit sehr langen Achsenbeschriftungen umgehen kann. Mithilfe von guide_axis() können wir beispielswiese festlegen, dass überlappende (check.overlap = TRUE) Achsenbeschriftungen in 2 Zeilen (n.dodge = 2) angezeigt werden. p + scale_x_discrete(guide = guide_axis( n.dodge = 2, check.overlap = TRUE) ) Eine andere Möglichkeit besteht darin, lange Achsenbeschriftungen untereinander zu schreiben, wenn sie eine bestimmte Breite (z.B. width = 10) erreichen. Dafür würde man die Funktion str_wrap() aus dem stringr Package verwenden (siehe Kapitel 6.9). p + scale_x_discrete( labels = ~ str_wrap(.x, width = 10) ) 8.10.4 Legende und Facetten a &lt;- p b &lt;- p + lims(fill = c(&quot;m&quot;, &quot;f&quot;)) big5_means_rev &lt;- big5_means2 %&gt;% mutate(Geschlecht = fct_relevel(Geschlecht, &quot;m&quot;)) c &lt;- ggplot(big5_means_rev, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col( position = position_dodge(0.95), color = &quot;black&quot;, alpha = 0.8 ) + geom_errorbar( mapping = aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2, position = position_dodge(0.95) ) p + theme( legend.position = c(0, 1), legend.title = &quot;none&quot; ) plot &lt;- plot + theme( legend.title = element_text(size = 14), legend.text = element_text(size = 13), legend.position = c(0.25, 0.9) ) # legend.direction auch &quot;horizontal&quot; # legend.position auch &quot;top&quot; und &quot;bottom&quot; # for legend fill name see scale p + theme( strip.background = element_blank(), strip.text = element_text(size = 15) ) 8.11 Anordnen mehrerer Graphen Häufig möchte man mehrere Graphen innerhalb einer Abbildung darstellen, die optimalerweise auch noch beschriftet sind. Dafür verwenden wir das Package patchwork. library(patchwork) Zuerst müssen wir die Abbildungen in Variablen abspeichern. Exemplarisch nutzen wir an dieser Stelle das Histogramm, das Streudiagramm, das Balkendiagramm und die Q-Q Plots aus den vorherigen Kapiteln und speichern diese jeweils als a, b, c und d. a &lt;- ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = 0.2 ) b &lt;- ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) c &lt;- ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col(fill = &quot;white&quot;, color = &quot;black&quot;) + geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4) d &lt;- ggplot(big5_mod, aes(sample = Alter)) + geom_qq() + geom_qq_line() Nun müssen wir die verschiedenen Graphen lediglich in der gewünschten Reihenfolge addieren. Die Funktion plot_layout() spezifiziert unter anderem die Anzahl der Spalten (ncol) und plot_annotation() ergänzt mit dem tag_levels Argument Beschriftungen zu jeder Abbildung (siehe Abbildung 8.14). a + b + c + d + plot_layout(ncol = 2) + plot_annotation(tag_levels = &quot;A&quot;) Abbildung 8.14: Anordnung mehrerer Graphen Neben \"A\" kann der Funktion plot_annotation() außerdem \"i\", \"I\", \"a\" und \"1\" übergeben werden. Die tag_levels können durch tag_prefix und tag_suffix weiter an die eigenen Bedürfnisse angepasst werden. Außerdem können Abbildungen auch in unterschiedlicher Anzahl neben- und untereinander gesetzt werden. Dafür muss man lediglich die oben stehenden Abbildungen mit vertikalen Linien unterteilen und diese dann durch die Abbildung, die unten stehen soll, teilen (siehe Abbildung 8.15). (a | b | c) / d + plot_annotation( tag_levels = &quot;I&quot;, tag_prefix = &#39;Abb. &#39;, tag_suffix = &#39;:&#39; ) Abbildung 8.15: Alternative Anordnung mehrerer Graphen Für ein komplexeres Beispiel mit verschiedenen Verhältnissen zwischen den Abbildungen sei auf das Kapitel 8.14 verwiesen. g1 &lt;- p + theme_classic() g2 &lt;- p + theme_classic() g3 &lt;- p + theme_classic() g1 + g2 + g3 + guide_area() + plot_layout(ncol = 2, guides = &quot;collect&quot;) + plot_annotation( tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot; ) &amp; labs(x = &quot;&quot;) plots[[1]] + theme_minimal() a + b + c &amp; theme_minimal() 8.12 Speichern von Graphen Abbildungen werden einfach mit ggsave() in dem aktuellen durch das Projekt festgelegten Ordner gespeichert. Dabei sind vor allem drei Argumente wichtig. Mit filename kannst du festlegen, wie die Abbildung heißen soll und in welchem Dateiformat (z.B. jpg oder png) selbige ausgegeben wird. Wenn man mit plot nichts genaueres festlegt, wird einfach die als letztes kreierte Abbildung gespeichert. Mit width und height legt man die Weite in Inches fest. ggsave( filename = &quot;plotA.png&quot;, width = 5, height = 5 ) Wenn man irgendeine Abbildung und nicht unbedingt die Letzte speichern möchte, muss man diesen dem plot Argument übergeben. Die festgelegte Breite und Höhe hat einen erheblich Einfluss auf die Auflösung. ggsave( filename = &quot;plotB.png&quot;, plot = plot1, width = 8, height = 7 ) In Abbildung 8.16 (a) ist plotA.png mit einer Breite und Höhe von 5 Inches abgebildet. In Abbildung 8.16 (b) wurde mit einer Breite von 8 und eine Höhe von 7 die Visualisierung als plotB.png gespeichert. Während in (a) die Achsenbeschriftungen groß sind und die Legende sogar mit der y-Achse überlappt, ist dies in (b) nicht zu beobachten. Abbildung 8.16: Auflösungsunterschiede je nach Größe der gespeicherten Abbildung. Die Größe der Abbildung wird durch die Breite (width) und Höhe (height) festgelegt. durch unterschiedliche Größen wird auch die Auflösung innerhalb der Abbildung beeinflusst, sodass man je nachdem zum Beispiel noch die Textgrößen der Achsenbeschriftungen anpassen muss. Die gespeicherte Abbildung hat nicht das selbe Format wie die Ausgabe innerhalb von RStudio und kann dementsprechend davon abweichen. 8.13 Exemplarische Erweiterungen Wir erinnern uns, dass die beiden g’s in ggplot für Grammar of Graphics stehen. Daraus resultiert nicht nur eine konsistente Anwendung, wie wir es in den bisherigen Kapiteln kennengelernt haben. Zusätzlich gibt es diverse Erweiterungen, die mal mehr und mal weniger konsistent anzuwenden sind. Alle haben jedoch die gleiche Basis, einen ggplot, gemein. Dadurch können wir unsere Visualisierung wie zuvor anpassen und erweitern. Die meisten Erweiterungen halten sich an eine einheitliche Namensgebung. Vor dem Zweck des Packages steht also auch bei den Erweiterungspackages immer ein gg (z.B. ggridges oder gghighlight). Leider hält sich nicht an eine konsistente Namensgebung und Anwendung. Ein Beispiel hierfür sehen wir in Kapitel 8.13.1. Im Folgenden werden wir uns vier Beispiele für Erweiterungen anschauen. 8.13.1 Kaplan-Meier-Kurve Für dieses Kapitel müssen sowohl das survival als auch das survminer Package installiert und geladen werden. Wenn man die kumulativen Inzidenzen visualisieren möchte, muss außerdem das cmprsk Package zur Verwendung bereit sein. library(survival) library(survminer) library(cmprsk) library(patchwork) Aus dem survival Package schauen wir uns den Datensatz lung an. Dabei interessieren uns die Spalten time, status und sex. Die Variable time gibt die Überlebenszeit in Tagen an, status beinhaltet die Information über den Tod (1) oder für ein zensiertes Ereignis (2). Die Variable sex beinhaltet das biologische Geschlecht in Form männlich (1) und weiblich (2). lung # A tibble: 228 × 10 inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 306 2 74 1 1 90 100 1175 NA 2 3 455 2 68 1 0 90 90 1225 15 3 3 1010 1 56 1 0 90 90 NA 15 4 5 210 2 57 1 1 90 60 1150 11 # … with 224 more rows Gesamte Überlebenszeit. Bevor wir eine Kaplan-Meier-Kurve zur Visualisierung der Überlebenszeiten erstellen können, müssen wir zuerst eine mithilfe von survfit() aus dem survival Package die Überlebenswahrscheinlichkeiten schätzen. Für genauere Informationen über die Verwendung dieser Funktion sei auf das Kapitel 9.6.6 verwiesen. res &lt;- survfit(Surv(time = time, event = status) ~ 1, data = lung) Mit der Funktion ggsurvplot() aus dem survminer Package können wir anschließend die Überlebenszeiten visualisieren. Zusätzlich können wir mit conf.int ein Konfidenzinterval und mit risk.table die Anzahl betroffener Personen hinzufügen. ggsurvplot(res, conf.int = TRUE, risk.table = TRUE) Abbildung 8.17: Abbildung von Überlebensraten mit Risikotabelle. Überlebenszeit nach kategorialer Variable. Es gibt noch diverse weitere Argumente zum Anpassen dieser Abbildung. Leider hat sich der Packageentwickler an dieser Stelle dazu entscheiden, stark von der bisherigen Syntax abzuweichen. Wir haben an dieser Stelle nicht mehr die Kombination aus ggplot() und geom_*(), sondern nur noch eine Funktion, die im Hintergrund alles weitere übernimmt. Außerdem ist das Ergebnis dieser Funktion zwar optisch noch basierend auf einem ggplot, allerdings wurde die Klasse so stark verändert, sodass wir nicht wie gewohnt die Skalen und das Thema anpassen können. Deshalb haben wir in dieser Abbildung anstelle der veränderten, für Farbenblindheit angepasste Farben die Standard ggplot Farben vorliegen (hellrot und türkis). res2 &lt;- survfit(Surv(time = time, event = status) ~ sex, data = lung) p1 &lt;- ggsurvplot( fit = res2, xlab = &quot;Months&quot;, ylab = &quot;Überlebenswahrscheinlichkeit&quot;, risk.table = TRUE, break.x.by = 100, legend = c(0.15, 0.15), legend.title = &quot;Geschlecht&quot;, legend.labs = c(&quot;Männlich&quot;, &quot;Weiblich&quot;), risk.table.y.text = FALSE, palette = &quot;nejm&quot; ) p1 Die Abbildung besteht aus zwei Teilen, der Kaplan-Meier-Kurve (plot) und der Tabelle (table). Zum Abspeichern müssen wir aus unserem Ergebnis p1 diese beiden Anteile einzeln mit dem Dollar-Operator ansprechen und mithilfe des patchwork Packages zusammenfügen (siehe Kapitel 8.11). Mit plot_layout() wird festgelegt, dass die Abbildungen untereinander seien sollen (ncol) sowie das Verhältnis der Abbildungen (heights). Letzteres Argument ist wichtig, da der Hauptteil der Abbildung der Kaplan-Meier-Kurve und nicht der Tabelle gewidmet werden soll. kaplan_meier &lt;- p1$plot + p1$table + plot_layout(ncol = 1, heights = c(6, 1)) Anschließen kann das Ergebnis, wie in Kapitel 8.12 gelernt, abgespeichert werden. ggsave(&quot;Kaplan_meier.png&quot;, kaplan_meier, height = 7, width = 9) 8.13.2 Residuen überprüfen Für das schnelle graphische Überprüfen der notwendingen Voraussetzungen in Bezug auf die Residuen, müssen wir erst das ggfortify Package installieren und laden. library(ggfortify) library(patchwork) Wenn wir ein lineares Regressionsmodell erstellen (siehe Kapitel (linregression)), in dem die Variation der mittleren Extraversionsausprägung durch Geschlecht und Alter erklärt werden soll, model &lt;- lm(Extraversion ~ Geschlecht + Alter, data = big5) kann dieses Modell der Funktion autoplot() übergeben werden. Dies funktioniert für die üblichsten Regressionsmodelle. Zwingend notwendig dafür ist das laden von ggfortfify zuvor. Als weiteres Argument kann mit which ausgewählt werden, welche Abbildungen ausgegeben werden (hier alle 6). Das Argument label.repel sorgt dafür, dass der Text, der die Ausreißer beschriftet, die Linien nicht überschneidet. Als letztes wird mit smooth.colour noch die Farbe der Regressionsgeraden auf schwarz gesetzt. autoplot( model, which = 1:6, label.repel = TRUE, smooth.colour = &quot;black&quot; ) Wie in Kapitel (kaplan) können wir hier die einzelnen Abbildungen auswählen und müssen diese zum Speichern mithilfe des patchwork Packages zusammenfügen. Nehmen wir hierfür nur die ersten 4 Abbildungen exemplarisch heraus. res &lt;- autoplot( model, which = 1:4, label.repel = TRUE, smooth.colour = &quot;black&quot; ) Da die Plots mit ggplot erstellt wurden, können wir in üblicher Manier das Aussehen anpassen. Da die einzelnen Abbildungen, anders als bei den Kaplan-Meier-Kurven zuvor, nicht benannt sind, müssen wir auf die einzelnen Plots mit doppelten eckigen Klammern zugreifen. Weshalb diese doppelt sein müssen, wird in Kapitel (lists) genauer erläutert. Anschließend fügen wir die Abbildungen mit patchwork zusammen. res[[1]] + res[[2]] + res[[3]] + res[[4]] + plot_layout(ncol = 2) &amp; theme_classic(base_size = 14) 8.13.3 Ridgeline Plot Für dieses Kapitel muss das ggridges Package installiert und geladen sein. library(ggridges) Sogenannte Ridgeline Plots erlauben unter anderem den Vergleich von mehreren Dichtefunktionen innerhalb einer Abbildung. In Kapitel 8.2 haben wir die Wahrscheinlichkeitsdichte und somit die Verteilung der Werte für eine Variable angezeigt. In Kapitel 8.14 haben wir gesehen, wie man diese nach einer gruppierenden Variable aufteilen kann. Um mehrere Dichten untereinander innerhalb einer Abbildung zu vergleichen, müssen wir den Datensatz zuerst in das lange Datenformat bringen (siehe Kapitel 6.6. big_five_long &lt;- big_five %&gt;% pivot_longer( cols = Extraversion:Gewissenhaftigkeit, values_to = &quot;Auspraegung&quot;, names_to = &quot;Faktor&quot; ) %&gt;% relocate(Faktor, Auspraegung) big_five_long # A tibble: 800 × 14 Faktor Auspraegung Alter Geschlecht O1 O2 O3 O4 O5 O6 O7 O8 O9 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extravers… 3 36 m 5 1 5 1 4 1 5 5 4 2 Neurotizi… 1.9 36 m 5 1 5 1 4 1 5 5 4 3 Vertraegl… 3.4 36 m 5 1 5 1 4 1 5 5 4 4 Gewissenh… 3.3 36 m 5 1 5 1 4 1 5 5 4 # … with 796 more rows, and 1 more variable: O10 &lt;dbl&gt; Nun können wir anhand der neuen Funktion geom_density_ridges() die Verteilungen von Extraversion, Neurotizismus, Verträglichkeit und Gewissenhaftigkeit auf einen Schlag abbilden. Zusätzlich übergeben wir das Argument alpha, welches die Dichten leicht durchsichtig macht. ggplot(big_five_long, aes(x = Auspraegung, y = Faktor, height = stat(density))) + geom_density_ridges(stat= &quot;density&quot;, alpha = 0.8) Abbildung 8.18: Mehrere Dichteverteilungen untereinander mit Ridgeline Plots. Etwas ungewöhnlich ist an dieser Stelle, dass wir auf der x-Achse die diskrete Variable haben, die wir vergleichen und auf der y-Achse die mittlere Ausprägung des jeweiligen Persönlichkeitsfaktors. ggplot(big_five_long, aes(x = Auspraegung, y = Faktor, height = stat(density))) + geom_density_ridges(stat= &quot;density&quot;, alpha = 0.8) + scale_x_continuous(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme_classic(base_size = 14) + theme( axis.text.y = element_text(hjust = 0, color = &quot;black&quot;, size = 12, family = &quot;sans&quot;), axis.title.x = element_text(hjust = 1, size = 13, vjust = 0.2, family = &quot;sans&quot;) ) + labs( x = &quot;Mittlere Ausprägung&quot;, y = &quot;&quot; ) Abbildung 8.19: Ridgeline plots mit angepasstem Thema. 8.14 Anwendungsbeispiel Wir werden uns abschließend in diesem Kapitel anhand eines Eye-Tracking Datensatzes namens eye_tracking anschauen, wie wir mithilfe der bisher gelernten Funktionen, eine fortgeschrittene und optische ansprechende Abbildung kreieren können. Diese ist in Abbildung 8.20 illustriert. Abbildung 8.20: Anzahl fixierter Gesichter in Abhängigkeit beobachteter Einwohner pro Quadratkilometer. Diese Abbildung besteht im Prinzip aus vier Teilen, welche nacheinander erstellt und dann miteinander kombiniert werden müssen. Wir haben oben links und rechts unten jeweils eine gruppierte Wahrscheinlichkeitsdichte. Links unten ist unsere Hauptabbildung, mit Komponenten, die wir alle bereits kennengelernt haben. Neu hinzu gekommen ist lediglich geom_rug(), welches ebenfalls eine Möglichkeit zu Verteilungsdarstellung darstellt. Die Hauptabbildung wird als plot1 gespeichert. plot1 &lt;- ggplot(eye_tracking, aes(x = Density, y = Face_sum, color = Group)) + geom_point(aes(color = Group), size = 3 , alpha = 0.8) + geom_point(shape = 1, color = &quot;black&quot;, size = 3) + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE) + geom_rug() + labs( x = &quot;Einwohner pro Quadratkilometer&quot;, y = &quot;Anzahl fixierter Gesichter&quot;, color = &quot;Gruppe&quot; ) + scale_y_continuous(limits = c(0, 205), expand = c(0, 0)) + scale_x_continuous(limits = c(1, 4), expand = c(0, 0)) + theme_classic() + theme(legend.position = c(0.15, 0.9)) Bei den beiden Abbildungen zur Wahrscheinlichkeitsdichte müssen wir zusätzlich das Thema (auch die Achsen) mithilfe von theme_void() vollständig ausblenden. Außerdem entfernen wir die Legende. Gespeichert werden die beiden Ergebnisse als dens1 und dens2. dens1 &lt;- ggplot(eye_tracking, aes(x = Density, fill = Group)) + geom_density(alpha = 0.4) + theme_void() + theme(legend.position = &quot;none&quot;) dens2 &lt;- ggplot(eye_tracking, aes(y = Face_sum, fill = Group)) + geom_density(alpha = 0.4) + theme_void() + theme(legend.position = &quot;none&quot;) Jetzt müssen wir die Abbildungen nur noch zusammenfügen. Neu ist an dieser Stelle zum einen die Funktion plot_spacer(), die oben rechts den leeren Raum ausfüllt. Auf der anderen Seite verwenden wir innerhalb von plot_layout() nun die widths und height Argumente, die uns die Spezifizierung der Verhältnisse ermöglicht. Schließlich möchten wir die Wahrscheinlichkeitsdichte oben links und rechts unten kleiner dargestellt haben als unsere Hauptabbildung links unten. dens1 + plot_spacer() + plot1 + dens2 + plot_layout( ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4) ) "],["inductive.html", "Kapitel 9 Inferenzstatistik 9.1 Einführung 9.2 Voraussetzungen überprüfen 9.3 Ein- und Zweistichprobenszenarien 9.4 Unterschiede mehrerer Gruppen 9.5 Korrelationskoeffizienten 9.6 Regressionsmodelle 9.7 Kontingenztafeln 9.8 Ergebnisse formatieren", " Kapitel 9 Inferenzstatistik 9.1 Einführung Für die verschiedenen inferenzstatistischen Verfahren verwenden wir die Funktionen aus dem inductive Package. Die darin enthaltenen Funktionen beginnen alle mit dem Präfix sta_* (für statistical) und folgen den selben Regeln (sta_wilcox() für den Wilcoxon Test). Dabei unterscheiden wir zwischen eindimensionalen und hierarchischen Modellierungen. Eindimensionale Methoden sind jene, die Zwischensubjektfaktoren miteinander vergleichen. Also beispielsweise Personen aus zwei Gruppen in Bezug auf irgendein Merkmal wie Geschlecht oder Alter zu vergleichen. Hierarchische Modelle erlauben hingegen nicht nur den Zwischensubjektvergleich, sondern auch jenen innerhalb der Person beziehungsweise Beobachtung. Ein klassisches Beispiel hierfür sind Messwiederholungen der selben Personen zu unterschiedlichen Zeitpunkten. Schauen wir uns zuerst eindimensionale Modelle an. In Abbildung 9.1 ist ein exemplarischer Anfang eines Datensatzes mit den Spalten Personenname, Extraversion, Neurotizismus und Geschlecht. Abbildung 9.1: Breiter Datensatz mit Geschlecht als zusätzlicher Zwischensubjektfaktor. Dem jeweiligen statistischen Test übergibt man immer die Variablen als Formel, dem sogenannten formula Argument. In Gleichung (9.1) ist der einfachste Fall, das Einstichprobenszenario illustriert. Würden wir beispielsweise überprüfen wollen, ob ein Merkmal normalverteilt ist, könnte man wie hier gezeigt einfach den Spaltennamen als Argument übergeben. Einstichproben-Szenario: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\mathrm{Extraversion} \\tag{9.1} \\end{equation}\\] Wenn wir zwei Merkmale wie Extraversion und Neurotizismus miteinander hinsichtlich Unterschiede bezüglich der mittleren Ausprägung untersuchen möchten, muss man diese Variablen lediglich mit einer sogenannten Tilde (~) trennen. Diese haben wir zwar bereits im Kontext von Lambda Funktionen kennengelernt, allerdings dienen sie hier einem anderen Zweck. In Gleichung (9.2) sehen wir das gerade beschriebene Beispiel. Die Reihenfolge ist beim Vergleich von zwei Variablen nicht weiter von Bedeutung. Vergleich zweier Vektoren: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\mathrm{Extraversion} \\sim \\mathrm{Neurotizismus} \\tag{9.2} \\end{equation}\\] Wenn die zweite eine gruppierende Variable wie Geschlecht ist, können wir auch einfach diese der Funktion übergeben. Dabei muss die gruppierende Variable auf der rechten Seite der Tilde stehen (siehe Gleichung (9.3)) . Vergleich mehrerer Gruppen: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\mathrm{Extraversion} \\sim \\mathrm{Geschlecht} \\tag{9.3} \\end{equation}\\] Wenn wir den Einfluss im Rahmen eines Regressionsmodells von unabhängigen Variablen auf unsere abhängige Variable herausfinden wollen, schreiben wir die abhängige immer auf die linke Seite und die unabhängigen immer auf die rechte Seite der Tilde. Wenn wir den Einfluss der unabhängigen Variablen getrennt voneinander betrachten möchten, trennen wir diese mit einem Plus. Würden wir eine Interaktion (Moderation) zwischen beispielsweise Neurotizismus und Geschlecht erwarten, würde man stattdessen einen Asterisk (*, das Multiplikationszeichen) verwenden. In Gleichung (9.4) ist dieser Anwendungsfall illustriert. Untersuchung einer abhängigen Variable: \\[\\begin{equation} \\mathrm{formula} \\quad = \\quad \\overbrace{\\mathrm{Extraversion}}^{\\text{abhaengige Variable}} \\sim \\overbrace{\\mathrm{Neurotizismus} + \\mathrm{Geschlecht}}^{\\text{unabhaengige Variablen}} \\tag{9.4} \\end{equation}\\] Dies ist in selbiger Form auf die Untersuchung von Unterschiedshypothesen übertragbar (z.B. bei Varianzanalysen). Für hierarchische Modellierungen muss der Datensatz erst in das lange Format umgewandelt werden. Falls dir das nichts sagst, solltest du vorm Rechnen dieser Modelle noch einmal Kapitel 6.6 studieren. In Abbildung 9.2 ist dies für das einfach Beispiel der Messwiederholung bezüglich der Persönlichkeitsfaktoren Extraversion und Neurotizismus illustriert. Abbildung 9.2: Langer Datensatz mit Persönlichkeitsfaktor als Innersubjektfaktor und Alter sowie Geschlecht als Zwischensubjektfaktoren. Bei der hierarchischen Modellierung ändert sich nichts für die Analyse von Zwischensubjektfaktoren wie Geschlecht. Möchten wir nun zusätzlich die hierarchische Struktur der Daten berücksichtigen, müssen wir einen eingeklammerten Term hinzufügen. Das einfachste Beispiel hierfür ist die Messwiederholung aus der obigen Abbildung. Dafür schreiben wir die in die Klammer den Persönlichkeitsfaktor gegeben (|) einer Identifikation der jeweiligen Person. Dieser Anwendungsfall ist in Gleichung (9.5) abgebildet. Hierarchische Modelle: \\[\\begin{equation} \\overbrace{\\mathrm{Extraversion}}^{\\text{abhaengige Variable}} \\sim \\overbrace{\\mathrm{Geschlecht}}^{\\text{Fixed Effect(s)}} + \\underbrace{(\\text{Faktor} \\mid \\text{Person})}_{\\text{Random Effect(s)}} \\tag{9.5} \\end{equation}\\] Alle statistischen Tests innerhalb des inductive Packages funktionieren nach dem selben Prinzip. Sie beginnen alle mit dem Präfix sta_*() und nur die data und formula Argumente. Wenn du die in diesem Kapitel eingeführte Formelsyntax verstanden hast, kannst du alle in diesem Buch vorgestellten statistischen Tests anwenden. 9.2 Voraussetzungen überprüfen 9.2.1 Normalverteilung der Residuen 9.2.1.1 Shapiro-Wilk Test Beim Shapiro-Wilk Test zur Überprüfung der Normalverteilung müssen wir der Funktion shapiro.test() lediglich die entsprechende Spalte des Datensatzes als Zahlenreihe (respektive Vektor) übergeben. shapiro.test(big5$Extraversion) Shapiro-Wilk normality test data: big5$Extraversion W = 0.95715, p-value = 9.845e-06 Da der p-Wert hier bei einem \\(\\alpha\\) Niveau von 5% signifikant ist, würden wir an dieser Stelle von einer Verletzung der Normalverteilung sprechen. Die Annahme der Normalverteilung muss demnach abgelehnt werden. 9.2.1.2 Kolmogorov-Smirnov Test Der Kolmogorov-Smirnov Test wird fast gleich wie der Shapiro-Wilk Test angewandt. Mit dem y Argument muss man allerdings zusätzlich die Verteilung, mit der wir die Spalte Extraversion vergleichen (hier Normalverteilung), explizit festlegen. ks.test(big5$Extraversion, y = rnorm) Asymptotic one-sample Kolmogorov-Smirnov test data: big5$Extraversion D = 3.1301, p-value &lt; 2.2e-16 alternative hypothesis: two-sided Der statistische Test kommt auf die gleiche Schlussfolgerung wie der des vorherigen Kapitels. Die beobachteten Extraversionwerte sind nicht normalverteilt. Der Kolmogorov Smirnov Test funktioniert nur gut, wenn es keine doppelten Werte gibt. Dies ist bei intervallskalierten im Regelfall gegeben. Da wir hier lediglich aus ordinalen Daten einen Mittelwert gebildet haben, sind die Extraversionswerte natürlich trotzdem nicht intervallskaliert. 9.2.2 Varianzhomogenität 9.2.2.1 F Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Mit dem F Test können wir die Hypothese der Varianzgleichheit zwischen zwei Gruppen (hier Männer und Frauen) testen. var.test(Extraversion ~ Geschlecht, data = big5) F test to compare two variances data: Extraversion by Geschlecht F = 1.1868, num df = 117, denom df = 81, p-value = 0.413 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.7871945 1.7628258 sample estimates: ratio of variances 1.186837 Da der p-Wert mit 0.413 größer als 0.05 (häufig gewähltes \\(\\alpha\\) Niveau) ist, würden wir hier von Varianzhomogenität (Varianzgleichheit) ausgehen. Wir erhalten außerdem den F Wert (statistic), die Freiheitsgrade und das Konfidenzintervall, welches bei Gültigkeit der Nullhypothese die 1 enthalten soll. 9.2.2.2 Levene Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für den Levene Test wird derzeit noch das car Package benötigt. library(car) Nun können wir der Funktion leveneTest() in gewohnter Formelsyntax die Spalten Extraversion und Geschlecht des Datensatzes big5 übergeben. leveneTest(Extraversion ~ Geschlecht, data = big5) Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 1 1e-04 0.9923 198 Wir erhalten auch hier die Freiheitsgrade, Teststatistik und den p-Wert. Dabei kommen wir ausgehend vom p-Wert auf den gleichen Schluss wie beim F Test. Der Vorteil des Levene und des Bartletts Test ist der, dass wir auf einen Schlag die Varianz mehrere Gruppen miteinander vergleichen könnten. 9.2.2.3 Bartletts Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Der Bartlett Test funktioniert genau wie die beiden bisher eingeführten Tests zur Überprüfung der Varianzhomogenität. bartlett.test(Extraversion ~ Geschlecht, data = big5) Bartlett test of homogeneity of variances data: Extraversion by Geschlecht Bartlett&#39;s K-squared = 0.69049, df = 1, p-value = 0.406 Auch hier kommen wir auf die selbe Schlussfolgerung der vorhandenen Varianzhomogenität zwischen Männern und Frauen hinsichtlich der mittleren Extraversionsausprägung. 9.3 Ein- und Zweistichprobenszenarien 9.3.1 t Test und Welch Test Skalenniveau: intervallskaliert Voraussetzungen: Normalverteilung Varianzhomogenität für t-Test (nicht für Welch Test) Für einen Einstichproben t Test, indem wir die Hypothese testen, ob der Mittelwert gleich 0 ist, müssen wir erneut die Spalte Extraversion aus dem Datensatz big5 herausziehen. Die zu testende Hypothese kann mit dem mu Argument entsprechend angepasst werden. Außerdem müssen wir var.equal = TRUE setzen, da ansonsten mit der gleichen Funktion der Welch Test berechnet wird. t.test(big5$Extraversion) One Sample t-test data: big5$Extraversion t = 125.51, df = 199, p-value &lt; 2.2e-16 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 3.027672 3.124328 sample estimates: mean of x 3.076 Die Hypothese kann mit dem alternative Argument auf einseitige Tests (\"less\" oder \"greater\") umgestellt werden. Die Standardeinstellung ist ein zweiseitiger Test. Das \\(\\alpha\\) Niveau kann über das Argument conf.level modifiziert werden. Ausgelassen wird ein Konfidenzlevel von 0.95 verwendet. Für einen Mittelwertsvergleich zwischen zwei Gruppen (hier Männer und Frauen) nutzen wir die bekannte Formelsyntax. Exemplarisch sei hier der Welch Test berechnet (var.equal = FALSE ist die Standardeinstellung der Funktion). Diesen würde man verwenden, wenn zwar Normalverteilung aber keine Varianzhomogenität gegeben wäre. t.test(Extraversion ~ Geschlecht, data = big5, var.equal = TRUE) Two Sample t-test data: Extraversion by Geschlecht t = -1.2746, df = 198, p-value = 0.2039 alternative hypothesis: true difference in means between group f and group m is not equal to 0 95 percent confidence interval: -0.16152463 0.03469536 sample estimates: mean in group f mean in group m 3.050000 3.113415 Für einen Test auf abhängige Stichproben, kann das paired Argument hinzugefügt werden. t.test(Extraversion ~ Geschlecht, data = big5, paired = TRUE) 9.3.2 Wilcoxon Test Skalenniveau: intervallskaliert Voraussetzungen: beide Gruppen folgen derselben Verteilung (welche Verteilung spielt dabei keine Rolle) Der Wilcoxon Test (auch Mann Whitney U-Test genannt) wird bei Verletzungen der Normalverteilung und Varianzhomogenität verwendet. Die Formelsyntax bleibt dabei die selbe wie bei den anderen Tests. wilcox.test(Extraversion ~ Geschlecht, data = big5) Wilcoxon rank sum test with continuity correction data: Extraversion by Geschlecht W = 4265, p-value = 0.1528 alternative hypothesis: true location shift is not equal to 0 9.4 Unterschiede mehrerer Gruppen 9.4.1 Varianz- und Kovarianzanalyse Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Varianzanalysen (auch ANOVA genannt) benötigen für Typ 2 und Typ 3 Quadratsummen in R das Package car. library(car) Von hier an muss zuerst mit aov() (Akronym für analysis of variances, engl. für Varianzanalyse) das eigentliche Modell aufgestellt werden. Die Syntax ist wie im Einführungskapitel beschrieben. Wenn man die verschiedenen unabhängigen Variablen mit einem Plus-Zeichen hinzufügt, werden die Unterschiede unabhängig voneinander geprüft. result &lt;- aov(Extraversion ~ Gruppe + Alter, data = big5_mod) result %&gt;% Anova(type = 3) Anova Table (Type III tests) Response: Extraversion Sum Sq Df F value Pr(&gt;F) (Intercept) 100.840 1 850.7365 &lt;2e-16 *** Gruppe 0.314 2 1.3235 0.2686 Alter 0.084 1 0.7052 0.4021 Residuals 23.232 196 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Alternativ kann das Plus-Zeichen mit einem Asterisk (beziehungsweise Muliplikationszeichen) ausgetauscht werden. So würde man die Interaktion der Kovariaten berücksichtigen und somit eine so genannte ANCOVA (Akronym für analysis of covariance, engl. für Kovarianzanalyse) berechnen. aov(Extraversion ~ Gruppe * Alter, data = big5_mod) %&gt;% Anova(type = 3) Anova Table (Type III tests) Response: Extraversion Sum Sq Df F value Pr(&gt;F) (Intercept) 55.687 1 471.7948 &lt;2e-16 *** Gruppe 0.410 2 1.7388 0.1785 Alter 0.083 1 0.7067 0.4016 Gruppe:Alter 0.334 2 1.4152 0.2454 Residuals 22.898 194 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Die Interaktionseffekte werden mit einem Doppelpunkt in der Ausgabe markiert (hier Geschlecht:Alter). Für die genauer Untersuchung des Haupteffekts Gruppe, müssen wir bei Signifikanz noch einen Post-Hoc Test machen. Eine Möglichkeit dafür ist der Test von Tukey mit der Funktion TukeyHSD() (Akronym für Honest Significant Differences). TukeyHSD(result, &quot;Gruppe&quot;) Warning in replications(paste(&quot;~&quot;, xx), data = mf): non-factors ignored: Alter Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = Extraversion ~ Gruppe + Alter, data = big5_mod) $Gruppe diff lwr upr p adj Mittel-Jung 0.008320251 -0.1381335 0.15477398 0.9901253 Weise-Jung -0.210544218 -0.4379624 0.01687397 0.0760155 Weise-Mittel -0.218864469 -0.4721886 0.03445966 0.1053580 Eine Alternative dazu stellt der t Test dar, welcher die einzelnen Gruppen bezüglich der Extraversion paarweise vergleicht. Dafür müssen wir der Funktion pairwise.t.test() die entsprechenden Spalten als Vektoren übergeben. pairwise.t.test(big5_mod$Extraversion, big5_mod$Gruppe) Pairwise comparisons using t tests with pooled SD data: big5_mod$Extraversion and big5_mod$Gruppe Jung Mittel Mittel 0.89 - Weise 0.09 0.09 P value adjustment method: holm 9.4.2 Varianzanalyse mit Messwiederholung Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Für die Berechnung von Varianzanalysen mit Messwiederholung benötigen wir das afex Package. library(afex) library(emmeans) Bevor wir nun zur Berechnung kommen, müssen wir den Datensatz erst in ein langes Format bringen. big5_long &lt;- big5_mod %&gt;% pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) result2 &lt;- aov_4(Auspraegung ~ Gruppe + (Faktor | ID), data = big5_long) result2 Anova Table (Type 3 tests) Response: Auspraegung Effect df MSE F ges p.value 1 Gruppe 2, 197 0.30 2.69 + .014 .070 2 Faktor 1, 197 0.27 0.10 &lt;.001 .749 3 Gruppe:Faktor 2, 197 0.27 1.60 .008 .205 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Als Post-Hoc Test können wir in diesem Fall die geschätzten Randsummen mithilfe von emmeans() (Akronym für estimated marginal means) ausgeben lassen. post_aov &lt;- emmeans(result2, ~ Gruppe) post_aov Gruppe emmean SE df lower.CL upper.CL Jung 3.14 0.0321 197 3.08 3.20 Mittel 3.03 0.0624 197 2.91 3.15 Weise 2.93 0.1042 197 2.73 3.14 Results are averaged over the levels of: Faktor Confidence level used: 0.95 Für Signifikanztests der jeweiligen Randsummen können wir jene der Funktion pair() übergeben. pairs(post_aov) contrast estimate SE df t.ratio p.value Jung - Mittel 0.1097 0.0702 197 1.563 0.2642 Jung - Weise 0.2083 0.1090 197 1.911 0.1382 Mittel - Weise 0.0986 0.1214 197 0.812 0.6959 Results are averaged over the levels of: Faktor P value adjustment: tukey method for comparing a family of 3 estimates 9.4.3 Kruskal-Wallis Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Den Kruskal-Wallis Test können wir anwenden, wenn wir mehr als zwei Gruppen untersuchen und wir nicht von Normalverteilung und Varianzhomogenität ausgehen können. Die Syntax bleibt dabei wie gewohnt. kruskal.test(Extraversion ~ Gruppe, data = big5_mod) %&gt;% tidy() # A tibble: 1 × 4 statistic p.value parameter method &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; 1 4.65 0.0976 2 Kruskal-Wallis rank sum test 9.4.4 Friedman Test Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Den Friedman Test verwenden wir in den selben Anwendungsfällen wie den Kruskal-Wallis Test. Der Unterschied ist an dieser Stelle die zusätzliche Berücksichtigung von Innersubjektfaktoren. Wir benötigen daher wie im Kontext der Varianzanalyse mit Messwiederholung in Kapitel 9.4.2 den Datensatz wieder im langen Format. Von da unterscheidet sich die Syntax nur insofern, als das wir keine Klammern um den Messwiederholungsterm auf der rechten Seite der Tilde schreiben. friedman.test(Auspraegung ~ Faktor | ID, data = big5_long) Friedman rank sum test data: Auspraegung and Faktor and ID Friedman chi-squared = 1.3474, df = 1, p-value = 0.2457 9.5 Korrelationskoeffizienten 9.5.1 Korrelationstests Skalenniveau: intervallskaliert für Pearson, ordinal für Spearman oder Kendall Voraussetzungen: … Mit der Funktion cor.test() können die Produkt-Moment Korrelation nach Pearson (“pearson”), Rangkorrelation nach Spearman (spearman) oder Kendall (kendall) angewendet werden. Dafür muss nur das method Argument entsprechend angepasst werden. Als Argumente übergeben wir außerdem die beiden numerischen Spalten, die wir mit dem Dollar-Operator aus dem Datensatz herausziehen müssen. cor.test(big5$Extraversion, big5$Neurotizismus, method = &quot;pearson&quot;) Pearson&#39;s product-moment correlation data: big5$Extraversion and big5$Neurotizismus t = 0.98352, df = 198, p-value = 0.3266 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.06968989 0.20646897 sample estimates: cor 0.06972529 Für polychorische Korrelationen muss das polycor Package installiert und geladen sein. library(polycor) Die Funktion polychor() benötigt lediglich die zwei numerischen Spalten, die untersucht werden sollen. Dabei muss die Spalte als Zahlenreihe (Vektor) aus dem Datensatz herausgezogen werden. polychor(big5$Extraversion, big5$Neurotizismus) [1] 0.05354389 9.5.2 Korrelationstabellen Mit der Zeit werden alle Statistikfunktionen auf das inductive Package umgestellt. Wenn mehrere Korrelationen auf einmal berechnet werden sollen, können wir die Funktion cor() benutzen. Dabei können wir der Funktion grundsätzlich so viele numerische Spalten übergeben, wie wir wollen. big5 %&gt;% select(Alter, Extraversion, Neurotizismus) %&gt;% cor() Alter Extraversion Neurotizismus Alter 1.00000000 -0.07385301 -0.03123432 Extraversion -0.07385301 1.00000000 0.06972529 Neurotizismus -0.03123432 0.06972529 1.00000000 9.6 Regressionsmodelle 9.6.1 Lineare Regression Skalenniveau der abhängigen Variable: intervallskaliert Skalenniveau der unabhängigen Variable(n): intervallskaliert oder binomial; bei multinomialen Variablen mit mehr als zwei Ausprägungen oder Gruppen müssen Dummy Variablen erstellt werden Voraussetzungen: Normalverteilung der Residuen, … Die lm() Funktion (Akronym für lineares Modell) haben wir bereits im Kontext von Varianzanalysen kennengelernt (siehe Kapitel 9.4.1). Für Regressionsmodelle können wir uns einen Schritt sparen. Die Syntax bleibt allerdings die gleiche. Beachte an dieser Stelle, dass die unabhängigen Variablen bei Regressionsanalysen entweder binär (0, 1) oder intervallskaliert sein müssen. model &lt;- lm(Extraversion ~ Geschlecht + Alter, data = big5) Die Zusammenfassung der Ergebnisse erhalten wir wieder schön formatiert mit tidy(). tidy(model) # A tibble: 3 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3.06 0.0327 93.4 4.12e-165 2 Geschlechtm 0.0597 0.0499 1.19 2.34e- 1 3 Alter -0.000119 0.000126 -0.945 3.46e- 1 Zusätzliche Ausgaben wie Informationskriterien (AIC, BIC) erhalten wir durch glance(), welches auch im broom Package enthalten ist. glance(model) # A tibble: 1 × 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.0126 0.00259 0.346 1.26 0.286 2 -70.1 148. 161. 23.6 197 # … with 1 more variable: nobs &lt;int&gt; Falls wir, wie bei einer Varianzanalyse, eine gruppierende Variable wie Altersgruppe (Gruppe) mit mehr als 2 Stufen innerhalb einer Regression untersuchen möchten, müssen wir zuerst so genannte Dummy Variablen erstellen. Das heißt, wir erstellen uns für jede Altersgruppe außer unserer Referenzgruppe (z.B. “Jung”) eine binäre neue Spalte. Während wir das ebenfalls manuell mit if_else() bewerkstelligen könnten, verwenden wir an dieser Stelle Faktoren, welche die Aufgabe für uns intern übernehmen. big5_mod1 &lt;- big5_mod %&gt;% mutate(Gruppe = as.factor(Gruppe)) Wenn die gruppierende Variable ein Faktor ist, erstellt R von selbst die dummy Variablen und gibt \\(\\beta\\) Koeffizienten mit entsprechendem Signifikanztest für jede der Faktorstufen (außer der Referenzgruppe) aus. lm(Extraversion ~ Gruppe, data = big5_mod1) %&gt;% tidy() # A tibble: 3 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3.09 0.0284 109. 6.14e-178 2 GruppeMittel 0.00832 0.0620 0.134 8.93e- 1 3 GruppeWeise -0.211 0.0962 -2.19 2.98e- 2 9.6.2 Logistische Regression Skalenniveau der abhängigen Variable: binomial Skalenniveau der unabhängigen Variable(n): intervallskaliert oder binomial; bei multinomialen Variablen mit mehr als zwei Ausprägungen oder Gruppen müssen Dummy Variablen erstellt werden Voraussetzungen: Normalverteilung der Residuen, … Falls die abhängige Variable nicht intervallskaliert (respektive metrisch) sondern binär (0, 1) ist, müssen wir eine logistische Regression (auch binomiales Logit Modell) verwenden. In R trägt die Funktion den Namen glm() (Akronym für Generalisiertes Lineares Modell). Exemplarisch erstellen wir uns zu Beginn eine binäre abhängige Variable (Geschlecht). big5_bin &lt;- big5 %&gt;% mutate(Geschlecht = if_else(Geschlecht == &quot;m&quot;, 1, 0)) Nun können wir genau wie bei der linearen Regression das Modell aufstellen und mit tidy() eine schöne Ausgabe erstellen. model2 &lt;- glm(Geschlecht ~ Extraversion + Alter, data = big5_bin) tidy(model2) # A tibble: 3 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 0.0475 0.313 0.152 0.880 2 Extraversion 0.121 0.101 1.19 0.234 3 Alter -0.000186 0.000179 -1.03 0.302 Mit glance() erhalten wir auch hier wieder zusätzliche Informationen. glance(model2) # A tibble: 1 × 8 null.deviance df.null logLik AIC BIC deviance df.residual nobs &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 48.4 199 -141. 289. 302. 47.7 197 200 9.6.3 Multinomiales Logit Modell Skalenniveau der abhängigen Variable: multinomial Skalenniveau der unabhängigen Variable(n): intervallskaliert oder binomial; bei multinomialen Variablen mit mehr als zwei Ausprägungen oder Gruppen müssen Dummy Variablen erstellt werden Voraussetzungen: Normalverteilung der Residuen, … Zur Berechnung von multinomialen Logit Modellen muss das VGAM Package installiert und geladen sein. library(VGAM) Multinomiale Logit Modelle (auch Baseline Logit Modelle) verwenden wir dann, wenn unsere abhängige Variable nicht binär ist, sondern mehrere ungeordnete Stufen hat. Die abhängige Variable ist also multinomial. Hier schauen wir uns exemplarisch den Einfluss von Geschlecht und Alter auf eine Frage zur Offenheit (O1) an. Diese ist zwar eigentlich ordinal skaliert, allerdings bleiben wir der einfachheitshalber bei dem Beispiel. Entscheiden ist hier das family Argument. Wir müssen festlegen, welche Faktorstufe die Referenzkategorie ist, mit der wir die restlichen Kategorien vergleichen wollen. vglm(O1 ~ Geschlecht + Alter, data = big5, family = multinomial(refLevel = 1)) %&gt;% summary() Call: vglm(formula = O1 ~ Geschlecht + Alter, family = multinomial(refLevel = 1), data = big5) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 -0.43014 1.56406 -0.275 0.783 (Intercept):2 0.85767 1.50833 0.569 0.570 (Intercept):3 0.98431 1.46124 0.674 0.501 (Intercept):4 0.66895 1.46387 0.457 0.648 Geschlechtm:1 1.57251 1.20827 1.301 0.193 Geschlechtm:2 1.70249 1.16454 1.462 0.144 Geschlechtm:3 0.25837 1.15895 0.223 0.824 Geschlechtm:4 0.89654 1.15546 0.776 0.438 Alter:1 0.05434 0.06709 0.810 0.418 Alter:2 0.02951 0.06614 0.446 0.655 Alter:3 0.06521 0.06442 1.012 0.311 Alter:4 0.06540 0.06442 1.015 0.310 Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1]), log(mu[,4]/mu[,1]), log(mu[,5]/mu[,1]) Residual deviance: 534.8599 on 788 degrees of freedom Log-likelihood: -267.43 on 788 degrees of freedom Number of Fisher scoring iterations: 9 No Hauck-Donner effect found in any of the estimates Reference group is level 1 of the response Beachte auch, dass wir hier summary() und nicht tidy() zur Ergebnisausgabe verwenden müssen. 9.6.4 Kumulatives Logit Modell Skalenniveau der abhängigen Variable: ordinal Skalenniveau der unabhängigen Variable(n): intervallskaliert oder binomial; bei multinomialen Variablen mit mehr als zwei Ausprägungen oder Gruppen müssen Dummy Variablen erstellt werden Voraussetzungen: Normalverteilung der Residuen, … Zur Berechnung von kumulativen Logit Modellen muss das VGAM Package installiert und geladen sein. library(VGAM) Kumulative Logit Modelle (auch Proportional Odds Model) verwenden wir bei ordinal skalierter abhängiger Variable. Die Variable O1 ist eine Frage zur Offenheit die von 0 (trifft gar nicht zu) bis 5 (trifft zu) bewertet wird. Im Vergleich zur Baseline Logit Modellen verändert sich an dieser Stelle nur das family Argument. Hier müssen wir die cumulative() Funktion verwenden. vglm(O1 ~ Geschlecht + Alter, data = big5, family = cumulative(parallel = TRUE)) %&gt;% summary() Call: vglm(formula = O1 ~ Geschlecht + Alter, family = cumulative(parallel = TRUE), data = big5) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 -3.8522159 0.4216987 -9.135 &lt; 2e-16 *** (Intercept):2 -2.0843957 0.1859841 -11.207 &lt; 2e-16 *** (Intercept):3 -0.7749747 0.1356009 -5.715 1.10e-08 *** (Intercept):4 0.6702009 0.1488966 4.501 6.76e-06 *** Geschlechtm 0.4442114 0.1552295 2.862 0.00421 ** Alter -0.0005654 0.0004816 -1.174 0.24040 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Names of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2]), logitlink(P[Y&lt;=3]), logitlink(P[Y&lt;=4]) Residual deviance: 552.8905 on 794 degrees of freedom Log-likelihood: -276.4453 on 794 degrees of freedom Number of Fisher scoring iterations: 17 Warning: Hauck-Donner effect detected in the following estimate(s): &#39;(Intercept):1&#39; Exponentiated coefficients: Geschlechtm Alter 1.5592601 0.9994348 9.6.5 Poisson Regression Skalenniveau der abhängigen Variable: Häufigkeiten Skalenniveau der unabhängigen Variable(n): intervallskaliert oder binomial; bei multinomialen Variablen mit mehr als zwei Ausprägungen oder Gruppen müssen Dummy Variablen erstellt werden Voraussetzungen: Normalverteilung der Residuen, … TEXT mit negative binomial und quasipoisson bei overdispersion 9.6.6 Cox Regression Skalenniveau der abhängigen Variable: binomal (zeitabhängig) Skalenniveau der unabhängigen Variable(n): intervallskaliert oder binomial; bei multinomialen Variablen mit mehr als zwei Ausprägungen oder Gruppen müssen Dummy Variablen erstellt werden Voraussetzungen: Normalverteilung der Residuen, … Zur Berechnung einer sogenannten Cox Regression, muss das survival Package installiert und geladen sein. library(surival) Für unser Beispiel nehmen wir uns den lung Datensatz aus dem survival Package zur Hand. lung # A tibble: 228 × 10 inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 306 2 74 1 1 90 100 1175 NA 2 3 455 2 68 1 0 90 90 1225 15 3 3 1010 1 56 1 0 90 90 NA 15 4 5 210 2 57 1 1 90 60 1150 11 # … with 224 more rows Mit der Funktion coxph() können wir nun in fast gewohnter Manier unser Modell schätzen. Besonders ist an dieser Stelle, dass wir der Funktion keine einzelne abhängige Variable übergeben. Stattdessen übergeben wir der Funktion Surv() sowohl die Überlebenszeit in Tagen als auch die Information über den Tod beziehungsweise Zensierung (event = status). Falls eine Modellierung in Abhängigkeit der Start- und Endzeit gewünscht ist, kann die Startzeit dem time und die Endzeit dem time2 Argument übergeben werden. Hier schauen wir uns exemplarisch den Einfluss der Kovariate Geschlecht auf die Überlebenszeit an. Dabei wird das biologische Geschlecht in männlich (1) und weiblich (2) kodiert. cox_res &lt;- coxph(Surv(time = time, event = status) ~ sex, data = lung) summary(cox_res) Call: coxph(formula = Surv(time = time, event = status) ~ sex, data = lung) n= 228, number of events= 165 coef exp(coef) se(coef) z Pr(&gt;|z|) sex -0.5310 0.5880 0.1672 -3.176 0.00149 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 exp(coef) exp(-coef) lower .95 upper .95 sex 0.588 1.701 0.4237 0.816 Concordance= 0.579 (se = 0.021 ) Likelihood ratio test= 10.63 on 1 df, p=0.001 Wald test = 10.09 on 1 df, p=0.001 Score (logrank) test = 10.33 on 1 df, p=0.001 In der Ausgabe sehen wir die Ergebnisse des Likelihood Ratio, Rao Score und Wald Tests. Die entsprechende Hazard Ratio der Kovariaten kann unter exp(coef) abgelesen werden (hier 0.5880). Eine Hazard Ratio unter 1 sagt in unserem Beispiel also eine Risikoreduktion bei weiblichem Geschlecht aus. # surfdiff fuer 2 Gruppen # wenn auch hr oder mehr als 2 cox regression # glance bei coxregression fuer log rank test fit6 &lt;- coxph(Surv(time = OS_months, event = OS) ~ Abteilung, data = daten1) summary(fit6) tidy(fit6) glance(fit6) survdiff(Surv(time = OS_months, event = OS) ~ Abteilung, data = daten1) 9.7 Kontingenztafeln Für die Auswertung von Kontingenztafeln nutzen wir die in Kapitel 7.2 erstellte Vierfeldertafel, die die Extraversionsausprägung über 3 gegen das Geschlecht aufträgt. tbl &lt;- table(big5_mod$Extraversion &gt; 3, big5_mod$Geschlecht) 9.7.1 Fisher-Exact Test Skalenniveau: Häufigkeiten Voraussetzungen: … Nach Erstellung der Kontingenztafel mit table() können wir diese einfach der Funktion fisher.test() übergeben. fisher.test(tbl) Fisher&#39;s Exact Test for Count Data data: tbl p-value = 0.6663 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.6284963 2.1062910 sample estimates: odds ratio 1.150348 Als Ausgabe erhalten wir unter anderem die Odds Ratio (estimate) und Konfidenzintervalle. Auch hier können wir mit dem alternative Argument auf einseitiges Testen umstellen (“less”, “greater”). 9.7.2 Chi Quadrat-Tests Skalenniveau: Häufigkeiten Voraussetzungen: … Für die Berechnung des \\(\\chi^2\\) Tests nach Pearson verwenden wir die Funktion chisq.test(). chisq.test(tbl) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: tbl X-squared = 0.11804, df = 1, p-value = 0.7312 Bei abhängigen Gruppen würde man hingegen stattdessen Mcnemars \\(\\chi^2\\) Test verwenden. mcnemar.test(tbl) McNemar&#39;s Chi-squared test with continuity correction data: tbl McNemar&#39;s chi-squared = 0.67368, df = 1, p-value = 0.4118 9.8 Ergebnisse formatieren library(broom) "],["ergebnisse-exportieren.html", "Kapitel 10 Ergebnisse exportieren 10.1 Einführung 10.2 Berichte 10.3 Tabellen", " Kapitel 10 Ergebnisse exportieren 10.1 Einführung Nachdem die Analysen berechnet sind, möchte man die Ergebnisse davon gerne in das Programm exportieren, in dem die man die eigentliche Arbeit schreibt. Bevor wir uns in den folgenden Kapiteln anschauen, wie das genau funktioniert, müssen wir zuvor ein paar Begrifflichkeiten klären. R besitzt eine nahezu perfekte Möglichkeit, die Ergebnisse mithilfe des sogenannten rMarkdowns in verschiedene Formate umzuwandeln. Genauer gesagt kann ein spezielles R Dokument (rMarkdown) direkt in HTML, PDF oder ein Word Dokument umgewandelt werden. HTML. Das sogenannte Hypertext Markup Language (HTML) Format ist das Rückrad des Internets. Es gibt die Form von Internetseiten vor, die von Browsern (z.B. Firefox, Chrome oder Brave) gelesen werden. Dadurch ist es sehr praktisch zum gegenseitigen Teilen von Inhalten, da jeder einen Browser auf dem Computer installiert hat. Man benötigt zum Umwandeln von rMarkdown in HTML auch keinerlei zusätzliche Software. PDF. Das PDF Format wird durch LaTeX erstellt. LaTeX ist ein Programm, was in abgeänderter Form bereits seit Ende der 70er Jahre vor allem von Naturwissenschaftlern für die Erstellung wissenschaftlicher Arbeiten verwendet wird. Es ist also notwendig, dieses Programm auf dem Computer installiert zu haben, wenn man seine Analyse in ein PDF umwandeln möchte. Die Installation wird in den Folgekapitel an geeigneter Stelle erläutert. Word. Die Erstellung von Word Dokumenten funktioniert hinter den Kulissen durch Pandoc. Glücklicherweise musst du Dir um die Installation seit RStudio 1.3 keine Sorgen mehr machen, da es bereits vorinstalliert ist. Grundsätzlich sind Word Dokumente immer ein kleines Sorgenkind, da es sich hierbei nicht um ein frei zugängliches Format handelt. Deswegen ist es für Package-Entwickler auch deutlich schwieriger, dafür Erweiterungen zu entwickeln. Komplexe Tabellen sind daher in HTML und PDF deutlich schöner als in Word. Tatsächlich wird dort kein .docx Dokument sondern ein .rtf (Rich Text Format) kreiert. Dies erlaubt den Entwicklern etwas mehr Flexibilität. Eine Möglichkeit für Word-Nutzer besteht darin, die Werte mit rMarkdown umzuwandeln und alles weitere, potentiell komplexere dann direkt in Word anzupassen. rMarkdown. Markdown ist an sich eine Möglichkeit, einfacher und intuitiver HTML Inhalte zu schreiben (z.B. Texte für Internetseiten). Man kann beispielsweise ohne viel Aufwand Wörter fett oder kursiv schreiben und Abbildungen integrieren. rMarkdown erweitert dies um einige Funktionen und ermöglicht es uns, direkt aus R verschiedene Dokumenttypen zu erstellen. Das rmarkdown Package ist dabei direkt in RStudio integriert. Es handelt sich hierbei nicht um ein R Skript, sondern um einen eigenen Dateityp, welcher mit .Rmd und nicht mit .R endet. 10.2 Berichte Für das Kreieren der Tabellen im Rahmen dieses Kapitel wird das knitr Package benötigt. library(knitr) Öffne zum Erstellen von rMarkdown (.Rmd) Dokumenten das Dropdown-Menü mit dem Papier und dem grünen Plus-Zeichen unter dem Reiter File. Abbildung 10.1: Erster Schritt in der Erstellung eines neuen R Markdown Skripts. Wähle dort R Markdown aus. Die Punkte implizieren, dass noch weitere Informationen vor dem Erstellen notwendig sind. Anschließen kann man den gewünschten Ausgabetypen, Titel und Autor festlegen. Beim Klicken auf OK wird eine Vorlage erstellt, die man nach belieben anpassen kann. Abbildung 10.2: Zweiter Schritt zur Erstellung eines neuen R Markdown Skripts. Die wichtigsten Formatierungsmöglichkeiten in Markdown sehen wie folgt aus: Fett gedruckt: **fett** ergibt fett Kursiv: *kursiv* ergibt kursiv Code Integration im Text: `mean(x)` ergibt mean(x) Links: [Klicke hier](https://cran.r-project.org/) wird zu: Klicke hier (im gebundenen Buch sind Links nicht extra farblich hervorgehoben) Überschriften können mit einer führenden Raute (# Überschrift 1) erstellt werden. Für Abschnitte innerhalb des Hauptkapitels können beliebig viele Rauten hinzugefügt werden (z.B. ## Unterkapitel 1). In der Praxis sähe das wie folgt aus: – Beginn der Datei Beispiel.Rmd – --- title: &quot;Vorläufige Ergebnisse&quot; author: &quot;Student A&quot; date: &quot;2022-10-02&quot; output: html_document --- ```{r setup, include = FALSE} library(tidyverse) library(knitr) library(remp) data(big_five) ``` ## Deskriptive Statistik Verschiedene Lagemaße wie Minimum, 1. Quartil, Mittelwert, Median, 3. Quartil, Maximum, Standardabweichung und Standardfehler. ```{r} big_five %&gt;% select(Extraversion, Neurotizismus) %&gt;% descriptive() %&gt;% kable() ``` ## Visualisierung Ein Streudiagramm zur anschaulichen Darstellung des Zusammenhangs zwischen **Extraversion** und **Neurotizismus**. ```{r, echo = FALSE} ggplot(big_five, aes(x = Extraversion, y = Neurotizismus)) + geom_point(position = &quot;jitter&quot;) ``` – Ende der Datei Beispiel.Rmd – Die Ausgabe nach Umwandlung in ein HTML Dokument ist in Abbildung 10.3 zu sehen. Abbildung 10.3: Umwandlung von Markdown in HTML. Jedes rMarkdown Dokument (Endung .Rmd) beginnt mit einem so genannten YAML-Kopf, der durch drei Bindestriche oben und unten vom restlichen Dokument abgegrenzt ist. In diesem Beispiel wird der Titel, Autor, das Datum und Ausgabeformat festgelegt. Für weitere Anpassungsoptionen sei auf das Buch R Markdown - The Definitive Guide von Xie, Allaire und Grolemund verwiesen. Als nächstes sehen wir die grau hinterlegten Code Abschnitte. Diese werde mithilfe von drei Backticks (```) erstellt und erlauben das Ausführen von R Code. Den Backticks folgen geschweifte Klammern. Die erste Information ist die verwendete Programmiersprache (hier R). Getrennt mit einem Komma können hier diverse weitere Argumente angegeben werden. Zum Beispiel kann mit echo = FALSE der Code versteckt werden. So könnte man ein Dokument erstellen, in dem nur die Ergebnisse in Form von Tabellen und kein R Code enthalten ist. Umwandeln können wir das rMarkdown Dokument durch Klicken auf das Wort Knit (engl. für stricken) in der Leiste unter dem Reiter der geöffneten Dateien. Wenn man nur auf das Symbol drückt, wird die Datei in das an erster Stelle im YAML Kopf festgelegten Format umgewandelt. Besser ist es jedoch, das Dropdown-Menü durch einen Klick auf den Pfeil nach unten zu öffnen und den gewünschten Dateityp auszuwählen. Abbildung 10.4: Umwandlung des R Markdown Skripts in HTML, PDF oder Word. Die eigentliche Umwandlung übernimmt das knitr Package, was im Hintergrund geladen wird. Zur Umwandlung zu HTML muss nichts weiter beachtet werden. Bei der Umwandlung in Word wird das Programm Pandoc (https://pandoc.org/installing.html) benötigt, welches in neueren RStudio Versionen direkt integriert ist. Möchte man in ein PDF umwandeln, muss eine Version von LaTeX auf dem Computer installiert sein. Die einfachste Möglichkeit hierfür ist die Installation des R Packages tinytex. library(tinytex) rMarkdown Dateien können nur umgewandelt werden, wenn alle nötigen Informationen enthalten sind. Es müssen also innerhalb der Datei alle Packages und Datensätze explizit geladen werden. Dies trifft auch zu, obwohl man die Packages oder Datensätze möglicherweise bereits vorher verwendet hat. Man kann jede rMarkdown Datei als isoliert von allem anderen in RStudio geöffneten betrachten. Das Unternehmen RStudio hat eine Alternative namens Quarto entwickelt. Wenn man nur R und keine anderen Programmiersprachen verwendet, bietet der Umstieg in absehbarer Zeit keinen Vorteil. Die Syntax von rMarkdown und Quarto ist allerdings sehr ähnlich, weswegen sich ein Umstieg als einfach herausstellen sollte. Verpflichtend ist dieser aber nicht, da rMarkdown auch weiterhin Fehlerbehebungen erhalten und in der kennengelernten Form weiter existieren wird. 10.3 Tabellen Abgesehen von Kurzberichten benutzen die meisten für umfangreichere Arbeiten oder Publikationen andere Programme wie Word oder LaTeX. Oft möchte man daher nur kurz eine Tabelle im richtigen Format ausgeben lassen. 10.3.1 Exportieren nach Word Für die Tabellen benötigen wir auch hier das knitr Package, für die Umwandlung der Tabellen direkt als PDF brauchen wir tinytext. library(knitr) library(tinytext) --- title: &#39;Beispiel&#39; output: word_document: default fontsize: 12pt --- ```{r setup, include = FALSE} library(tidyverse) library(knitr) library(remp) data(big_five) ``` ```{r, echo=FALSE} big_five %&gt;% select(Extraversion, Neurotizismus) %&gt;% descriptive() %&gt;% kable() ``` Es gibt verschiedene Möglichkeiten, Tabellen im Word Format durch R Code zu erstellen. Wir werden uns hier auf das bereits in RStudio integrierte Pandoc und das in Kapitel 10.2 kennengelernte rMarkdown verlassen. Denn mit nur einer Funktion aus dem knitr Package, welches diverse Modifikationen für rMarkdown Dokumente bietet, können schöne Tabellen erstellt werden. Der Funktion kable() aus besagtem Package wird lediglich der Datensatz übergeben. Dann muss man nur noch das rMarkdown Dokument mithilfe von Pandoc in ein Word-Dokument umwandeln (siehe Kapitel 10.2. Abbildung 10.5: Beispielhafter Output einer Tabelle von R Markdown in Word. Das Aussehen kann innerhalb von Word dann entsprechend angepasst werden. Beachte die Breite einer gewöhnlichen Din A4 Seite in Word. Wenn du eine gigantische Korrelationsmatrix ausgeben möchtest, solltest du entweder eine Word Vorlage in Querformat verwenden (die Vorlage muss den gleichen Dateinamen wie das rMarkdown Dokument haben) oder die Tabelle in Teilen ausgeben lassen. Natürlich können wir auf gleiche Art und Weise auch Tabellen in pdf oder html Form erstellen. Da wir damit aber schlecht weiterarbeiten können, ist in diesem Kapitel der Fokus auf Word gerichtet. Für das Umwandeln von rMarkdown in Word durch Pandoc ist eine Installation von entweder Microsoft Office oder LibreOffice auf dem Computer nötig. 10.3.2 Exportieren nach LaTeX Für dieses Kapitel wird das xtable Package benötigt. library(xtable) Es gibt auch die Möglichkeit, tibbles aus R direkt in LaTeX Code umzuwandeln, um diesen dann in die zugehörige TeX Datei zu kopieren. Zuvor wurde LaTeX nur hinter den Kulissen direkt zum Erstellen von PDFs verwendet. Wenn man die Arbeit direkt in LaTeX schreibt, benötigt man allerdings den tatsächlichen LaTeX Code. Dafür greift man auf die Funktion xtable() aus dem gleichnamigen Package zurück. Das wichtigste Argument der Funktion ist digits, mit dem man pro Spalte die Anzahl der gerundeten Nachkommastellen festlegt. Dabei kann man entweder eine einzelne Zahl eingeben und somit für alle Spalten die gleiche Rundung generieren oder für jede einzeln als Vektor mithilfe von c(). Da in der Funktion auch die Zeilennamen berücksichtigt werden, muss der Vektor immer um eins länger sein als die Anzahl der Spalten des eigentlichen Datensatzes. big_five %&gt;% select(Extraversion, Neurotizismus) %&gt;% descriptive() %&gt;% select(Variable, Min, Mean, Median, Max, SE) %&gt;% xtable(digits = c(0, 1, 2, 2, 2, 2, 3)) %&gt;% print(include.rownames = FALSE) % latex table generated in R 4.2.1 by xtable 1.8-4 package % Sun Oct 2 14:10:09 2022 \\begin{table}[ht] \\centering \\begin{tabular}{lrrrrr} \\hline Variable &amp; Min &amp; Mean &amp; Median &amp; Max &amp; SE \\\\ \\hline Extraversion &amp; 2.30 &amp; 3.08 &amp; 3.00 &amp; 4.30 &amp; 0.020 \\\\ Neurotizismus &amp; 1.40 &amp; 3.13 &amp; 3.10 &amp; 4.60 &amp; 0.050 \\\\ \\hline \\end{tabular} \\end{table} Die Funktion print() mit dem Argument include.rownames verhindert, dass jede Zeile im Datensatz nummeriert ausgegeben wird. "],["datatypes.html", "Kapitel 11 Datenstrukturen 11.1 Vektor 11.2 Matrix 11.3 Data.frame und tibble 11.4 Liste 11.5 Umwandlungen 11.6 Objektorientierung", " Kapitel 11 Datenstrukturen Es mag Dir bereits aufgefallen sein, dass immer wieder Verweise auf dieses Kapitel im Verlauf des Buches gemacht wurden. Noch vor einigen Jahren gab es für niemanden einen Zweifel daran, dass Datenstrukturen in Programmiersprachen direkt zu Beginn gelehrt werden sollten. Wenn du es bis hierher geschafft hast, wirst du jedoch gemerkt haben, auch prima ohne diese Grundlagen der Programmiersprache zurecht gekommen zu sein. Trotzdem ist ein Verständnis der verschiedenen Datenstrukturen, wie man darauf zugreift und diese ineinander umwandelt, eine essentielle Fertigkeit, wenn man sich tiefer mit R beschäftigt. Das tidyverse bietet viele Funktionen, um diesen klassischen in base R implementierten Umgang mit Datenstrukturen zu umgehen. Das hat vor allem Gründe der Inkonsistenz und unpraktischen Designentscheidungen, die aus historischen Gründen bestehen. R gibt es mittlerweile seit über 25 Jahren. Nicht alles, was aus damaliger Sicht Sinn gemacht hat, ist in der Form heute auch noch vernünftig für Anwender. Für Anwendungen, die über das tidyverse hinaus gehen, ist es unabdingbar, dieses Wissen zu haben. Wir schauen uns nun die Datenstrukturen und wie man auf sie zugreift der Reihe nach an. 11.1 Vektor Manche sagen, alle Datenstrukturen in R sind Objekte. Während das zwar grundsätzlich korrekt ist, ist es verwirrend im Sinne dessen, was normalerweise mit Objektorientierung gemeint ist. Deswegen gehen wir an dieser Stelle eine Abstraktionsstufe hinunter und sagen guten Gewissens: Alles in R ist ein Vektor. Ein Skalar, also ein einzelner Zahlenwert, wie zum Beispiel 42 existiert demnach in R nicht. Wenn man 42 in R in einer Variable speichert, hat man einen Vektor der Länge 1. Vektoren sind uns im Verlaufe des Buches schon oft begegnet. Jede Spalte innerhalb eines tibbles ist für sich genommen ein Vektor. Damit kommen wir direkt zur wichtigsten Besonderheit. Vektoren können immer nur einen Datentypen enthalten. Wenn eine Zahl mit einem Wort kombiniert wird, wird der ganze Werte zum Typ Character umgewandelt. Einen Vektor kann man auf verschiedene Art und Weise erstellen. Beispielsweise mit vector(), rep(), seq() oder c(). Die für uns wichtigste ist die bereits verwendete Combine Funktion c(). Alle mit einem Komma getrennten Argumente innerhalb von c() werden aneinander gebunden und als Vektor ausgegeben. c(1, 3, 2, 4) [1] 1 3 2 4 Auch bereits verwendet wurde der Doppelpunkt als Äquivalent für von Zahl 1 bis Zahl 2. Zahlen in geordneter Reihenfolge von 1 bis 4 währen demnach 1:4 [1] 1 2 3 4 Auf Elemente (hier Werte) innerhalb eines Vektors kann mithilfe von eckigen Klammern zugegriffen werden. Möchte man beispielsweise nur das dritte Element des Vektors c(1, 3, 2, 4) gespeichert in der Variable vec ausgeben lassen, würde man vec[3] [1] 2 schreiben. Da diese 3 im Prinzip auch nur ein Vektor der Länge 1 ist, kann diese Syntax auch mit c() und dem Doppelpunkt kombiniert werden, um mehrere Elemente ausgeben zu lassen. vec[c(1, 4)] vec[1:2] Probiere es ruhig selbst einmal aus und spiele ein wenig damit herum. 11.2 Matrix Wenn man nun mehrere Vektoren eines Datentyps aneinander bindet, erhält man eine Matrix. Für zeilenweises Binden der Vektoren verwendet mit rbind() (für row bind). Dabei müssen die Vektoren die selbe Länge haben. rbind( c(1, 3, 2, 4), 1:4 ) [,1] [,2] [,3] [,4] [1,] 1 3 2 4 [2,] 1 2 3 4 Es können im Übrigen auf diese Weise auch tibble zusammen gebunden werden. Das spaltenweise Äquivalent ist cbind() (für column bind). cbind( c(1, 3, 2, 4), 1:4 ) [,1] [,2] [1,] 1 1 [2,] 3 2 [3,] 2 3 [4,] 4 4 Seltener in der Datenanalyse benutzt, aber trotzdem manchmal nützlich, ist der matrix() Befehl. Als Argumente müssen der Vektor, die Anzahl der Zeilen oder Spalten sowie die Information übergeben werden, ob es zeilenweise (byrow) abgebildet werden soll. matrix( 1:9, ncol = 3, byrow = TRUE ) [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 Angenommen, die vorhergehende Matrix sei in der Variable mat gespeichert. Da nun zwei Dimensionen involviert sind, müssen zum Zugreifen auf Elemente innerhalb der Matrix auch zwei Parameter berücksichtigt werden: die Spalten- und Zeilenposition. Dabei werden innerhalb der eckigen Klammern getrennt von einem Komma zuerst die Zeilen und dann die Spalten angegeben. Möchte man den Wert aus Zeile 2 und Spalte 3 erhalten, würde man [2, 3] an den Variablennamen hängen. mat[2, 3] [1] 6 Wenn man eine ganze Zeile oder Spalte zurückgeben möchte, lässt man schlichtweg das auszulassende Argument weg. Für die erste Zeile schreibt man folglich: mat[1, ] [1] 1 2 3 Das Leerzeichen nach dem Komma ist zwar nicht zwingend notwendig, allerdings macht es deutlich, dass dort ein zweiter Wert fehlt. Auch das Zugreifen auf Elemente einer Matrix kann, wie bereits im Kontext der Vektoren in Kapitel 11.1 beschrieben, mit c() und dem Doppelpunkt auf mehr als eine Auswahl erweitert werden. Genau wie Vektoren können auch in Matrizen nur Daten von einem Typ gespeichert werden. Bei Vermischung von Datentypen werden automatisch die gleichen Umwandlungsregelungen angewandt. Außerdem müssen die Vektoren innerhalb der Matrix die selbe Länge haben. Aufgrund der Limitation, nur einen Datentyp enthalten zu können, fällt die Matrix als Datenstruktur in der gewöhnlichen Datenanalyse in der Regel aus. Deswegen bedarf es einer allgemeineren Datenstruktur, den data.frames und tibbles. 11.3 Data.frame und tibble Bevor wir auf die Datenstrukturen eingehen, sollte erst einmal geklärt werden, was es mit der Unterscheidung zwischen data.frame und tibble auf sich hat. Grundsätzlich ist der Zweck und Einsatz beider Strukturen fast identisch. Es gibt aber doch ein paar sehr nützlich Erweiterungen von tibbles gegenüber data.frames. Ein data.frame ist ein Datenformat, was in base R integriert ist. tibbles hingegen sind aus dem gleichnamigen tibble Package, welches im tidyverse enthalten ist. Der wohl praktischste Vorteil ist die übersichtlichere Ausgabe. Es werden nur 10 Zeilen ausgegeben, auf einen Blick sieht man die Datentypen der Spalten und die Dimensionen des Datensatzes. Außerdem sind die Zahlen zur besseren Übersichtlichkeit entsprechend eingerückt und negative Werte rot hervorgehoben. Das automatische Runden von tibbles bei der Anzeige ist hingegen nicht immer ein Vorteil. Während es beim explorativen Anschauen der Daten sehr praktisch ist, muss beim deskriptiven oder inferenzstatistischen Betrachten der Daten eine bestimmte Anzahl von Kommastellen sichtbar sein, um sie in einer wissenschaftlichen Arbeit einzusetzen. Grundsätzlich können Funktionen, die auf data.frames angewendet werden können, abgesehen von wenigen Ausnahmen auch auf tibbles angewendet werden. Im Folgenden werden wir nur noch von tibbles reden, da alles Gesagte auch für data.frames gilt. tibbles sind wie Matrizen mit dem Unterschied, dass jede Spalte einen anderen Datentyp beinhalten darf. Innerhalb jeder Spalte muss der Datentyp trotzdem gleich bleiben, da jede diese Spalten letztlich ein Vektor ist. Einen tibble selbst zu erstellen kommt in der gewöhnlichen Datenanalyse denkbar selten vor. Schließlich erhebt man die Daten und schreibt sie gewöhnlich in eine Form von Spreadsheets. Es ist eher umständlich, einen ganzen Datensatz manuell direkt in R einzutragen. Im Vergleich zum Erstellen von Matrizen mithilfe von rbind() oder cbind ändert sich im Grunde nicht viel. Die Funktion tibble() erstellt den tibble. Vor dem Gleichheitszeichen kann eine Name zugewiesen werden. Eine Besonderheit von tibbles ist die Möglichkeit, direkt im Erstellungsprozess für das Erstellen einer weiteren Spalte, eine vorher erstellte Spalte zu verwenden. tibble( a = 1:5, b = 6:10, sum = a + b ) # A tibble: 5 × 3 a b sum &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 6 7 2 2 7 9 3 3 8 11 4 4 9 13 # … with 1 more row Wichtig sind die verschiedenen Arten, auf einen tibble zuzugreifen. Den Großteil des Buches wurde die Helferfunktion select() aus dem tidyverse verwendet. Der Grund, weshalb wir andere Möglichkeiten kennenlernen müssen, ist der, dass select() immer einen tibble zurückgibt. Während das in den meisten Fällen das gewünschte Verhalten ist, gibt es einige Funktionen, denen man die Spalte als Vektor übergeben muss. Wir haben das beispielsweise bereits im Kontext von Kontingenztafeln kennengelernt, bei denen wir mit dem so genannten Dollar-Operator eine Spalte aus dem Datensatz herausgeholt haben. Die Syntax ist dabei allgemeingesprochen datensatzname$spaltenname. Angenommen, der zur erstellte tibble sei als tib gespeichert. Um die Spalte sum aus dem Datensatz als Vektor ausgeben zu lassen, würden wir [1] 7 9 11 13 15 schreiben. Letztlich ist der Dollar-Operator technisch gesehen ein Shortcut für datensatzname[[\"spaltenname\"]]. Im Kontext reiner Datenanalyse kann allerdings guten Gewissens die kurze Version mit dem Dollar-Zeichen verwendet werden. Neben dieser beiden Möglichkeiten zur Ausgabe einer einzelnen Spalte kann ebenfalls wie bei Matrizen mit einfachen eckigen Klammern auf ganze Zeilen zugegriffen werden. Hier gibt es einen weiteren Unterschied zwischen data.frames und tibbles. Während die Standardeinstellung von data.frames dafür sorgt, dass bei Auswahl nur einer Spalte (zum Beispiel tib[ ,1]) ein Vektor zurückgegeben wird, gibt ein tibble immer auch einen tibble zurück. Eine weitere Besonderheit im Vergleich zu Matrizen (sofern deren Spalten unbenannt sind) besteht darin, dass innerhalb der eckigen Klammern auch die Spaltennamen als Character übergeben werden können. tib[ ,c(&quot;a&quot;, &quot;b&quot;)] # A tibble: 5 × 2 a b &lt;int&gt; &lt;int&gt; 1 1 6 2 2 7 3 3 8 4 4 9 # … with 1 more row Allerdings bringt uns das in dem Fall keinerlei Vorteil zum bereits kennengelernten select(). 11.4 Liste Listen sind der allgemeinste Datentyp. Tatsächlich sind data.frames nur eine besondere Art von Listen. Etwas kontraintuitiv resultiert daraus, dass Listen am wenigsten Zeit bei Berechnungen benötigen. Deswegen wird eine listenorientierte Programmierung mithilfe von Variationen von map() in Kapitel 12.2 eingeführt. Ein Listenelement kann jede Datenstruktur enthalten – sogar ganze tibbles. Beim Erstellen ändert sich der Befehl zu list(). list( Vektor = vec, Matrix = mat, Tibble = tib ) $Vektor [1] 1 3 2 4 $Matrix [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 $Tibble # A tibble: 5 × 3 a b sum &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 6 7 2 2 7 9 3 3 8 11 4 4 9 13 # … with 1 more row Die Zeichen vor dem Gleichheitszeichen sind dabei die Namen der Listenelemente, die man zum Abrufen verwenden kann. Mit Listen haben wir nun bis zu drei Dimensionen. Die verschiedenen Elemente innerhalb der Liste, die wiederum zwei-dimensionale tibbles enthalten können. Es ist sogar möglich, Listen innerhalb von Listen zu haben. Im Rahmen des Einnistens greifen wir den Gedanken in Kapitel 12.3 wieder auf. Das Prinzip beim Zugreifen ändert sich nicht. Nur die Anzahl der Dimensionen steigt. Würde man also auf den tibble der eben erstellen Liste ls zugreifen wollen, könnte man ls$Tibble oder ls[[3]] schreiben. Möchte man direkt auf Elemente innerhalb des tibbles zugreifen, kann man auf übliche Weise darauf zugreifen. ls[[3]][1 ,2] # A tibble: 1 × 1 b &lt;int&gt; 1 6 Eine der Liste sehr ähnliche Struktur ist der Array, auf den aufgrund seltenen Nutzens im Kontext der Datenanalyse an dieser Stelle verzichtet sei. Eine besondere Art der Liste ist der tibble. Daher können wir grundsätzlich in eine Zelle nicht nur Zahlen oder Buchstaben hineinschreiben, sondern sogar ganze andere Datensätze darin verstecken. df &lt;- tibble( a = c(1, 2, 3), b = list( tibble(a = c(1, 2, 3, 4), b = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;)), tibble(x = 4:5, y = 6:7), Number = 1 ) ) df # A tibble: 3 × 2 a b &lt;dbl&gt; &lt;named list&gt; 1 1 &lt;tibble [4 × 2]&gt; 2 2 &lt;tibble [2 × 2]&gt; 3 3 &lt;dbl [1]&gt; Wie du siehst sind in der Spalte b nun in den ersten zwei Zeilen tibbles enthalten. Wenn wir mit df$b oder df[[2]] nur diese Spalte anschauen, sehen wir eine Liste als Ausgabe. df[[2]] [[1]] # A tibble: 4 × 2 a b &lt;dbl&gt; &lt;chr&gt; 1 1 m 2 2 f 3 3 f 4 4 m [[2]] # A tibble: 2 × 2 x y &lt;int&gt; &lt;int&gt; 1 4 6 2 5 7 $Number [1] 1 11.5 Umwandlungen Sofern die Voraussetzungen erfüllt sind, können Datenstrukturen ineinander überführt werden. Dabei haben die Funktionen immer das Präfix as. und beim tibbles ausnahmsweise as_. as.numeric() as.character() as.vector() as.matrix() as.data.frame() as_tibble() Besonders nützlich ist hierbei as.numeric() zum Umwandeln von fälschlicherweise erstellen Character Spalten und as.data.frame(), um einen tibble in einen data.frame umzuwandeln, falls die angezeigten Rundungen zu ungenau sind. Beachte, dass as_tibble() das vorherige Laden des Packages tibble oder gleich des gesamten tidyverse benötigt. 11.6 Objektorientierung Für dieses Kapitel muss das sloop Package installiert und geladen werden. library(sloop) Grundsätzlich basieren die meisten bisher kennengelernten Datenstrukturen auf dem sogenannten S3 System der Objektorientierung. Das ist für den normalen R Nutzer völlig irrelevant, allerdings gibt es noch die anderen beiden Systeme namens S4 und R6. Diese können wir nicht ohne weiteres mit denen in diesem Buch kennengelernten Funktion verwenden. Ein populäres Beispiel dafür ist die Seite Bioconductor (Alternative zu CRAN), welche diverse Packages mit biologischen Fokus zur Verfügung stellt. Dort haben beinahe alle Packages das S4 System als zugrunde liegen. Dadurch kann beispielsweise nicht mehr mit dem Dollar-Operator auf Spalten zugegriffen werden. Stattdessen würde man das @ Zeichen verwenden. ls@Tibbel Dies sei an dieser Stelle nur deshalb beschrieben, da S4 und R6 Systeme zu seltsamen Fehlermeldungen führen können, wenn man versucht, wie gewohnt damit zu arbeiten. Mit der Funktion otype() aus dem sloop Package kann der Objekttyp herausgefunden werden. otype(big_five) [1] &quot;S3&quot; Wer sich tiefer mit Objektorierentierung in R beschäftigen möchte, sollte einen Blick in Advanced R von Hadley Wickham werfen. "],["iterationmain.html", "Kapitel 12 Iterationen 12.1 Was sind iterative Prozesse? 12.2 Listenbasierte Berechnungen 12.3 Einnisten", " Kapitel 12 Iterationen 12.1 Was sind iterative Prozesse? Wenn wir wiederholt etwas sehr ähnlich machen möchten, wäre die wohl offensichtlichste Möglichkeit Copy &amp; Paste. Man nimmt also den Code für den einen Anwendungsfall und modifiziert ihn leicht für andere Anwendungsfälle. Das große Problem dabei ist die Fehleranfälligkeit. Eine sehr gute Lösung dafür sind iterativ Prozesse. Solange eine Bedingung zutritt, soll ein bestimmter Befehl ausgeführt werden und dabei Kleinigkeiten der Reihe nach anpassen. Das wäre die Beschreibung eines sehr grundlegenden programmatischen Prinzips, den sogenannten Schleifen (z.B. for-Schleife und while-Schleife). Diese werden wir uns innerhalb der folgenden Kapitel allerdings nicht anschauen. Dies hängt von zwei Tatsachen ab: 1. Das Schreiben von fehlerfreien Schleifen ist bei fortgeschritteneren Fällen schwer und 2. ist es schwer, effiziente Schleifen in R zu schreiben, die nicht ewig für ihre Berechnungen brauchen. Im Kontext der Datenanalyse benötigen wir das allerdings auch gar nicht. Wir können mit map() über verschiedene Listenelemente die selbe Funktion anwenden (siehe Kapitel 12.2). Mit nest() kann dies auch direkt innerhalb eines tibbles gemacht werden (siehe Kapitel 12.3). Schauen wir uns einmal Schritt für Schritt die Problematik an. Wenn wir drei separate Regressionsmodelle für den Einfluss von Geschlecht auf Extraversion für jede Altersgruppe machen möchten, könnten wir den Code jeweils kopieren. Wir filtern also zuerst unseren Datensatz. mod1 &lt;- big5_mod %&gt;% filter(Gruppe == &quot;Jung&quot;) mod2 &lt;- big5_mod %&gt;% filter(Gruppe == &quot;Mittel&quot;) mod3 &lt;- big5_mod %&gt;% filter(Gruppe == &quot;Weise&quot;) Anschließend werden die drei Regressionsmodelle berechnet. model1 &lt;- lm(Extraversion ~ Geschlecht, data = mod1) model2 &lt;- lm(Extraversion ~ Geschlecht, data = mod2) model3 &lt;- lm(Extraversion ~ Geschlecht, data = mod3) Die Berechnung ist falsch, sobald wir eine Zahl in den Variablennamen vergessen haben, entsprechend anzupassen. Würden wir 20 oder 30 verschiedene Modelle auf diese Art und Weise rechnen wollen, können wir uns fast sicher sein, dass sich irgendwo ein Fehler einschleicht. 12.2 Listenbasierte Berechnungen Das purrr Package aus dem tidyverse enthält diverse Funktionen für iterative Prozesse. Wir werden uns im Folgenden map() und map_df() genauer anschauen. Dafür müssen wir die verschiedenen Datensätze in eine Liste packen. Pro Listenelement ein Datensatz auf den wir unsere Funktion (hier das Regressionsmodell) anwenden wollen. mod_ls &lt;- list(mod1, mod2, mod3) mod_ls [[1]] # A tibble: 147 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 30 f 3.1 3.4 Jung 2 2 23 m 3.4 2.4 Jung 3 3 24 f 3 2.8 Jung 5 4 14 f 2.8 3.5 Jung 6 # … with 143 more rows [[2]] # A tibble: 39 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 32 m 3.5 3.1 Mittel 7 3 42 m 2.9 1.7 Mittel 20 4 34 m 3.5 2.2 Mittel 21 # … with 35 more rows [[3]] # A tibble: 14 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 54 m 3.3 4.2 Weise 4 2 56 f 3.2 2.3 Weise 17 3 59 m 2.7 2.3 Weise 29 4 56 f 2.7 2.3 Weise 55 # … with 10 more rows Der erste Schritt ist die Berechnung der Regressionsmodelle mit einer Lambda Funktion innerhalb von map(). Als Ergebnis erhalten wir ebenfalls eine Liste mit einem Modell pro Listenelement. Um das ganze angenehmer ausgeben zu lassen, fügen wir als zweiten Schritt noch map_df() (für map data frame) hinzu. Wie der Name suggeriert, werden (falls durch die Datenstruktur möglich) die Listenelemente als tibble/data.frame ausgegeben. Dies erreichen wir wieder mit der Funktion tidy aus dem broom Package. mod_ls %&gt;% map(~ lm(Extraversion ~ Geschlecht, data = .x)) %&gt;% map_df(tidy) # A tibble: 6 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3.07 0.0376 81.6 4.49e-123 2 Geschlechtm 0.0550 0.0598 0.919 3.60e- 1 3 (Intercept) 3.07 0.0649 47.3 1.08e- 34 4 Geschlechtm 0.0563 0.0930 0.605 5.49e- 1 # … with 2 more rows Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). 12.3 Einnisten Durch die Funktion nest() können innerhalb von Zellen eines tibbles Datenstrukturen jeder Art eingenistet werden. Dies wurde bereits in Kapitel 11.4 anhand eines einfachen Beispiels eingeführt. Nun wenden wir das Ganze auf unseren big5_mod Datensatz an. Dafür gruppieren wir wie gewohnt mithilfe von group_by(). Anschließend muss nur noch die Funktion nest() ohne Argumente ausgeführt werden. big5_mod %&gt;% group_by(Gruppe) %&gt;% nest() # A tibble: 3 × 2 # Groups: Gruppe [3] Gruppe data &lt;chr&gt; &lt;list&gt; 1 Mittel &lt;tibble [39 × 5]&gt; 2 Jung &lt;tibble [147 × 5]&gt; 3 Weise &lt;tibble [14 × 5]&gt; Als Ergebnis kriegen wir einen tibble, welcher in der ersten Spalte die Altersgruppen abgebildet hat. Daneben gibt es eine neue zweite Spalte namens data, in der wiederum drei tibbles mit den Dimensionen 39x4, 147x4 und 14x4. Jetzt können wir ähnlich wie bei den listenbasierten Berechnungen im vorherigen Kapitel mit map() eine Funktion auf jeden dieser eingenisteten Datensätze anwenden. Der Unterschied besteht darin, dass wir den Befehl innerhalb von mutate() ausführen müssen. Schließlich haben wir einen tibble vorliegen. Wenn wir dort eine neue Spalte erstellen möchten, benötigen wir dafür mutate(). Im zweiten Schritt nehmen wir die Modelle und geben diese in einem aufgeräumten Format mit tidy() wieder aus. big5_mod %&gt;% group_by(Gruppe) %&gt;% nest() %&gt;% mutate( Modelle = map(data, ~ lm(Extraversion ~ Geschlecht, data = .x)), Ergebnisse = map(Modelle, tidy) ) # A tibble: 3 × 4 # Groups: Gruppe [3] Gruppe data Modelle Ergebnisse &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 Mittel &lt;tibble [39 × 5]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; 2 Jung &lt;tibble [147 × 5]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; 3 Weise &lt;tibble [14 × 5]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; Nun sehen wir eine neue Spalte namens Modelle mit Einträgen des Datentyps &lt;lm&gt; (unsere linearen Modelle). Gleich daneben sind unsere Ergebnisse (p-Werte und Co.) wieder als tibble eingenistet. Damit wir nun an diese Ergebnisse herankommen, müssen wir abschließend lediglich diese mit unnest() aus der eingenisteten Struktur herausholen. big5_mod %&gt;% group_by(Gruppe) %&gt;% nest() %&gt;% mutate( Modelle = map(data, ~ lm(Extraversion ~ Geschlecht, data = .x)), Ergebnisse = map(Modelle, tidy) ) %&gt;% unnest(Ergebnisse) %&gt;% select(Gruppe, term, estimate, p.value) # A tibble: 6 × 4 # Groups: Gruppe [3] Gruppe term estimate p.value &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Mittel (Intercept) 3.07 1.08e- 34 2 Mittel Geschlechtm 0.0563 5.49e- 1 3 Jung (Intercept) 3.07 4.49e-123 4 Jung Geschlechtm 0.0550 3.60e- 1 # … with 2 more rows Der besseren übersichtshalber haben wir nur die Spalten Gruppe, term, estimate und p.value ausgeben lassen. Das ist ein Platzhalter für eine Übung. Starte die Übung mit hands_on(\"test\"). "],["appendix.html", "Appendix Datensatzerläuterungen Literaturverzeichnis Verwendete Softwareversionen", " Appendix Datensatzerläuterungen Big Five Indonesisch Heartoprolol Bitcoin Statistical Literacy Eye Tracking Video Artists Tracks Fitness Tipp WM Literaturverzeichnis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
