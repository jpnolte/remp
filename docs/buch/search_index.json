[["index.html", "R für empirische Wissenschaften Vorwort Lizenz", " R für empirische Wissenschaften Jan Philipp Nolte 2023-05-13 Vorwort Willkommen zur Internetseite des Buches R für empirische Wissenschaften. Hier wirst du ohne Vorwissen lernen, die Programmiersprache R zur Datenanalyse anzuwenden. Die Zielgruppen sind WissenschaftlerInnen und StudentInnen der Fachrichtungen Medizin, Psychologie, Betriebswirtschaftslehre, Wirtschaftswissenschaften, soziale Arbeit, Pharmazie, Agrarwissenschaften, Neurowissenschaften, Biologie, Journalismus, Tourismus, Data Science, Biostatistik und allen weiteren Disziplinen, die Daten auswerten möchten. Ergänzend wird das remp Package bereitgestellt, welches Datensätze, Übungen und praktische Funktionen beinhaltet. In der Navigationsleiste auf der linken Seite können zwölf Kapitel ausgewählt werden. Alternativ kannst du auch mithilfe der linken und rechten Pfeiltaste zwischen den Hauptkapiteln wechseln. Oben in der Leiste ruft die Lupe die Suchfunktion auf, während man über das A die Schriftgröße, Schriftart und das Farbthema anpassen kann. Lizenz Veröffentlicht ist das gesamte Buch unter der Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Lizenz. "],["intro.html", "Kapitel 1 Einleitung 1.1 Für wen ist dieses Buch? 1.2 Aufbau und Bearbeitungsstrategie 1.3 Boxen, Übungen und Datensätze 1.4 Ergänzende Literatur", " Kapitel 1 Einleitung In diesem Kapitel wird die allgemeine Herangehensweise an die verschiedenen Themen dieses Buches erläutert. Außerdem werden hilfreiche Bücher zur Vertiefung und zum Erlangen des Statistikverständnisses vorgestellt. 1.1 Für wen ist dieses Buch? Das Buch ist grundsätzlich für jeden geeignet, der R lernen möchte. Ein Vorwissen über Programmiersprachen wird nicht vorausgesetzt. Neben der Grundlagen von R wirst du nach Lesen dieses Buches einen Datensatz einlesen und bereinigen können. Du wirst verstehen, wie man publikationsreife Visualisierungen und Tabellen erstellt. Auch wirst du deskriptive Maße und die üblichsten statistischen Hypothesentests berechnen können. Die statistischen Verfahren werden nicht separat eingeführt, da es dafür bereits ausführliche Lehrbücher gibt. Dieses Buch konzentriert sich folglich auf die computergestützte Datenauswertung und nicht auf die zugrundeliegende Statistik. 1.2 Aufbau und Bearbeitungsstrategie Tatsächlich sind die Kapitel in der Reihenfolge aufgebaut, wie man normalerweise mit einem frisch erhobenen Datensatz umgeht. Nachdem alles richtig eingerichtet und aufgesetzt ist (Teil I), werden die Daten bereinigt und aufbereitet (Teil II), bis die Fragestellungen mithilfe statistischer Analysen beantwortet werden (Teil III). Vertiefende Konzepte für eine fortgeschrittene Verwendung runden den Inhalt des Buches schließlich ab (Teil IV). Teil I: Die ersten Schritte. Kapitel eins und zwei bilden den ersten Teil, der auch von Lesern, die bereits mit R gearbeitet haben, gründlich durchgelesen werden sollte. Vor allem die Installation bereitet häufig schon die ersten großen Probleme. Auch werden verschiedene Hilfestellungen eingeführt. Teil II: Vorbereitung. Die meiste Zeit in der Datenanalyse wird von der Datenvorbereitung beansprucht. Die eigentliche Auswertung geht anschließend meistens vergleichsweise schnell. Daher ist die Datenvorbereitung auch eines der ausführlichsten Kapitel dieses Buches. Außerdem werden die essentiellen R Projektdateien sowie einige notwendige Grundlagen erläutert. Darüber hinaus wird das Einlesen von Datensätzen verschiedener Dateienarten erklärt. Teil III: Auswertung. Wenn der Datensatz endlich fertig aufbereitet ist, können Abbildungen erstellt sowie deskriptive Statistiken und inferenzstatistische Hypothesentests berechnet werden. Die Visualisierungen und in Tabellen dargestellten Ergebnisse werden dabei direkt publikationsreif ausgegeben. Teil IV: Vertiefung. Hier werden weiterführende und vertiefende Konzepte vorgestellt, die nicht zwingend für die eigentliche Datenanalyse benötigt werden. Es wird erklärt, wie man Tabellen oder ganze Berichte in Word oder PDF umwandeln kann. Die verschiedenen Datenstrukturen werden verglichen und abschließend fortgeschrittenere Programmiertechniken vorgestellt. Jeder hat einen individuellen Lernstil und liest ein Lehrbuch auf unterschiedliche Art und Weise. Die einzelnen Kapitel des Buches bauen zwar grundsätzlich aufeinander auf, allerdings wurde gleichzeitig darauf geachtet, die Kapitel zum schnellen Nachschlagen möglichst in sich geschlossen zu halten. Wer also nicht das gesamte Buch Schritt für Schritt durcharbeiten möchte, sollte aber zumindest nachfolgende Kapitel gelesen haben. Dies gilt auch für jene, die nur an einem ganz bestimmten statistischen Test für die Bachelorarbeit interessiert sind. Zeitmangel? Man sollte sich mindestens mit den Kapiteln 2, 3, 4, 5 und 6.1 vertraut machen, da diese essentiell zum Arbeiten mit R sind. 1.3 Boxen, Übungen und Datensätze Es gibt drei Arten von Boxen mit jeweils unterschiedlicher Farbe und Symbol. Die mit der Glühbirne markierten Boxen fassen besonders wichtige Konzepte zusammen, die mit dem Warnzeichnen weisen auf häufige Probleme hin und die mit dem Laptop enthalten interaktive Übungen. Nach Durcharbeiten dieses Buches kann jeder und jede selbstständig Fragestellungen verschiedener wissenschaftlicher Fachrichtungen anhand eigener Datensätze beantworten. Nach dem Starten von R sollte immer erst ein Projekt erstellt werden, bevor die Datensätze durch Befehle innerhalb eines R Skripts eingelesen werden. Übung 6.2. In dieser Übung lernst du das Auswählen und Umbenennen von Spalten eines Datensatzes. Starte die Übung mit uebung_starten(6.2). Die Funktion uebung_starten() öffnet zunächst ein Fenster innerhalb von RStudio, in welchem du dann die Übungen bearbeiten kannst. Dafür wird kein Internet benötigt. Alternativ stehen die Übungsaufgaben und Lösungen auch auf der Internetseite zum Buch bereit (https://r-empirische-wissenschaften.de). Das zugehörige Übungsverzeichnis findest ebenfalls auf der Website oder durch Ausführen der Funktion uebung_anzeigen(). Im Laufe des Buches werden aus didaktischen Gründen verschiedene Datensätze verwendet, die ebenfalls im Appendix genau vorgestellt werden. Zum Anwenden auf dem heimischen Computer oder Laptop kannst du entweder händisch den im Buch beschrieben Code abtippen oder du verwendest den Copy to Clipboard Button, der online oben rechts im Codeblock erscheint. Dieser macht genau dasselbe, als würdest du den Befehl markieren und mit Strg + c oder Rechtsklick + Kopieren kopieren. 1.4 Ergänzende Literatur Statistik: Der Weg zur Datenanalyse. Ein verständliches, deutschsprachiges Buch als Einleitung für die wichtigsten Konzepte der Statistik. Der Fokus liegt tendenziell eher auf Sozial- und WirtschaftswissenschaftlerInnen, allerdings eignet sich das Buch generell als Einführung in die Statistik. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Dieses Buch ist etwas fortgeschrittener als das zuvor genannte, wobei hier alle in diesem Buch verwendeten statistischen Modelle ausführlich erklärt werden. ggplot2: Elegant Graphics for Data Analysis (3rd Edition). In R für empirische Wissenschaften wird ein sehr umfangreiches Erweiterungspaket namens ggplo2 zum Erstellen der Visualisierungen verwendet. Während es hier um konkrete Anwendungen geht, werden im vorgeschlagenen Buch die technischen Hintergründe erläutert. Diese sind vor allem interessant, wenn man darauf basierende Erweiterungen entweder verwenden oder selbst erstellen möchte. Das Buch kann kostenlos unter https://ggplot2-book.org/ eingesehen werden. R Markdown: The Definite Guide. Dieses Buch erläutert die in Kapitel 10.2 eingeführten Exportierungsmöglichkeiten mithilfe von R Markdown deutlich ausführlicher. Denn mit R Markdown kann man nicht nur Tabellen und kurze Berichte, sondern auch Bücher, Internetseiten und Abschlussarbeiten erstellen. Es ist kostenlos auf der Seite https://bookdown.org/yihui/rmarkdown/ lesbar. Advanced R (2nd Edition). Hier werden tiefe Einblicke in R als Programmiersprache als solche gegeben. Wenn man seine Programmierfähigkeit mit R ausbauen möchte, kommt man an diesem Buch nicht vorbei. Auch hier kann das Buch im Internet kostenfrei gelesen werden (https://adv-r.hadley.nz/). "],["start.html", "Kapitel 2 Startvoraussetzungen 2.1 Installation von R und RStudio 2.2 Aufbau von RStudio 2.3 RStudio anpassen 2.4 Funktionen und ihre Argumente 2.5 Packages (Erweiterungen) 2.6 Fehler- und Warnmeldungen", " Kapitel 2 Startvoraussetzungen Die erste große Hürde beim Programmieren ist häufig die Einrichtung der notwendigen Programme auf dem eigenen Computer. Auch wenn im Falle von R die Installation vergleichsweise einfach funktioniert, haben doch viele AnfängerInnen Probleme damit. Schritt für Schritt wird außerdem erklärt, was Funktionen sind und wie man mit deren Hilfe Erweiterungen installieren und laden kann. Abschließend wird das Beheben von Fehlern besprochen. 2.1 Installation von R und RStudio Die Unterscheidung zwischen R und RStudio ist für viele Anfänger verwirrend. Entgegen des ersten Eindrucks, handelt es sich dabei nicht um zwei austauschbare Alternativen. Stattdessen stellt R die Programmiersprache und RStudio eine Programmierumgebung dar. Den Unterschied können wir uns am Zusammenspiels eines Motors in einer Karosserie vor Augen führen, welcher in Abbildung 2.1 illustriert ist. Man braucht die Programmiersprache R, damit überhaupt etwas voran geht. Dabei könnte man das Auto grundsätzlich auch mit einem spartanischen Stahlgerüst fahren. Abbildung 2.1: Illustration des Unterschieds zwischen R und RStudio. Die Programmierumgebung RStudio macht aus dem minimalistischen Stahlgerüst mit dem Motor R eine komfortable Luxuslimousine mit Navigationssystem und Sitzheizung. Man kann R folglich auch ohne RStudio benutzen, aber RStudio nicht ohne R. Sonderlich viel Spaß bereitet das allerdings nicht. RStudio bietet eine Vielzahl von großartigen und praktischen Features, weshalb wir im Laufe des Buches nur innerhalb von RStudio arbeiten werden. Es wird aber trotzdem häufig von R die Rede sein, da RStudio lediglich die verwendete Programmierumgebung ist. In den folgenden Kapiteln wird Schritt für Schritt erklärt, wie du R und RStudio auf den gängigsten Betriebssystemen installierst und wichtige Anpassungen innerhalb von RStudio vornimmst. Auch wenn sich RStudio bereits auf deinem Computer oder Laptop befindet, solltest du Kapitel 2.3 unbedingt lesen. 2.1.1 Programmiersprache R Zum Bearbeiten der Übungen dieses Buches benötigst du die R Version 4.1.0 oder neuer. Falls beim späteren Installieren der Packages (siehe Kapitel 2.5) ein Fehler auftritt, liegt das aller Wahrscheinlichkeit an einer zu alten R Version. Am besten installierst du R, genau wie es hier beschrieben wird, neu. Bei der Installation gibt es Unterschiede zwischen den verschiedenen Betriebssystemen Windows, macOS (Apple) und Ubuntu (Linux). In der späteren Benutzung gibt es hingegen keine Unterschiede. Du musst dir also nur den jeweiligen Abschnitt für dein Betriebssystem anschauen. Nach der Installation musst du mit R nichts weiter machen und kannst sofort zum Herunterladen von RStudio hinübergehen. Windows: Geh auf cloud.r-project.org und wähle Download R for Windows Klicke anschließend auf den Link base. Drücke dann auf Download R for Windows und achte darauf, wohin du die Installationsdatei abspeicherst. Führe die Installationsdatei (z.B. R-4.1.0-win.exe) mit einem Doppelklick aus. Folge schließlich den Installationsanweisungen. Hierbei ist das Entfernen des Häkchen bei Message translations zwingend erforderlich (siehe Abbildung 2.2). Ansonsten muss nichts an den Standardeinstellungen der Installation geändert werden. Nach erfolgreicher Installation kannst du die heruntergeladene Installationsdatei (z.B. R-4.1.0-win.exe) wieder löschen. Abbildung 2.2: Richtige Installation von R ohne Sprachsupport macOS (Apple) : Gehe auf cloud.r-project.org und wähle Download R for (Mac) OS X. Je nachdem wie alt dein Mac ist, muss eine andere Version von R heruntergeladen werden. Deine macOS Version darf zur Benutzung der in diesem Buch vorgestellten Methoden nicht älter als 10.9 Mavericks sein. macOS 10.13 High Sierra und neuer: R-4.1.0.pkg (beziehungsweise die derzeit aktuellste Version) macOS 10.11 El Capitan und neuer: R-3.6.3.pkg macOS 10.9 Mavericks und neuer: R-3.3.3.pkg Achte auf den Speicherort der Installationsdatei und führe diese aus. Falls deine Einstellungen die Installation externer Programme verhindern, musst du das Installieren von externen Paketen (mit der Endung .pkg) explizit erlauben (siehe unten). Bei der Installation kann, ohne was zu verändern, stets auf weiter gedrückt werden. Nach fertiger Installation kann die Installationsdatei (z.B. R-4.1.0.pkg) gelöscht werden. Für englische Fehlermeldungen muss schließlich noch folgender Befehl innerhalb von R ausgeführt werden. Danach muss R geschlossen und erneut geöffnet werden. Alternativ kann man auch zuerst RStudio installieren und den Befehl dort entsprechend eingeben. system(&quot;defaults write org.R-project.R force.LANG en_US.UTF-8&quot;) Zum Schluss müssen wir noch manuell XQuartz (X11) installieren, da dies in neueren macOS Versionen nicht mehr standardmäßig enthalten ist. Dies brauchen wir später, um Datensätze auch innerhalb von RStudio anschauen zu können. Gehe dafür auf xquartz.org, lade die Installationsdatei (z.B. XQuartz-2.8.2.dmg) herunter und führe diese mit Doppelklick aus. Je nach Einstellungen des Betriebssystems kann die Installation externer Software (nicht von Apple zertifiziert) wie R aus Sicherheitsgründen blockiert werden. Um das zu verhindern, musst du mit dem Finder (nicht Launchpad) nach der Installationsdatei (z.B. R-4.1.0.pkg) suchen. Halte anschließend Ctrl gedrückt und klicke auf die Datei. Aus dem erscheinenden Kontextmenü, kann schließlich Öffnen ausgewählt werden. Ubuntu (Linux): Drücke die Tastenkombination Strg + Alt + T oder tippe Terminal in die Suchleiste ein, um den Terminal zu öffnen. Bringe die Informationen deiner Repositories auf den neuesten Stand und installiere ein notwendiges Paket. sudo apt update -qq sudo apt install --no-install-recommends software-properties-common dirmngr Füge nun den Key hinzu, um für jetzt und zukünftige Updates einen sicheren Download zu gewährleisten. wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | \\ sudo tee -a /etc/apt/trusted.gpg.d/cran\\_ubuntu\\_key.asc Das entsprechende Repository kann anschließend mit folgendem Befehl hinzugefügt werden. Bei Wechsel der Hauptversion (z.B. von 4.0.0 auf 5.0.0) muss die Endung cran40 auf cran50 geändert werden. sudo add-apt-repository &quot;deb https://cloud.r-project.org/bin/linux/ubuntu \\ $(lsb_release -cs)-cran40/&quot; Zum Installieren von R führe nun folgenden Befehl aus: sudo apt install --no-install-recommends r-base Zum Einstellen englischer Fehlermeldungen muss LANGUAGE=en an beliebiger Stelle in Renviron.site kopiert und gespeichert werden. Öffne die Datei dazu mit einem Texteditor wie gedit. Vergiss dabei nicht, die Datei im Anschluss abzuspeichern. sudo gedit /etc/R/Renviron.site 2.1.2 Programmierumgebung RStudio Nachdem R auf deinem Computer oder Laptop eingerichtet ist, kannst du die Installationsdatei für RStudio unter posit.co/download/rstudio-desktop/ herunterladen. Scrolle dafür hinunter und suche bei All Installers and Tarballs die richtige Datei aus. Wie beim Installieren von R musst du die heruntergeladene Installationsdatei mit der für dein Betriebssystem richtigen Endung (.exe, .pkg, .deb) ausführen und den anschließend eingeblendeten Anweisungen folgen. Nach Installation des Programms, kannst du die Installationsdatei auch in diesem Fall wieder löschen. Das eigentliche Programm RStudio findest du nun auf dem Computer zum Beispiel beim Drücken der Windows Taste und Eingabe des Wortes RStudio oder bei macOS über den Finder. Von jetzt an solltest du zum Arbeiten mit R immer RStudio öffnen und nicht R. Die extra vorhandene minimalistische Oberfläche von R kannst du indes ignorieren. Es ist bloß wichtig, es auf dem Computer installiert zu haben. 2.2 Aufbau von RStudio Wir schauen uns nun die einzelnen Komponenten innerhalb von RStudio an. Die Oberfläche ist dabei in vier Bereiche unterteilt: Nach frischer Installation ist die Console links unten, das R Skript als Source links oben, Environment &amp; History rechts oben und Plots, Hilfe, Packages und mehr unten rechts. Diese Reihenfolge kann beliebig nach eigener Präferenz verändert werden (siehe Kapitel 2.3). Abbildung 2.3: Aufbau von R Studio mit Skript (oben links), Konsole (unten links), Environment (oben rechts) und u.a. Plots und Help (unten rechts). Console. In der Konsole befindet sich im Prinzip die reine Programmiersprache R. Wir können also jegliche Befehle direkt in die Konsole eingeben, auf Enter Enter drücken und das Ergebnis erhalten. Im Anschluss ist der eingegebene Befehl jedoch weg. Mit der oberen und unteren Pfeiltaste kannst du die in der bisherigen Sitzung bereits ausgeführten Befehle durchgehen. Da das weder sonderlich praktisch noch reproduzierbar ist, sollte man so genannte Skripte verwenden, die in einer gesonderten Datei (mit der Endung .R) gespeichert werden und nach Öffnen immer wieder ausführbar sind. Source. Beim initialen Starten von R wird kein Skript angezeigt. Die Konsole nimmt also zuerst die gesamte linke Seite ein. Zum Erstellen eines R Skriptes kann man entweder die Tastenkombination Strg + Shift + N in Windows und Linux oder Cmd + Shift + N in macOS verwenden oder auf das unterhalb des Reiters File gelegene Blatt Papier mit dem Pluszeichen klicken und dort R Script auswählen. Gespeichert wird das R Skript wie üblich mit Strg + S oder über das Menü. Die Endung des Skriptes muss dabei .R sein, da die darin enthaltenen Befehle sonst nicht ausgeführt werden können. Man kann die Befehle innerhalb des Skriptes nun mit Strg / Cmd + Enter ausführen. Möchte man mehrere Zeilen ausführen, müssen diese erst markiert werden. Beispielsweise könnte man das gesamte R Skript ausführen, indem man erst mit Strg / Cmd + A alles markiert und anschließend mit Strg / Cmd + Enter bestätigt. Man kann mit der Tastenkombination Strg / Cmd + 1 und Strg / Cmd + 2 mit der Tastatur zwischen Konsole und Skript wechseln. Environment &amp; History. In der Environment werden alle gespeicherten Variablen angezeigt (siehe Kapitel 4.2). Man kann dort auch auf die eingelesenen Datensätze klicken und sich diese innerhalb von RStudio in einem eigenen Reiter ansehen. Vorsicht sei hier bei großen Datensätzen geboten, da diese Ansicht relativ rechenintensiv ist und RStudio dadurch abstürzen kann. Es empfiehlt sich daher, große Datensätze in ein anderes Format wie CSV oder Excel umzuwandeln, um dort einen guten Überblick über den Datensatz zu erhalten (siehe Kapitel 5). Die History zeigt alles an, was in der Konsole innerhalb einer R Sitzung ausgeführt wurde. Wenn du vollständig mit Skripten arbeitest, kannst du die History ignorieren. Tatsächlich solltest du das automatische Speichern und Laden der History, wie in Kapitel 2.3 erklärt, sogar ausschalten. Schließlich haben wir sämtliche durchgeführten Berechnungen bereits reproduzierbar durch das R Skript gegeben. Plots, Help &amp; Packages. Plots zeigt die erstellten Visualisierungen an (siehe Kapitel 8). Dabei kann man unter Zoom ein eigenes Fenster mit der Abbildung öffnen. Die Abbildung verändert sich, wenn man die Länge oder Breite des Fensters entsprechend verschiebt. Grundsätzlich kann mit Export die Visualisierung direkt in der Form gespeichert werden. Darauf sollte allerdings aufgrund der Auflösungsunterschiede verzichtet werden (siehe Kapitel 8.12). Unter Help wird die Dokumentation der verschiedenen R Funktionen angezeigt (siehe Kapitel 2.6). Der Reiter Packages ist auch nicht weiter von Bedeutung, da wir Packages im Rahmen dieses Buches über die Konsole installieren und laden. 2.3 RStudio anpassen Die ersten zwei in diesem Kapitel erklärten Anpassungen sind essentielle und unabdingbare Voraussetzungen, um das erfolgreiche Ausführen des Codes auch auf anderen Computern zu gewährleisten. Diese Einstellungen nicht vorgenommen zu haben, ist eine häufige und schwer zu findende Fehlerquelle. Die anderen Anpassungen stellen Empfehlungen dar, die dir das Programmieren erleichtern sollen. Niemals den Workspace und die History speichern. Um zu gewährleisten, dass dein Code nicht nur auf deinem Computer funktioniert, ist es dringend notwendig, das ständige Speichern und Laden des Workspaces auszustellen. Auch gewährleistest du dadurch, dass der Code immer funktioniert, wenn du diesen neu durchlaufen lässt. Gehe zu: Tools/Global Options.../General Entferne den Haken bei: Restore .RData into workspace at startup Ändere Save workspace to .RData on exit zu never Entferne den Haken bei: Always save history Keine Sorge, auf das Speichern deines Codes hat das keine Auswirkung. Im Sinne der Reproduzierbarkeit für Andere und für die eigene Fehlerbehebung ist es essentiell, beim erneuten Start von RStudio keine alten Zwischenergebnisse zu laden. Alle Berechnungen sollten durch die im Skript enthaltenen Befehle der Reihe nach auszuführen sein. Standard Zeichenencodierung. Damit man den geschriebenen Code fehlerfrei auf anderen Geräten lesen kann, muss dieselbe Zeichencodierung gewählt werden. Die modernste und am weitesten verbreitetste ist UTF-8. Gehe zu: Tools/Global Options.../Code/Saving Ändere: Default text encoding zu UTF-8 Programmierhilfen. Aller Anfang ist schwer und warum sollte man dann nicht jede zur Verfügung stehende Hilfe nutzen wollen? Hier wird eingestellt, dass Funktionen vom Rest des Codes farblich hervorgehoben werden. Außerdem wirst du darauf hingewiesen, wenn zu wenige oder zu viele Leerzeichen gesetzt wurden. Zum Schluss stellen wir die Vorschläge zur Vervollständigung von Code noch auf eine kürzere Zeit ein. Gehe zu: Tools/Global Options.../Code Wechsel zu Display und mache einen Haken bei Highlight R function calls Wechsel zu Diagnostics und mache einen Haken bei Provide R style diagnostics Wechsel zu Completion und ändere im Abschnitt Completion Delay die Zahlen auf 1 (Character) und 0 (ms) Schickes Aussehen. Es hat einen Grund, weshalb heutzutage viele Internetseiten und Smartphone Apps mit einem dunklen Farbthema angezeigt werden. Das ganze sieht nicht nur besser aus, sondern ist auch deutlich angenehmer für die Augen. Gehe zu: Tools/Global Options.../Appearance Nun kannst du aus verschiedenen Themen wählen und auch die Schriftart und Schriftgröße anpassen. Anordnung der vier Layer. Die Anordnung von Console, Skript und Co ist Geschmackssache. Sinnvoll ist beispielsweise eine Aufteilung mit dem Skript auf der linken oberen Seite und der Console auf der rechten oberen Seite. Die Fenster unten links kannst du dann für immer Minimieren und hast so mehr Platz zum Arbeiten im Skript. Gehe zu: Tools/Global Options.../Pane Layout Oben links: Source Oben rechts: Console Unten links: History, Connections Unten rechts: Environment, Files, Plots, Packages, Help, Build, VCS, Viewer Komfortables Arbeiten. Wenig ist nerviger als dauernd die R Datei im Unterordner des Unterordners zu finden, um das Programm zu öffnen. Deshalb kann man einstellen, dass sich immer das zuletzt verwendete R Skript und Projekt öffnet. Über R Projekte erfährst du mehr in Kapitel 3. Gehe zu: Tools/Global Options.../General Mache einen Haken bei Restore most recently openend project at startup Und bei Restore previously open source documents at startup 2.4 Funktionen und ihre Argumente Eine Funktion erkennst du immer daran, dass sie von zwei runden Klammern gefolgt ist. In diesem Buch sind alle Funktion in orange hevorgehoben. Innerhalb von RStudio werden diese ebenfalls farblich markiert werden, wenn du den Anweisungen aus Kapitel 2.3 gefolgt bist. Aber was ist eine Funktion eigentlich? Schauen wir uns exemplarisch die Funktion zur Berechnung der Summe mehrerer Werte an. Hier sollen die Werte 3, 4, 7 und 2 mithilfe der Funktion sum() zusammengezählt werden. Sämtliche Argumente (hier die vier Zahlen) müssen innerhalb runder Klammern der Funktion übergeben und dabei mit einem Komma voneinander getrennt werden. sum(3, 4, 7, 2) [1] 16 Auf diese Art und Weise können beliebig viele Argumente der Funktion übergeben werden. So könnte man bei einem vorliegendem fehlenden Wert (definiert als NA) diesen bei Zusammenzählen der Werte ignorieren und erhält nach wie vor ein Ergebnis. Das Argument zum Entfernen fehlender Werte heißt dabei na.rm. sum(3, 4, 7, NA, 2, na.rm = TRUE) [1] 16 Solange die Reihenfolge der Argumente, wie sie in der Funktion vorgeschrieben ist, eingehalten wird, können wir den Namen des Arguments weglassen. Davon werden wir im Verlauf des Buches immer wieder Gebrauch machen. In diesem Beispiel hingegen müssen wir na.rm explizit nennen, um den fehlenden Wert entfernen zu können. Die Entwickler der jeweiligen Funktionen definieren immer das Standardverhalten, falls keine weiteren Argumente genannt werden. Bei der Funktion sum() werden fehlende Werte bspw. standardmäßig nicht entfernt, weswegen wir dieses Verhalten mit na.rm = TRUE explizit hervorrufen müssen. Funktionen stellen eine Abkürzung zu Berechnung dar, die wir ansonsten manuell durchführen müssten. Der Funktionsname wird von einer öffnenden und einer schließenden runden Klammer gefolgt (z.B. sum() für die Summe, mean() für den Mittelwert oder median() für den Median). Argumente verändern das Verhalten einer Funktion und können innerhalb der runden Klammern, getrennt durch ein Komma, mit einem Gleichheitszeichen verändert werden. 2.5 Packages (Erweiterungen) Direkt in R ist eine Bandbreite an Funktionen integriert. Darüber hinaus gibt es zahlreiche Erweiterungen, die das Lösen verschiedener Problemstellungen erheblich erleichtern. Diese kostenlosen Erweiterungen nennt man Packages. Stell dir vor, du kaufst dir ein neues Smartphone, auf dem von Anfang an verschiedene Apps installiert sind. Du könntest dieses Smartphone mit den vorinstallierten Apps grundsätzlich verwenden. Für eine bessere Nutzerfreundlichkeit oder andere Funktionalitäten können allerdings auch zusätzliche Apps von Drittanbietern installiert werden. Genauso sind Packages in R zu verstehen. Auch mit den von Anfang an integrierten Funktionen könnte man die meisten Sachen irgendwie hinbekommen. Nur wäre dies mit deutlich mehr Aufwand verbunden als heutzutage notwendig. Deshalb arbeiten wir im Verlaufe des Buches mit verschiedenen Packages, die erst einmal installiert und geladen werden müssen. Die Packages werden unter anderem auf CRAN, Github oder Bioconductor geteilt, von wo sie heruntergeladen und in die eigene Analyse integriert werden können. Es gibt beispielsweise mittlerweile über 11000 Packages alleine auf CRAN. 2.5.1 Installieren und laden Wir unterscheiden zwischen einer Funktion zum einmaligen Installieren und einer Funktion zum wiederholten Laden des Packages innerhalb von R. Abbildung 2.4 verdeutlicht den Unterschied der beiden Funktionen. Abbildung 2.4: Vergleich vom (a) Installieren und (b) vom Laden von Packages. Während install.packages() das Package installiert, muss man es mit library() jedes Mal beim Starten von R neu laden. Erstere Funktion kannst du dir wie das Eindrehen einer Glühbirne vorstellen. Es bleibt dunkel im Zimmer, solange du nicht den Lichtschalter betätigst. Wenn du das Zimmer verlässt (R beendest), wird das Licht automatisch wieder ausgeschaltet. Jedes Mal, wenn du das Zimmer erneut betrittst, muss das Licht also erneut eingeschaltet werden. Beim Starten von R muss jedes Mal aufs Neue der Befehl library() für jedes Package ausgeführt werden, welches du benutzen möchtest. Es gilt generell, so wenige Packages wie möglich und so viele wie nötig zu verwenden. In diesem Buch werden nur aufeinander abgestimmte Packages verwendet, aber das ist nicht immer der Fall. Auch verändern sich manche Packages im Laufe der Zeit nennenswert, sodass die eigene Analyse zwei Jahre später ggf. nicht mehr funktionieren könnte. Die Funktion install.packages(\"package\") benötigt zur Installation als Argument den Namen des Packages in Anführungszeichen. install.packages(&quot;here&quot;) Innerhalb von RStudio wird der Befehlsaufruf in der Konsole eingegeben. Wie in Abbildung 2.5 illustriert, erscheint nach Ausführen der Funktion durch Drücken der Enter Enter Taste der Installationsverlauf in roter Schrift. Dabei handelt es sich, anders als die rote Farbe suggeriert, weder um eine Warnmeldung noch um eine Fehlermeldung. Ein Fehler würde in diesen Benachrichtigungen als Error beschrieben werden und die Installation stoppen. Abbildung 2.5: Eingabe der einmaligen Installation in die Konsole. Falls bei der Installation Fehler auftreten, sollte das Argument dependencies = TRUE, getrennt mit einem Komma, zusätzlich verwendet werden. Damit werden die Packages installiert, von denen das gewünschte Package gegebenenfalls zusätzlich abhängt. install.packages(&quot;here&quot;, dependencies = TRUE) Damit man auf die Funktionen des Packages zugreifen kann, muss das Package jedes Mal – also nach jedem neuen Öffnen von RStudio – aus der Bibliothek mithilfe von library(package) erneut geladen werden. Hierbei sind keine Anführungszeichen notwendig. library(here) Zur besseren Übersichtlichkeit solltest du alle library() Befehle am Anfang des jeweiligen R Skriptes untereinander schreiben und aufrufen. Nach der Installation der Packages gibt es zwei Gründe, die Packages erneut installieren zu müssen. Der Umstieg auf eine neue Hauptversion von R (z.B. von R 4.0.0 auf R 5.0.0): Dabei wird der Ordner, der die Packages enthält, neu angelegt. Daher müssen alle Packages erneut installiert werden. Es gibt leider keinen einfachen Weg, alle installierten Packages automatisch in den neuen Ordner zu kopieren. In einem der Packages gibt es eine neue Funktion, die man verwenden möchte. Der einfachste Weg, Packages zu updaten, ist durch die erneute Installation. Es gibt zwar den Befehl update.packages(), allerdings muss dieser als Administrator in R und nicht RStudio ausgeführt werden. Selbst bei Ausführung als Administrator können dabei Probleme auftreten. Also ist man mit der erneuten Installation der Packages im Regelfall besser beraten. In aktuellen Versionen von RStudio wird automatisch eine Benachrichtigung am Anfang des Skriptes angezeigt, falls in dem Skript erwähnte Packages nicht installiert sind. Diese können dann über entsprechenden Mausklick durch RStudio installiert werden. Die gute Nachricht ist, dass man im Grunde genommen weder R noch die Packages updaten muss. Alle in diesem Buch verwendeten Packages werden sich in Zukunft voraussichtlich nicht mehr nennenswert verändern und auch in R kommen nur selten für die Datenanalyse relevante Neuerungen hinzu. 2.5.2 Notwendige Packages für dieses Buch Diese vier für dieses Buch essentiellen Packages werden wir nun zuerst installieren. Das remotes Package verwenden wir nur einmalig, um das Package begleitend zum Buch namens remp von Github herunterladen zu können. Wofür jedes einzelnen Package genau zuständig ist, wirst du im Verlaufe des Buches erfahren. Die Installation kann je nach Internet und Computer einige Minuten in Anspruch nehmen. Jeder Aufruf der Funktion install.packages() muss nur einmal ausgeführt werden. Kopiere nacheinander die einzelnen Befehle in die Console und warte jeweils, bis die Installation abgeschlossen ist. install.packages(&quot;tidyverse&quot;) install.packages(&quot;here&quot;) install.packages(&quot;rio&quot;) install.packages(&quot;remotes&quot;) Um auf die Übungsdatensätze und interaktive Übungen zugreifen zu können, musst du das Package remp installieren. Dafür wird zunächst das Package namens remotes geladen. Während der Installation des Packages kann es sein, dass du gefragt wirst, ob du bestimmte Packages aktualisieren möchtest. Du solltest dies an der Stelle ohne weiteres Zutun mit durch Klicken der Enter Enter Taste verneinen. library(remotes) install_github(&quot;j3ypi/remp&quot;) Alle anderen teilweise speziell auf einen Kontext zugeschnittenen Packages, die im Rahmen des Buches vorgestellt werden, kannst du bei Bedarf installieren. Am Anfang jedes Kapitels mit einem neuen Package wird immer explizit darauf hingewiesen. Im remp Package sind Datensätze, Funktionen und Übungen für dieses Buch enthalten. Da dieses Package nicht auf CRAN sondern auf Github gespeichert ist, muss hier zur Installation die Funktion install_github() aus dem remotes Package anstelle der üblichen Funktion install.packages() verwendet werden. Das remotes Package muss hierbei vorher installiert werden. Wir werden im Rahmen dieses Buches vor allem diese vier Packages verwenden, weswegen zu Beginn jedes Skriptes die vier Packages geladen werden müssen. library(tidyverse) library(here) library(rio) library(remp) In diesem Fall ist die Reihenfolge des Ladens nicht entscheidend. Falls verschiedene Packages jedoch gleichnamige Funktionen beinhalten, führt dies zu Problemen, auf die im folgenden Kapitel näher eingegangen wird. 2.5.3 Namespace Beim Laden mehrerer Packages kann es sein, dass diese Funktionen mit demselben Namen verwenden (siehe Kapitel 6.1). Während beim tidyverse lediglich zwei selten verwendete base R Funktionen überschrieben werden, kann es beim Arbeiten mit vielen verschiedenen nicht aufeinander abgestimmten Packages durchaus häufiger zur Namensgleichheit kommen. Das ist eine schwierig zu identifizierende Fehlerquelle, weil sich die Funktion mitunter plötzlich nicht mehr so verhält wie erwartet. Dabei verwendet man automatisch die Funktion, die aus dem zuletzt geladenen Package stammt. Lädt man beispielsweise zuerst das Package tidyverse und anschließend data.table, erhält man folgende Meldung: library(tidyverse) library(data.table) Attaching package: ‘data.table’ The following objects are masked from ‘package:dplyr’: between, first, last The following object is masked from ‘package:purrr’: transpose Drei Funktionen (between(), first(), last()) aus dem Package dplyr und eine Funktionen (transpose()) aus dem Package purrr werden von data.table überschrieben. Es ist also wichtig, diese Meldungen beim Laden eines Packages nicht zu ignorieren. Im Notfall kann dies durch die Verwendung von Doppelpunkten package::funktion() verhindert werden. So teilt man R explizit mit, welche Funktion aus welchem Package man meint. Um trotz des späteren Ladens von data.table auf die between() Funktion von dplyr zuzugreifen, würde man also beim Aufrufen der Funktion folgendes schreiben: dplyr::between(1:12, 7, 9) Eine Ausnahme stellt dabei das tidyverse dar. Weil das tidyverse nur andere Packages lädt, kann man nicht tidyverse::funktion() schreiben. Stattdessen muss man das Package, aus dem die Funktion stammt (z.B. dplyr), direkt ansprechen. Für genauere Informationen zum Thema Namespace und die Hintergründe der damit verbundenen Environments sei auf das Buch Advanced R verwiesen. 2.6 Fehler- und Warnmeldungen 2.6.1 Der Unterschied Es gibt einen großen Unterschied zwischen Fehler- und Warnmeldungen. Wie der Name bereits suggeriert, stoppen Fehlermeldungen den Code, während Warnmeldungen ein Ergebnis zurückgeben und nur auf mögliche Probleme hinweisen. Es ist also sehr wichtig, die roten Meldungen in der Konsole genau zu lesen, anstatt direkt in Panik zu geraten. Ein Beispiel für eine Fehlermeldung sehen wir, wenn wir die Zahl 1 mit dem Buchstaben c summieren möchten. 1 + &quot;c&quot; Error in 1 + &quot;c&quot;: non-numeric argument to binary operator Hier ist die Fehlermeldung eindeutig. Wir versuchen einen nicht-numerisches Buchstaben (das c) mit einem numerischen zu addieren. Leider sind Fehlermeldungen in R keineswegs immer so eindeutig zu interpretieren. Nicht selten sind sie kryptisch und vor allem am Anfang wird man oft im Internet nach einer Lösung suchen müssen. Warnmeldungen in R haben zwar dieselbe erschreckende rote Schrift wie Fehlermeldungen, allerdings starten sie mit Warning Message und stoppen den Code nicht. Warnmeldungen sind für dich als Benutzer gedacht, um auf mögliche Probleme bei deiner Eingabe hinzuweisen. Gerade bei der statistischen Auswertung können Warnungen dir schon einmal häufiger über den Weg laufen und sollten keinesfalls blind ignoriert werden. Die Sprache der Fehler- und Warnmeldungen muss dabei Englisch sein, da sonst nicht vernünftig im Internet danach gesucht werden kann. Bitte beachte die dafür notwendigen Anpassungen bei der Installation, die in Kapitel 2.1.1 erklärt wurden. 2.6.2 Wo bekomme ich Hilfe? Während der ersten Kontakte mit einer Programmiersprache dauert das Warmwerden möglicherweise eine gewisse Zeit. Damit man nicht gleich die Motivation verliert und aufgibt, sind die richtigen Quellen für eine schnelle Hilfe essentiell. Glücklicherweise ist dies einer der großen Vorteile von R. Es existieren nicht nur ausführliche Dokumentationen der verschiedenen Funktionen mit Anwendungsbeispielen. Darüber hinaus ist die R Community auch besonders hilfsbereit. Falls du verzweifelt vor deinem Computer oder Laptop sitzt und nicht weißt, wieso dein Code schon wieder nicht funktioniert, solltest du besonders an vier Orten nach Antworten suchen. Package Dokumentation. Das naheliegendste ist die Dokumentation der R Funktionen, die auch ohne Internet direkt in R aufgerufen werden können. Dafür kannst du entweder ein Fragezeichen vor die Funktion schreiben und diese ausführen oder mit dem Cursor auf der Funktion F1 drücken. Wenn wir zum Beispiel mehr über die Argumente der Funktion install.packages() erhalten möchten, können wir dies so erreichen: ?install.packages Die meisten modernen Packages stellen außerdem so genannte Vignetten zur Verfügung, in denen häufige Probleme ausführlich diskutiert und erklärt werden. Diese können mit vignette(\"nameDerVignette\") auch direkt innerhalb von R aufgerufen werden. Suchmaschine. In die Suchmaschine gibst du einfach r gefolgt von der Fehlermeldung oder dem Problem ein, an dem du aktuell festhängst. Es gibt kaum eine Fehlermeldung, bei der das Programmierforum StackOverflow nicht an erster Stelle angezeigt wird. Denn meistens hat jemand anderes schon einmal genau dasselbe Problem gehabt hat. Stackoverflow. Dies ist wohl das mit Abstand größte Forum für Programmierfragen. Allerdings kann es einschüchternd sein, dort als absoluter Anfänger selbst eine Frage zu stellen. Voraussetzung zum Fragestellen ist dort das Erstellen eines kurzes reproduzierbares Beispiel für den Fehler. Du erreichst die Seite unter https://stackoverflow.com/. RStudio Community. Eine etwas kleinere Alternative zu StackOverflow ist die RStudio Community. Statte ihr bei Bedarf unter https://community.rstudio.com/ einen Besuch ab. 2.6.3 Fehler beheben Während deiner ganzen Zeit beim Programmieren wirst du immer wieder Code schreiben, der unweigerlich zu einem Fehler führt. Ob nun ein Package nicht geladen oder eine Klammer zu wenig gesetzt ist. Irgendetwas läuft häufig schief – so mag es zu Beginn erscheinen. Richtig um Hilfe fragen, ist dabei gar nicht so einfach. Es ist zum Beispiel wenig informativ, wenn du jemandem schreibst: “RStudio funktioniert nicht”. Was funktioniert nicht? Wann tritt der Fehler auf? Hat es vorher funktioniert? Alle diese Informationen müssen dann erst einmal mühselig erfragt werden. Nun können die bereitzustellenden Informationen grundsätzlich in drei Abschnitte gegliedert werden. Eine kurze Problembeschreibung. Stelle ein minimales, reproduzierbares Beispiel bereit. Erkläre, was du bereits probiert hast. Während Punkt 1 und 3 soweit verständlich sind, bedarf es eine Erklärung für das reproduzierbare Beispiel. Ein reproduzierbares Beispiel enthält alle Informationen, die jemand anderes zum exakten Replizieren des Fehlers benötigt. Es müssen also zum Beispiel alle geladenen Packages angegeben werden. Außerdem ist es essentiell, den Datensatz in einer vereinfachten Kurzform zur Verfügung zu stellen. Wenn du beim Analysieren deines eigenen Datensatzes auf ein Problem stößt, kann natürlich niemand anderes auf diesen Datensatz zugreifen. Entweder erstellt man dann einen kleinen Datensatz innerhalb von R, mit dem das Problem repliziert wird oder man lässt die Datenstruktur des eigenen Datensatzes ausgeben. Angenommen, wir haben eine Fehlermeldung bei der Auswertung eines Datensatzes namens demogr. demogr # A tibble: 4 × 3 ID Sex Alter &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 AX161095 m 28 2 NM020683 f 47 3 IO240576 f 40 4 JH051199 m 24 Dieser Datensatz ist relativ kompakt und kann mit der Funktion dput() in ein Format gebracht werden, welches ohne Zugriff auf die Originaldatei von Anderen in R aufgerufen werden kann. dput(demogr) structure(list(ID = c(&quot;AX161095&quot;, &quot;NM020683&quot;, &quot;IO240576&quot;, &quot;JH051199&quot; ), Sex = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;), Alter = c(28, 47, 40, 24)), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;), row.names = c(NA, -4L)) Bei größeren Datensätzen sollte man nur einen Teil der Daten auswählen (siehe Kapitel 6.2 und 6.3). Bei sensiblen Daten bietet es sich an, einen kleinen Datensatz innerhalb von R neu zu erstellen, der anstelle der Originaldaten strukturell ähnliche Werte in den Spalten beinhaltet (siehe Kapitel 5.3). Häufig kommt man von ganz alleine auf die Lösung, während man Schritt für Schritt die durchgeführten Zeilen Code auseinander nimmt, um das Problem innerhalb eines minimalen Beispiels zu reproduzieren. Bevor du allerdings die Frage an jemand anderes richtest, solltest du diese häufigen Fehlerquellen bereits ausgeschlossen haben: Tippfehler. Ein Package ist nicht installiert oder geladen. Ein oder mehrere Packages sind zu alt. Eine Klammer ist zu viel oder zu wenig. Anführungszeichen bei wurden vergessen oder an falsche Stelle gesetzt. Ein Komma anstelle eines Punktes wurden füri Dezimalzahlen verwendet. Numerische Spalten im Datensatz wurden als Character eingelesen (häufig bei Excel Dokumenten). Character Spalten sind fälschlicher Weise als Faktoren dargestellt. Das R Skript wurde nicht innerhalb eines R Projekts geöffnet, sodass der Datensatz nicht gefunden wird. "],["project.html", "Kapitel 3 R Projekte 3.1 Die Probleme der Pfade 3.2 Projekte in RStudio erstellen 3.3 Der Pfad zum Datensatz", " Kapitel 3 R Projekte Woher weiß der Computer, an welchem Ort sich mein Datensatz befindet? Die Antwort auf diese Frage war lange umständlich und je nach Betriebssystem unterschiedlich. Das Konzept der Projekte innerhalb von R stellt die Basis zum einfachen Einlesen deines Datensatzes dar. 3.1 Die Probleme der Pfade Erinnere dich an das letzte Mal zurück, als du eine Datei irgendwo hochgeladen hast. Normalerweise öffnet sich dann ein kleines Dialogfenster, durch das du bis zu deiner Datei navigieren kannst. Das hat den Hintergrund, dass dein Betriebssystem den genauen Ort der Datei – den so genannten Pfad – wissen muss, um diese finden und hochladen zu können. Das gleiche kannst du beobachten, wenn du gefragt wirst, wo genau du auf deinem Computer eine heruntergeladene Datei speichern möchtest. In R war das Einlesen von Datensätzen genau deswegen lange ein Problem. Dabei gibt es vor allem vier zentrale Probleme hervorzuheben: Wenn jemand auf einem anderen Computer die Analysen innerhalb deines R Skripts nachvollziehen möchte, ist der Pfad auf seinem oder ihrem Computer anders. Der Dateipfad ist oft lang, weil die Zieldatei in vielen verschachtelten Unterordnern liegt. Häufig verschiebt man den Ordner mit der Zeit an einen anderen Ort. Zwischen den Betriebssystemen ist die Art, den Pfad darzustellen, unterschiedlich. Zum Ersten Punkt kommt hinzu, dass die wenigsten überhaupt wissen, was genau ein Pfad ist und wie man diesen korrekt angeben müsste. Punkt zwei und drei ist vor allem hinsichtlich Reproduzierbarkeit ein Problem. Auch wenn du deine Analysen an KollegInnen oder BetreuerInnen schicken möchtest, wird der Code ohne Anpassungen nicht funktionieren. Glücklicherweise gibt es mittlerweile R Projekte, so dass du dich niemals mit Pfaden und daraus resultierenden fehlenden Reproduzierbarkeit auseinander setzten musst. Durch Projekte kann eine Funktion zum Einlesen des Datensatzes unabhängig vom Ort des Ordners sehen, wo sich die Datei befindet. Die einzigen beiden Voraussetzungen sind, dass der Datensatz im selben Ordner oder Unterordner wie die R Projektdatei liegt und dass man das here Package lädt. 3.2 Projekte in RStudio erstellen Nehmen wir einmal an, auf unserem Desktop liegt ein Ordner namens Beispiel. Wie in Abbildung 3.1 ersichtlich, befinden sich in diesem Ordner drei Dateien und ein Unterordner namens Daten. Abbildung 3.1: Beispielhafte Ordnerstruktur mit R Skript, Projektdatei und Datensatz. Auswertung.R ist unser R Skript, Beispiel.Rproj unsere Projektdatei und video.xlsx der Datensatz, den wir zur Auswertung einlesen wollen. Die Projektdatei muss manuell vor dem Einlesen erstellt werden. Dafür benötigt man nur ein paar Klicks. Oben rechts befindet sich ein Reiter namens Project: (None), wenn kein Projekt geöffnet ist und ansonsten der Projektname (zum Beispiel das Projekt Beispiel). Öffne zuerst das Dropdown Menu. Abbildung 3.2: Erster Schritt beim Erstellen eines neuen Projektes. Uns interessieren zum einen New Project... und Open Project... und zum anderen sind weiter unten andere Projekte aufgelistet, die vorher geöffnet wurden (hier 07_Buch und remp). Dadurch kann man mit einem einfachen Klick zwischen den eigenen Projekten wechseln. Dieses Feature erleichtert die Arbeit ungemein, da man auf dem Computer nicht mehr diverse Ordner nach den richtigen Dateien durchsuchen muss. Zum Erstellen eines neuen Projekts klicke auf New Project..., wodurch ein neues Fenster erscheint. Abbildung 3.3: Zweiter Schritt beim Erstellen eines neuen Projektes. Wir entscheiden uns exemplarisch für die Option Existing Directory. Das bedeutet, unser Ordner Namens Beispiel existiert bereits auf dem Desktop. Ob dieser Ordner leer oder bereits mit anderen Dateien gefüllt ist, ist nicht weiter von Bedeutung. Abbildung 3.4: Letzter Schritt zur Erstellung eines neuen Projektes. Mit einem Klick auf Create Project wird nun eine Projektdatei mit der Endung .Rproj in den gewählten Ordner gespeichert. Projekte bieten übrigens bezüglich Reproduzierbarkeit einen weiteren Bonus. Bei jedem Start von RStudio wird eine neue und in sich abgeschlossene R Umgebung geladen. So kann garantiert werden, dass der Code genau so auch auf anderen Computern ausgeführt werden kann. Beachte, beim Öffnen eines R Projects in dem Ordner nicht auf das R Skript (hier Auswertung.R) sondern auf die Projektdatei zu klicken und erst im Anschluss das R Skript zu öffnen. Für alle zukünftigen Öffnungen kannst du in Zukunft einfach RStudio öffnen und in dem eingangs beschrieben Dropdown Menü rechts oben das Projekt auswählen. Damit R die Position der Projektdatei auch findet, brauchen wir nun außerdem das here Package. Beim Arbeiten in einer Cloud wie Dropbox kann es zu einer Fehlermeldung kommen, die besagt, dass RStudio nicht auf die Datei zugreifen kann. Um das zu umgehen, muss die Synchronisierung der Cloud für die Dauer des Arbeitens mit R angehalten werden. 3.3 Der Pfad zum Datensatz Die Magie im Kontext von Projekten passiert, wenn du das here Package lädst. Das Package findet selbstständig den relativen Pfad zu deiner Projektdatei heraus. Was bedeutet das? Während man früher beispielsweise auf Windows mit C:\\Users\\J-PhN\\Desktop\\Beispiel den absoluten Pfad zum Ordner eingeben musste, findet das here Package den Ordner Beispiel mit der Projektdatei unabhängig von der Lage des Ordners. In der Praxis sieht das beim Laden wie folgt aus: &gt; library(here) here() starts at C:/Users/J-PhN/Desktop/Beispiel Würden wir den Ordner verschieben, hätte das keine Auswirkungen auf unseren Code, denn das Package würde wieder zum Projektordner finden. Der erste Schritt ist also immer das Erstellen eines R Projekts und das Laden des here Packages am Anfang jedes neuen Skripts, mit dem man einen Datensatz einlesen möchte. Alternativ kann der Pfad zum Datensatz auch mit setwd() manuell festgelegt werden. Da der Pfad allerdings zwischen den Betriebssystemen unterschiedlich ist und man nach jedem Verschieben der Dateien erneut setwd() anpassen muss, wird ausdrücklich davon abgeraten. "],["vars.html", "Kapitel 4 Wichtiges Grundlagenwissen 4.1 Schritt für Schritt beginnen 4.2 Variablen speichern und verwenden 4.3 Datentypen 4.4 Struktur von Datensätzen 4.5 Der Dollar-Operator", " Kapitel 4 Wichtiges Grundlagenwissen Bevor wir richtig loslegen können, müssen wir einige Grundlagen besprechen. Das Abspeichern von Zwischenergebnissen in Form von Variablen ist genauso essentiell wie die verschiedenen Datentypen, die in den jeweiligen Spalten deines Datensatzes enthalten sind. Auch einzelne Spalten mithilfe des Dollar-Operators aus einem Datensatz herauszuziehen ist ein zentrales Konzept, auf das wir im Verlauf dieses Buches häufig zurückgreifen werden. 4.1 Schritt für Schritt beginnen 1. Schritt: RStudio starten. Nach der Installation von R und RStudio (siehe Kapitel 2.1.1 und 2.1.2) kann RStudio in Windows durch Tippen der Windows-Taste mit anschließender Eingabe von RStudio und in macOS über den Finder gestartet werden. In der Konsole wird dabei automatisch die installierte R Version angezeigt. Abbildung 4.1: Starten von RStudio. 2. Schritt: R Projekt erstellen. Als nächster Schritt sollte ein in sich abgeschlossenes Projekt erstellt werden (siehe Kapitel 3). Das erfolgreiche Erstellen und Öffnen des Projektes erkennt man daran, dass oben rechts in RStudio anstelle von Project: (None) der Name des Projektes steht (z.B. Beispiel). Manchmal wird, wie in Abbildung 4.2 gezeigt, der Überordner der Projektdatei mit angezeigt (hier der Ordner R Skripte). Wir sehen auch, dass unten rechts im Reiter Files jetzt die im Projektordner befindlichen Dateien angezeigt werden. Abbildung 4.2: Innerhalb von RStudio ein neues R Projekt erstellen. 3. Schritt: R Skript erstellen. Da die Funktionsaufrufe in der Konsole nur einmalig ausgeführt und nicht gespeichert werden, muss anschließend das R Skript entweder durch Klicken des Pluszeichens links oben oder durch die Tastenkombination Strg / Cmd + Shift + N erstellt werden (siehe Kapitel 2.1.2). Abbildung 4.3: Ein neues R Skript für die Analysen erstellen. 4. Schritt: Benötigte Packages laden. Bei jedem Start von RStudio müssen zunächst alle verwendeten Packages neu geladen werden. Dafür markieren wir die verschiedenen library() Aufrufe (hier vier Stück) und führen diese mit der Tastenkombination Strg / Cmd + Enter aus. Anders als in der Konsole werden die Funktion in einem R Skript also nicht alleine mit Enter sondern mit der Kombination Strg / Cmd + Enter ausgeführt. Um einzelne Zeilen auszuführen, genügt es, den Cursor auf der Zeile zu haben und dabei Strg / Cmd + Enter auszuführen. Manche Packages geben beim Laden Benachrichtigungen zurück. Was die Benachrichtigungen des tidyverse in Abbildung 4.4 bedeuten, wird in Kapitel 6 erläutert. Abbildung 4.4: Packages nach jedem Start neu markieren und ausführen. 5. Schritt: Datensatz einlesen und Analysen durchführen. Nach Laden der Packages kann der Datensatz eingelesen werden (siehe Kapitel 5.1). Durch das Einlesen und Speichern mit Zuweisungspfeil erscheint der Datensatz rechts oben in der Environment (siehe Kapitel 4.2). Man kann also im weiteren Verlauf auf den big5 Datensatz namens daten zugreifen. Auch dieser muss bei Neustart von RStudio erneut durch Auswahl der Zeile (hier Zeile 7) und Betätigen der Tastenkombination Strg / Cmd + Enter eingelesen werden. Anschließend sollte man im Rahmen der weiteren Auswertung der Daten immer Kommentare einbauen, die mit einer führenden Raute markiert werden (siehe Zeile 9 in Abbildung 4.5). Exemplarisch schauen wir uns hier die Spalten Extraversion und Neurotizismus als ersten Schritt unserer Datenanalyse an (siehe Kapitel 6.2). Abbildung 4.5: Datensatz einlesen und Analysen durchführen. 4.2 Variablen speichern und verwenden Ein zentrales Konzept in R ist das Speichern von Variablen mithilfe des Zuweisungspfeils. Dies ist vor allem für all jene ungewöhnlich, die das Erstellen von Variablen aus anderen Programmiersprachen mit dem Gleichheitszeichen kennengelernt haben. Wenn man das Ergebnis der durchgeführten Operation nicht speichert, ist es sofort weg und muss erneut ausgeführt werden. Würde man nun 2 + 2 [1] 4 rechnen, gibt R zwar 4 zurück, allerdings kann man später nicht mehr auf diese 4 zurückgreifen. Wenn man beispielsweise einen Datensatz einliest, ohne diesen mit dem Zuweisungspfeil zu speichern, kann man auf diesen im weiteren Verlauf nicht zugreifen. Mithilfe des Zuweisungspfeils wird die Variable in die lokale Environment gespeichert. Wir erinnern uns, die Environment ist in der Standardeinstellung nach Installation von RStudio im Fenster oben rechts. Möchten wir beispielsweise die Rechenoperation von vorhin namens rechnung speichern, würde man wie folgt vorgehen: rechnung &lt;- 2 + 2 Im Nachfolgenden könnte man nun diese Variable jederzeit wieder aufrufen. rechnung [1] 4 Auch ist es nun möglich, weitere Rechenoperationen mit dem vorherigen Ergebnis auszuführen. Zum Beispiel könnte man unser Ergebnis namens rechnung mit der Zahl 4 multiplizieren. rechnung * 4 [1] 16 Variablen kann man grundsätzlich fast so benennen, wie man möchte. Man darf nur nicht mit einer Zahl anfangen oder nach einem Punkt direkt eine Zahl als Namen wählen wie bspw. bei .2VariablenName. Auf Umlaute sollte im Zusammenhang mit Programmiersprachen aufgrund verschiedener Zeichenkodierungen ebenfalls immer verzichtet werden. Groß- und Kleinschreibung macht ebenfalls einen Unterschied. Achte beim Umgang mit Variablen immer auf die exakte Schreibweise beim Speichern in der Environment. Das Ergebnis als rechnung zu speichern ist nicht dasselbe wie als Rechnung oder rechnunG. In den Variablen können sämtliche Datenstrukturen (siehe Kapitel 11) verstaut werden. Für den Moment reicht es für uns zu wissen, dass wir Datensätze und Zwischenergebnisse in den Variablen abspeichern müssen, um weiter darauf zugreifen zu können. Variablen können einfach überschrieben werden, indem man der Variable einen anderen Wert zuweist. Gerade in der Datenvorbereitung kann es schon einmal verlockend sein, die Änderungen unter demselben Variablennamen zu speichern. Wenn man allerdings eine unbeabsichtigte Änderung abspeichert, kann dies nicht rückgängig gemacht werden. Der Datensatz muss als Resultat erneut eingelesen werden. Es sei also vor allem am Anfang Vorsicht geboten. Auf der anderen Seite sollte man auch nicht jeden einzelnen Schritt in der Datenvorbereitung mit einem bedeutungslosen Namen versehen. Schließlich hätten die Zwischenergebnisse rechnung1, rechnung2, rechnung3 und rechnung4 keine Information im Namen, welches Ergebnis nun welche Änderung enthält. Aussagekräftige Namen helfen anderen, deinen Code zu verstehen. Wenn du dir nun denkst, ohnehin nicht mit anderen zusammenarbeiten zu werden, lass dir gesagt sein: der oder die Andere bist in den meisten Fällen du selbst einige Wochen oder Monate später. Ohne Dokumentation und vernünftige Namensgebung sieht dein R Skript Monate später schnell so aus, als hätte es irgendein Fremder geschrieben. Dein zukünftiges Ich wird dir dankbar sein. Behalte immer im Hinterkopf, dass der Zuweisungspfeil zwar die Variable speichert, aber keinen direkten Output in der Konsole ausgibt. Oft erweckt das den Eindruck, als wäre nichts passiert. Es hilft dann, den Namen der Variable, wie zuvor gezeigt, explizit aufzurufen. Wenn man etwas im Code kommentieren möchte, muss eine führende Raute hinzugefügt werden. Das eignet sich nicht nur für kurze Beschreibungen, sondern auch als Gliederung eines langen R Skriptes. Beispielsweise könnte man so den Abschnitt der Berechnung vom Erstellen von Visualisierungen optisch trennen. Die Anzahl der Rauten spielt dabei keine Rolle. # Berechnung I # Dieser Code summiert 2 und 4 2 + 4 [1] 6 4.3 Datentypen Grundsätzlich gibt es in R vier verschiedene Grunddatentypen: Integer, Double, Character und Logical (siehe Abbildung 4.6). Dabei lassen sich Integer und Double zum Datentyp Numeric zusammenfassen, da die Unterscheidung dieser beiden in R selten von Bedeutung ist. Schauen wir uns die Datentypen nun etwas genauer an. Numeric (kurz: &lt;num&gt;) beschreibt numerische Werte, also Zahlen. Integer (kurz: &lt;int&gt;) sind ganze Zahlen und Doubles (kurz: &lt;dbl&gt;) Dezimalzahlen. Beachte dabei, dass Dezimalzahlen in R mit Punkten und nicht mit Kommata dargestellt werden. Abbildung 4.6: Schematische Übersicht über die wichtigsten Datentypen in R. 4.3.1 Zahlen, Buchstaben und logische Abfragen Beispiele für Numerics wären zum Beispiel Alter, Gehalt oder der Blutdruck. Wenn wir später Datentypen aus einem echten Datensatz ansehen, wirst du schnell merken, dass beinahe alle Zahlen als Double deklariert werden. Das liegt an der Eigenheit von R, ein L hinter die Zahl setzen zu müssen, wenn es sich um eine ganze Zahl hat. Dies hat allerdings keinerlei Auswirkung auf die in diesem Buch vorgestellten Funktionen. Double: 3.14 42 Integer: 42L Character (&lt;chr&gt;) ist der Datentyp, der Text enthalten kann – also einzelne Buchstaben, Zeichen, Wörter oder ganze Sätze. Dabei muss der Text immer in Anführungszeichen stehen. Solange die Anführungszeichen verwendet werden, kann mit Ausnahme vom Backslash (\\) alles geschrieben werden. Beispiele für Characters wären zum Beispiel die Blutgruppe, das Herkunftsland oder Allergien. &quot;Hallo Welt&quot; Logical oder auch logische Datentypen sind etwas abstrakter und kommen in Datensätzen seltener vor. Sie werden dann benötigt, wenn wir aufgrund von bestimmten Bedingungen manche Operationen durchführen möchten und andere wiederum nicht. Beispiele dafür wären die Auswahl derjenigen Personen, die über 50 Jahre alt sind oder eine neue Spalte namens Geschlecht zu erstellen, die nur bei weiblichen Personen eine 1 und ansonsten eine 0 einträgt. Dabei wird nämlich geprüft, ob die Aussage (zum Beispiel Person A ist älter als 50) wahr oder falsch ist. Das heißt, es gibt dabei zwei Zustände: TRUE oder FALSE (immer in Großbuchstaben). Eine Bedingung kann entweder zutreffen oder eben nicht. Es gibt verschiedene Funktionen, die TRUE oder FALSE zurückgeben, auf die wir im Verlaufe des Buches noch stoßen werden. Grundsätzlich gibt es dabei nur wenige verschiedene Grundoperatoren, die man kennen sollte. Um zu schauen, ob zwei Werte gleich sind, benutzen wir ein doppeltes Gleichheitszeichen (==). 2 == 2 [1] TRUE Gleiches Prinzip gilt für größer gleich (&gt;=) und kleiner gleich (&lt;=). Für größer (&gt;) oder kleiner (&lt;) reicht hingegen das einzelne mathematische Zeichen. Möchten wir nun logische Operationen kombinieren, verwenden wir UND (&amp;) oder ODER (|). Bei UND müssen beide Aussagen wahr sein. 1 &lt; 2 &amp; 1 == 2 [1] FALSE Im Falle vom ODER-Operator muss nur eine logische Abfrage zutreffen, um ein TRUE ausgegeben zu bekommen. Man würde es wie folgt lesen: Entweder ist 1 kleiner 2 ODER 1 ist gleich 2. Da die erste Aussage wahr ist, wird TRUE zurückgegeben. 1 &lt; 2 | 1 == 2 [1] TRUE Neben den logischen Operatoren &amp; und | existieren in R ebenfalls die doppelten Varianten &amp;&amp; und ||. Der Unterschied besteht in dem Ausgabeformat. Im Falle der einzelnen Operatoren (&amp;, |) können (vektorisiert) unbegrenzt viele Abfragen überprüft werden. Der Ausdruck c(\"A\", \"C\") == \"A\" gibt bspw. TRUE FALSE zurück, da der erste Buchstabe ein A ist und der zweite nicht. Bei den doppelten Operatoren (&amp;&amp;, ||) würde dieser Ausdruck einen Fehler produzieren, da hier die logische Abfrage immer nur einen einzigen logischen Wert zurückgeben darf (entweder TRUE oder FALSE). Dies ist vor allem bei if-Abfragen oder while-Schleifen nützlich. Im Kontext der wissenschaftlichen Datenanalyse greifen wir in den meisten Fällen auf die einzelnen logischen Operatoren zurück (&amp;, |). So können beliebig viele logischen Operationen miteinander kombiniert werden. Ein besonderer Fall logischer Datentypen ist NA (Akronym für Not Available), also die Bezeichnung für einen fehlenden Wert. Wir können einen Wert oder eine Variable auf seinen Datentyp überprüfen. Zurückgegeben wird uns nur TRUE oder FALSE. Die für uns interessanten Funktionen hierfür heißen is.numeric(), is.character(), is.logical(), is.factor(), is.Date(), is.POSIXct() und is.na(). Natürlich gibt es auch is.double() und is.integer(), allerdings genügt uns für die Anwendungen in diesem Buch is.numeric(). Möchte man generell herausfinden, mit welchem Datentyp man es zu tun hat, verwendet man typeof(). typeof(&quot;Kontrollgruppe&quot;) [1] &quot;character&quot; Da es unpraktisch ist, bei diversen Spalten eines Datensatzes einzeln den Typ abzufragen, wird dieser in tibbles (siehe Kapitel 11.3) – dem Datensatzformat, welches wir innerhalb von R konsistent im gesamten Buch verwenden – direkt unter dem Spaltennamen angezeigt. big5 # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # ℹ 196 more rows Da dort nur die Spalten angezeigt werden, die auf den Bildschirm passen, bietet glimpse() eine übersichtlichere Möglichkeit, einen schnellen Überblick über sämtliche Datentypen zu erhalten. glimpse(big5) Rows: 200 Columns: 7 $ Alter &lt;dbl&gt; 36, 30, 23, 54, 24, 14, 32, 20, 29, 17, 30, 15, 14, 23, 27, 15, 1964, 2… $ Geschlecht &lt;chr&gt; &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;… $ Extraversion &lt;dbl&gt; 3.0, 3.1, 3.4, 3.3, 3.0, 2.8, 3.5, 3.5, 3.0, 3.1, 3.2, 3.5, 3.0, 3.2, 2… $ Neurotizismus &lt;dbl&gt; 1.9, 3.4, 2.4, 4.2, 2.8, 3.5, 3.1, 2.6, 3.7, 3.6, 3.6, 2.8, 3.8, 2.0, 3… $ O1 &lt;dbl&gt; 5, 5, 3, 2, 5, 5, 3, 2, 4, 4, 5, 4, 2, 5, 4, 2, 5, 3, 5, 5, 3, 4, 2, 3,… $ O2 &lt;dbl&gt; 1, 3, 3, 5, 1, 1, 1, 1, 1, 3, 2, 3, 3, 1, 2, 3, 1, 2, 4, 1, 1, 1, 4, 3,… $ O3 &lt;dbl&gt; 5, 5, 5, 3, 5, 5, 5, 3, 5, 4, 5, 4, 3, 5, 4, 5, 5, 5, 4, 4, 1, 5, 3, 2,… Möchten wir nun mehrere Werte eines Datentypens aneinanderreihen, um sie zum Beispiel in eine Spalte eines Datensatzes zu schreiben, können wir c() (Abkürzung für Combine, engl. für kombinieren) verwenden. Möchtest du beispielsweise die Werte 1, 4, 5, und 10 kombinieren, erstellt dir c() einen entsprechenden Vektor (siehe Kapitel 11.1). Die Feinheiten und Merkmale von Vektoren brauchen dich an dieser Stelle nicht zu interessieren. Allerdings brauchen wir die Funktion c() des Öfteren, um Werte aneinander zu reihen. vec &lt;- c(1, 4, 5, 10) Wenn du verschiedene Datentypen innerhalb von c() miteinander kombinierst, werden die Datentypen ineinander umgewandelt. Dabei gilt Character &gt; Integer &gt; Logical. Wenn Zahlen mit Buchstaben kombiniert werden, wird also alles zu Buchstaben, auch wenn eigentlich Zahlen gemeint sind. Es gibt diverse Datentypen, die auf diesen vier Grundtypen aufbauen. Zwei wichtige, Faktoren und Zeitdaten, werden wir uns im Folgenden noch anschauen. Es kann passieren, dass Zahlen von R als Buchstaben interpretiert werden. Das kommt vor allem beim Einlesen von schlecht formatierten Datensätzen vor. Wenn eine Berechnung nicht so funktioniert, wie sie sollte, lohnt es sich, die Datentypen der jeweiligen Spalten zu überprüfen. Übung 4.3.1. (Noch nicht enthalten) Starte die Übung mit uebung_starten(4.3.1). 4.3.2 Faktoren Wenn wir wissen, wie viele Ausprägungen Characters (Zeichenketten) annehmen können, verwenden wir sogenannte Faktoren. Dabei handelt es sich um nominal skalierte Kategorien. Die einzelnen Kategorien sind folglich voneinander abgrenzbar, aber nicht zwingend in eine Reihenfolge zu bringen. Ein illustratives Beispiel hierfür wäre die Aufteilung in Experimental- und Kontrollgruppen. Bereits bei der Versuchsplanung überlegst du, wie viele Gruppen du benötigst (bspw. eine Experimental- und zwei Kontrollgruppen), um eine entsprechende Stichprobenplanung durchzuführen. Schauen wir uns das ganze nun etwas konkreter an. Angenommen, die Information über die Bedingung (Experimental, Kontrolle) befindet sich in der Variable namens Bedingung als Characters. bedingung &lt;- c(&quot;exp&quot;, &quot;kont1&quot;, &quot;exp&quot;, &quot;exp&quot;, &quot;kont2&quot;, &quot;kont1&quot;) Mit der Funktion factor() können nun Faktoren daraus gemacht werden. Zusätzlich sollte man das optionale levels Argument verwenden, um die Reihenfolge der Faktorstufen festzulegen. factor(bedingung, levels = c(&quot;exp&quot;, &quot;kont1&quot;, &quot;kont2&quot;)) [1] exp kont1 exp exp kont2 kont1 Levels: exp kont1 kont2 Das levels Argument ist außerdem wichtig, wenn wir nicht alle Faktorstufen beobachtet haben. Wenn beispielsweise eine der Gruppen in der Erhebung nicht vorkam, möchten wir eine Häufigkeit von 0 angezeigt bekommen. Würden wir die Faktorstufen nicht explizit definieren, würde die fehlende Gruppe in den im weiteren Verlauf des Buches verwendeten Funktionen schlichtweg nicht angezeigt werden. bedingung &lt;- c(&quot;exp&quot;, &quot;kont1&quot;, &quot;exp&quot;, &quot;exp&quot;, &quot;kont1&quot;) factor(bedingung, levels = c(&quot;exp&quot;, &quot;kont1&quot;, &quot;kont2&quot;)) [1] exp kont1 exp exp kont1 Levels: exp kont1 kont2 Die Ausgabe der Funktion zeigt uns, dass die Stufe kont2 vorliegen könnte und innerhalb des Faktors gespeichert ist. Wenn die als Faktor zu kodierende Spalte numerisch ist, können mit dem Argument labels die Namen der verschiedenen Ausprägungsgrade spezifiziert werden. Ein häufiges Beispiel hierfür wäre die Variable Geschlecht mit drei Ausprägungsgeraden. geschlecht &lt;- c(1, 1, 2, 3, 1, 3) Die Umkodierung geht mit dem labels Argument intuitiv, sofern man auf die Reihenfolge der Labels achtet. factor(geschlecht, labels = c(&quot;m&quot;, &quot;f&quot;, &quot;d&quot;)) [1] m m f d m d Levels: m f d Mithilfe des zusätzlich Arguments ordered ist es möglich, aus den nominalen Kategorien ein ordinales Skalenniveau zu bilden. Dies wäre z.B. bei der Untersuchung möglicher Zusammenhänge zwischen Schmerzintensität und verschiedenen Behandlungsmethoden notwendig (siehe Kapitel 9.5.4). Schließlich ist auf einer Schmerzskala von 0 (keine Schmerzen) bis 5 (unerträgliche Schmerzen) ein Schmerz von 3 als höher zu bewerten als 2. Da dennoch nicht bekannt ist, wie groß der Abstand zwischen den Stufen der Schmerzskala genau ist, haben wir hier ordinales Skalenniveau vorliegen (in Abgrenzung zu intervallskalierten Merkmalen). Angenommen, wir befragen fünf Personen über ihre momentan Schmerzen. Standardmäßig sind Faktoren nominal aufsteigend nach Größe sortiert. factor(c(0, 3, 1, 5, 4)) [1] 0 3 1 5 4 Levels: 0 1 3 4 5 Durch das ordered Argument erhalten diese eine explizite Reihenfolge. factor(c(0, 3, 1, 5, 4), ordered = TRUE) [1] 0 3 1 5 4 Levels: 0 &lt; 1 &lt; 3 &lt; 4 &lt; 5 Grundsätzlich sollte man Faktoren nur bei Bedarf erstellen (zum Beispiel unmittelbar vor der ANOVA oder vor Erstellen einer Abbildung), da Faktoren nicht mit allen Funktionen erwartungsgemäß harmonieren. Dies hängt damit zusammen, dass Faktoren als Zahlen gespeichert werden. So erhalten wir für den Faktor fact1 als class() den Datentyp des Faktors, fct1 &lt;- factor(bedingung, levels = c(&quot;exp&quot;, &quot;kont1&quot;, &quot;kont2&quot;)) class(fct1) [1] &quot;factor&quot; während uns typeof() den Datentyp Integer zurückgibt. typeof(fct1) [1] &quot;integer&quot; Tatsächlich behandelt R Faktoren als Integer (Zahlen), was zu überraschenden Outputs führen kann. Verwende Faktoren also am besten nur dann, wenn du sie wirklich brauchst. Beispielsweise zum Rechnen inferenzstatistischer Verfahren oder unmittelbar vor dem Erstellen von Visualisierungen. Wie man mit Faktoren konkret umgehen kann, wird in Kapitel 6.10 erklärt. Übung 4.3.2. (Noch nicht enthalten) Starte die Übung mit uebung_starten(4.3.2). 4.3.3 Zeitdaten Wie bereits in Abbildung 4.6 illustriert, interessieren uns die Datentypen Date und POSIXct. Letzteres ist im Regelfall ein unerwünschtes Format, welches häufig beim Einlesen von Excel Dokumenten entsteht. Der Datentyp POSIXct ist ein Akronym für Portable Operating System Inferface calendar time (in der Ausgabe von tibbles und der Funktion glimpse() mit dttm für Datetime abgekürzt). Enthalten ist das Datum (Jahr.Monat.Tag) und die Uhrzeit. Die Uhrzeit ist allerdings in den wenigstens Forschungskontexten von Interesse. date_time &lt;- as.POSIXct(&quot;2024-01-09 08:30&quot;) date_time [1] &quot;2024-01-09 08:30:00 CET&quot; In das gewünschte Date Format können wir mit as.Date() umwandeln. as.Date(date_time) [1] &quot;2024-01-09&quot; Hier haben wir nur Informationen über das Jahr, den Monat und den Tag. Wenn wir ein Datum ausgeben, sieht es zunächst wie eine Buchstabenfolge aus. date_past &lt;- as.Date(&quot;2002-02-15&quot;) date_past [1] &quot;2002-02-15&quot; Wir können uns aber mit is.Date() vergewissern, dass wir den Datentype Date vorliegen haben. is.Date(date_past) [1] TRUE Ähnlich wie bei Faktoren müssen wir auch hier zum Verständnis zwischen der Klasse des Datums class(date_past) [1] &quot;Date&quot; und dem Datentyp unterscheiden. typeof(date_past) [1] &quot;double&quot; Der zugrundeliegende Datentyp des Datums ist eine Dezimalzahl (Double). Intern speichert R das Datum als Anzahl von Tagen seit einem bestimmten Datum ab. Meistens ist dieses Datum der 1. Januar 1900. Von diesem Datum bis zum 15. Februar 2002 sind 37300 Tage vergangen. Überprüfen können wir das durch Subtrahieren der beiden Daten. date_past - as.Date(&quot;1900-01-01&quot;) Time difference of 37300 days Dieses Format ist eine sogenannte difftime (engl. für Zeitdifferenz). Es passiert tatsächlich öfter als man denkt, dass das Datum nicht als Datum sondern als Zahl angezeigt wird. Dann können wir unter Angabe des Startdatums (origin), die Zahl wieder in ein Datum umwandeln. as.Date(37300, origin = &quot;1900-01-01&quot;) [1] &quot;2002-02-15&quot; Die Gründe hierfür können vielfältig sein. Wenn man bspw. bei unbekanntem Tag des Datums \"UN.06.2022\" in eine Zelle einträgt. Beim Einlesen kann die Spalte dann nicht als Datum eingelesen werden. Dies zu korrigieren, kann unter Umständen sehr mühsam sein. Einige Methoden zum konkreten Umgang mit diesen und weiteren Problemstellungen im Kontext von Zeitdaten wird in Kapitel 6.11 erläutert. Übung 4.3.3. (Noch nicht enthalten) Starte die Übung mit uebung_starten(4.3.3). 4.3.4 Datentypen konvertieren Datentypen können auch umgewandelt werden. Die Namen sind ähnlich wie beim Abfragen des Datentypes, nur dass der Präfix hier nicht is, sondern as ist. Um den vorhin erstellten Vektor vec in den Typ Character umzuwandeln, würde man dementsprechend as.character() verwenden. ` as.character(vec) [1] &quot;1&quot; &quot;4&quot; &quot;5&quot; &quot;10&quot; Wie man sieht, stehen nun sämtliche Zahlen in Anführungszeichen, weswegen man zum Beispiel nicht mehr eine Zahl ohne Weiteres addieren könnte. as.numeric() as.character() as.factor() as.Date() Beachte hier, dass nur bei Date ein Großbuchstabe verwendet wird. Wenn leere Character in Numerics umgewandelt werden, generiert R automatisch fehlende Werte (NAs). 4.4 Struktur von Datensätzen Bis auf wenige Ausnahmen setzen sämtliche im Buch verwendete Funktionen eine bestimmte Struktur der Daten voraus: Jede Zeile bezieht sich auf eine Beobachtung (z.B. jede Zeile enthält die Werte von einem Proband oder einer Patientin). Jede Spalte enthält eine Variable (z.B. Alter, Familienstand oder Überlebenszeit). Jede Zelle beinhaltet einen einzigen Wert, der genau einer Variable einer Beobachtung zuzuordnen ist. Wenn diese Voraussetzungen erfüllt sind, spricht man auch von einem tidy (engl. für aufgeräumten) Datensatz. Achte also am besten bereits beim Erheben der Daten auf eine richtige Struktur, da sonst die Funktionen, die wir in Kapitel 6 kennenlernen werden, nicht ohne Weiteres anwendbar sind. Als ein Praxisbeispiel betrachten wir den big5 Datensatz. Darin entspricht jede Zeile einem Proband oder einer Probandin, der oder die einen Fragebogen zu den Big5 Persönlichkeitsfaktoren beantwortet haben. Die Spalten sind dabei eine Mischung aus demographischen Variablen (z.B. Alter) und den Werten der eigentlichen Fragen (z.B. O1 für die erste Frage der Dimension Offenheit für neue Erfahrungen). big5 # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # ℹ 196 more rows 4.5 Der Dollar-Operator Manche Funktionen benötigen jedoch nicht den gesamten Datensatz im vorherigen Kapitel beschriebenen Format als Argument, sondern lediglich eine Spalte daraus. Diese Spalte können wir mit dem sogenannten Dollar-Operator aus dem Datensatz extrahieren. Das Praktische an dem Dollar-Operator im Vergleich zu anderen Methoden zum Extrahieren der Spalte ist die Möglichkeit der automatischen Vervollständigung durch RStudio. Wenn wir den Namen des Datensatzes big5 gefolgt von einem Dollar-Zeichen angeben, werden uns alle enthaltenen Spalten in einem Dropdown-Menu angezeigt. Dies ist vor allem bei langen Namen der jeweiligen Spalten äußerst praktisch und schützt uns vor Tippfehlern. Hier wählen wir exemplarisch die Spalte Alter aus. big5$Alter Dieser Befehl würde uns alle 200 Alterswerte in einer Wertereihe bzw. als sogenannter Vektor zurückgegeben werden (siehe Kapitel 11.1). Um das ganze übersichtlich zu halten, lassen wir uns mit head() nur das Alter der ersten 6 Personen ausgeben. head(big5$Alter) [1] 36 30 23 54 24 14 Als zweites Beispiel schauen wir uns den demogr Datensatz an, in welchem die drei Spalten ID, Sex und Alter enthalten sind. demogr # A tibble: 4 × 3 ID Sex Alter &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 AX161095 m 28 2 NM020683 f 47 3 IO240576 f 40 4 JH051199 m 24 Die einzelnen Angaben über das biologische Geschlecht der Personen innerhalb des Datensatzes, werden erneut mit der Kombination aus Datensatznamen, Dollar-Zeichen und Spaltennamen als Aneinanderreihung von Werten extrahiert. demogr$Sex [1] &quot;m&quot; &quot;f&quot; &quot;f&quot; &quot;m&quot; Nach demselben Prinzip können wir auch die Personenkennungen aus der Spalte ID ausgeben lassen. Auch hier werden nur vier Werte zurückgegeben, da im demogr Datensatz die Informationen von vier Personen gespeichert sind. demogr$ID [1] &quot;AX161095&quot; &quot;NM020683&quot; &quot;IO240576&quot; &quot;JH051199&quot; Übung 4.5. (Noch nicht enthalten) Starte die Übung mit uebung_starten(4.5). "],["io.html", "Kapitel 5 Datensätze 5.1 Einlesen externer Dateien 5.2 Datensätze aus Packages laden 5.3 Datensätze in R erstellen 5.4 Speichern und Konvertieren", " Kapitel 5 Datensätze Mit einer Funktion alle erdenklichen Dateiformate einzulesen, klingt fast zu schön, um wahr zu sein. Durch ein Package ist dies in R tatsächlich bereits lange Realität. Neben den Einlesen externer Datensätze wird außerdem gezeigt, wie man Datensätze aus Packages laden oder direkt in R erstellen kann. Auch das Abspeichern der in R modifizierten Daten wird abschließend erklärt. 5.1 Einlesen externer Dateien Weißt du, was Projekte sind und kannst diese innerhalb von RStudio erstellen? Wenn nicht, lies dir das Konzept der Projektorientierung genau durch (siehe Kapitel 3.3). Ansonsten kann R die Datei, die deinen Datensatz enthält, nicht finden und somit auch nicht einlesen. Es muss also immer zunächst ein neues R Projekt erstellt werden. Der Datensatz muss sich dabei im gleichen Ordner wie die Projektdatei befinden. Im Regelfall möchte man den Datensatz nicht direkt in R erstellen, sondern einen bereits existierenden Datensatz zur Auswertung einlesen. Datensätze können dabei in verschiedenen Formaten vorliegen. Dies ist vor allem abhängig davon, mit welchen Programmen Unternehmen, Universitäten oder KollegInnen zur Datenerhebung arbeiten. Einige Beispiele für Dateientypen, in denen Daten häufig gespeichert werden, sind: R (.RData | .rda | .rds) Excel (.xlsx | .xls) SPSS (.sav) Stata (.dta) Tabular separated values (.tsv) Comma separated values (.csv) mit Punkt als Dezimaltrenner oder mit Komma (siehe Infobox) Sämtliche Datentypen können wir mit import() aus dem rio Package einlesen. Wir laden also zunächst das Package. library(rio) Um auch exotischere Dateiformate einlesen zu können, solltest du einmal den Befehl install_formats() ausführen. Es ist generell empfohlen, diese Funktion einmalig auszuführen, da sonst jedes Mal beim Laden des Packages eine entsprechende Meldung angezeigt wird. Wir möchten an dieser Stelle einen Datensatz über die Big 5 Persönlichkeitsfaktoren einlesen, welcher in Form eines Excel Dokument auf dem Computer gespeichert ist (siehe Abbildung 5.1). Abbildung 5.1: Big Five Datensatz als Excel Dokument. Beim Einlesen erkennt die Funktion import() die Art der Datei anhand der Endung und übernimmt hinter den Kulissen alles Weitere. Damit der Datensatz als sogenannter tibble eingelesen wird, solltest du zusätzlich das Argument setclass = \"tbl\" setzen. Weshalb wir die Sonderform der tibbles verwenden und was dies genau ist, wird im Verlaufe des Buches klar. Für den Moment musst du dir beim setclass Argument allerdings noch nichts denken, es aber trotzdem verwenden. Würden wir nur die import() Funktion aufrufen, würde der Datensatz zwar eingelesen, aber sofort wieder verschwinden, da dieser in R nicht abgespeichert wäre. Daher müssen wir den eingelesenen Datensatz, so wie in Kapitel Kapitel 4.2 beschrieben, einer Variable zuordnen. Der Name dieser Variable ist dabei nicht wichtig. Falls mit mehr als einem Datensatz gearbeitet wird, sollte ein aussagekräftigerer Name als der hier verwendete Name daten verwendet werden. Der Name des Datensatzes muss dabei in Anführungszeichen gesetzt werden. In unserem Beispiel heißt das Excel Dokument big_five.xlsx. Damit wir den Datensatz in R angezeigt bekommen, müssen wir den Namen des gespeicherten Datensatzes anschließend erneut ausführen (siehe Kapitel 4.2). Dabei wird der Variablenname (hier daten) und nicht der Name der eigentlichen Datei (hier big_five.xlsx) verwendet. daten &lt;- import(&quot;big_five.xlsx&quot;, setclass = &quot;tbl&quot;) daten # A tibble: 200 × 16 Alter Geschlecht Extraversion Neurotizismus Vertraeglichkeit Gewissenhaftigkeit O1 O2 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 3.4 3.3 5 1 2 30 f 3.1 3.4 3.1 2.7 5 3 3 23 m 3.4 2.4 3.6 3 3 3 4 54 m 3.3 4.2 3.6 3 2 5 # ℹ 196 more rows # ℹ 8 more variables: O3 &lt;dbl&gt;, O4 &lt;dbl&gt;, O5 &lt;dbl&gt;, O6 &lt;dbl&gt;, O7 &lt;dbl&gt;, O8 &lt;dbl&gt;, O9 &lt;dbl&gt;, # O10 &lt;dbl&gt; Die Datei (hier das Excel Dokument) muss sich innerhalb desselben Ordners wie die Projektdatei befinden. Falls die Datei in einem Unterordner ist, muss man den relativen Pfad – also den Weg bis zur Datei innerhalb der Unterordner (hier namens Daten) des Ordners (hier namens Beispiel) – zusätzlich der R Funktion mitteilen. Abbildung 5.2: Beispielhafte Ordnerstruktur mit Unterordnern für Datensätze, R Skript und Projektdatei. Dabei nutzen wir die Funktion here() aus dem gleichnamigen Package. Dort können wir als separate Argumente den gesamten relativen Pfad eintragen. In diesem Beispiel liegt innerhalb des Ordners Daten der Datensatz indonesisch.xlsx. daten &lt;- import( file = here(&quot;Daten&quot;, &quot;indonesisch.xlsx&quot;), setclass = &quot;tbl&quot; ) Manchmal liegen Daten jedoch nicht in einer, sondern in vielen verschiedenen Dateien vor. Das ist vor allem häufig bei biophysiologischen Messungen wie beim Eye Tracking der Fall, bei denen die erhobenen Daten pro Person abgespeichert werden. Da wir nicht 20 Mal import() kopieren möchten (da Copy &amp; Paste sehr fehleranfällig ist), gibt es die Funktion import_list(). Angenommen, im Ordner Daten wären unsere 20 Excel Dokumente, in denen jeweils die Daten pro Person liegen. Dann könnte man zuerst mit dir() die Dateinamen herausfinden, um diese dann mit import_list() einzulesen. Mit dem Zusatzargument rbind = TRUE können wir die Datensätze direkt zusammenfügen. Voraussetzung dafür ist natürlich, dass die Datensätze dieselben Spalten haben. Da die Dateien in einem Unterordner liegen, müssen wir import_list() zusätzlich mit der Funktion here() mitteilen, wo sich die Datensätze befinden. dateien &lt;- dir(&quot;Daten&quot;, pattern = &quot;.xlsx$&quot;) daten &lt;- import_list( file = here(&quot;Daten&quot;, dateien), setclass = &quot;tbl&quot;, rbind = TRUE ) Das Dollar-Zeichen signalisiert in diesem Fall nur, dass wir am Ende des Dateinamens die Endung .xlsx (also ein Excel Dokument) erwarten. Dasselbe funktioniert übrigens für den Fall, verschiedene Excel Sheets innerhalb eines Excel Workbooks zu haben. Mit import_list() können alle auf einen Schlag eingelesen werden. Die in diesem Kapitel kennengelernten Funktionen können wir praktisch für alle Dateienarten verwendet werden. Falls also anders als in unserem Beispiel die Daten nicht in einem Excel Dokument, sondern einer Datei mit der Endung .csv enthalten sind, müssen wir lediglich die Endung verändern (z.B. big_five.csv). Im deutschsprachigen Raum werden Dezimalzahlen meistens durch ein Komma anstelle eines Punktes getrennt. Dies ist vor allem im Kontext von CSV Dateien problematisch, da hier in allen anderen Teilen der Welt das Komma zur Trennung der einzelnen Spalten bzw. Variablen verwendet wird. Um trotzdem ein fehlerfreies Einlesen gewährleisten zu können, benötigen wir innerhalb der Funktion import() das zusätzliche Argument format = \"csv2\". Der vollständige Befehl könnte also wie folgt aussehen: import(\"big_five.csv\", setclass = \"tbl\", format = \"csv2\"). 5.2 Datensätze aus Packages laden Zum Bearbeiten dieses Buches brauchst du keine externen Datensätze, sondern nur jene, die im remp Package enthalten sind. So kannst du das Gelernte sofort begleitend in RStudio nachvollziehen und ausprobieren. Nach Laden des Packages hast du grundsätzlich sofort Zugriff auf alle enthaltenden Datensätze, allerdings werden diese erst nach Aufrufen der Funktion data() in R gespeichert. Zum Einlesen des big5 Datensatzes, müsste man demnach lediglich den Namen des Datensatzes der Funktion data() übergeben. library(remp) data(big5) Sollte das Package vorher nicht geladen sein, kann alternativ auch zusätzlich das Argument package zur Spezifizierung des Ursprungs des Datensatzes verwendet werden. data(big5, package = &quot;remp&quot;) Da der Befehl etwas länger ist und beim Üben das remp Package sowieso geladen werden sollte, ist allerdings der zuvor dargestellte Weg empfohlen. 5.3 Datensätze in R erstellen Direkt innerhalb von R Datensätze zu erstellen, ergibt nur in wenigen Anwendungsfällen wirklich Sinn. Der womöglich Wichtigste ist das Erstellen eines minimalen reproduzierbaren Beispiels, falls man auf einen Fehler stößt, den man selbst nicht lösen kann. Für größere Datensätze sollte man die Daten jedoch besser in Datenformaten wie .csv oder .xlsx kreieren. Möchte man einen Datensatz erstellen, muss man lediglich der Funktion tibble() Werte übergeben. In diesem Beispiel speichern wir den neuen Datensatz mit den zwei Spalten Extraversion und Geschlecht als my_tbl. Die Funktion c() (engl. für combine) kombiniert die einzelnen Werte eines Datentyps und kettet selbige aneinander. my_tbl &lt;- tibble( Extraversion = c(1.2, 2.7, 1.5, 4.8), Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;) ) my_tbl # A tibble: 4 × 2 Extraversion Geschlecht &lt;dbl&gt; &lt;chr&gt; 1 1.2 m 2 2.7 f 3 1.5 f 4 4.8 m 5.4 Speichern und Konvertieren Das Speichern von Datensätzen funktioniert durch das rio Package ähnlich intuitiv wie das Importieren von Dateien. Anstelle von import() benutzen wir dafür stattdessen export(). Das erste Argument der Funktion ist der Datensatzname und das zweite Argument ist der gewünschte Dateiname. Der Dateientyp wird durch die gewählte Endung festgelegt. Möchte man beispielsweise den fertig aufbereiteten Datensatz video_clean als csv Datei abspeichern, würde man export(big_five_clean, &quot;big_five_clean.csv&quot;) schreiben. Manchmal ist es nützlich, den Datensatz unabhängig vom Einlesen umzuwandeln. Das kann zum Beispiel der Fall sein, wenn deine Kollegen dir eine SPSS Datei schicken (.sav) und du selbst kein SPSS hast, aber trotzdem einen Blick in die Daten werfen möchtest. Die Entwickler des rio Packages haben auch daran gedacht und die Funktion convert() geschrieben. Als erstes Argument übergibst du der Funktion den ursprünglichen Dateinamen (mit Endung) und als zweites denselben oder einen anderen Dateinamen mit der Endung des gewünschten Dateientypen. Sinnvoll wäre in diesem Kontext das Umwandeln in ein Excel Dokument, da dieses mit MS oder Libre Office problemlos geöffnet werden kann. convert(&quot;big_five.sav&quot;, &quot;big_five.xlsx&quot;) Die neue Datei wird sowohl bei export() als auch bei convert() in deinem Projektverzeichnis gespeichert. Besonders wichtig ist auch das Abspeichern von kategorealen Variablen als Buchstaben oder Wörter (Datentyp Character) und nicht als Zahlen. Nach spätestens einem Jahr kann kein Mensch mehr nachvollziehen, ob beispielsweise eine 1 nun für weiblich und 0 für männlich oder eine 1 für männlich und eine 0 für weiblich verwendet wurde. # A tibble: 4 × 2 Extraversion Geschlecht &lt;dbl&gt; &lt;dbl&gt; 1 1.2 0 2 2.7 1 3 1.5 1 4 4.8 0 Stattdessen sollte man die Zahlen in Character umwandeln. Wie das geht, wird in Kapitel 6.4.3 gezeigt. # A tibble: 4 × 2 Extraversion Geschlecht &lt;dbl&gt; &lt;chr&gt; 1 1.2 m 2 2.7 f 3 1.5 f 4 4.8 m Stelle vor dem Abspeichern des Datensatzes immer sicher, dass alle Variablen unmissverständlich kodiert sind. Kategorien sollten immer explizit benannt werden. "],["datenvorbereitung.html", "Kapitel 6 Datenvorbereitung 6.1 Einführung 6.2 Spalten auswählen, umbenennen und umsortieren 6.3 Zeilen auswählen und umsortieren 6.4 Spalten hinzufügen und Spalteninhalte verändern 6.5 Umgang mit fehlenden Werten und Duplikaten 6.6 Breites und langes Datenformat 6.7 Spalten trennen 6.8 Datensätze zusammenführen 6.9 Buchstaben und Wörter bearbeiten 6.10 Faktoren verändern 6.11 Mit Zeitdaten arbeiten 6.12 Binäre Antwortmatrix erstellen", " Kapitel 6 Datenvorbereitung Spalten oder Zeilen auswählen und umbenennen, Spalten hinzufügen und entfernen, Funktion erstellen und Spalteninhalte unter Bedingungen verändern, Spalten trennen oder das Zusammenfügen von Datensätzen sind nur ein kleiner Auszug aus den benötigten Werkzeugen für die Aufbereitung und Bereinigung der eigenen Daten. Bis der Datensatz fehlerfrei und bereit zur Auswertung ist, vergehen häufig mehrere Tage oder sogar Wochen des aktiven Bereinigens. 6.1 Einführung Die Datenvorbereitung oder auch Datenaufbereitung ist in der Regel der mit Abstand aufwendigste und zeitintensivste Teil der Datenanalyse. Selten liegt nach der Datenerhebung bereits ein perfekt formatierter vor, den man statistisch auswerten kann. Mit den Funktionen des tidyverse ist die Datenvorbereitung unkompliziert und schneller zu bewerkstelligen. Das tidyverse ist ein Sammelsurium an Packages, die die gleiche Philosophie teilen. Dabei steht die Abkürzung für tidy universe, also einem aufgeräumten Universum. Beim Laden des tidyverse werden neun Packages gemeinsam bereitgestellt, womit man sich im Prinzip nur das einzelne Aufrufen der neun Packages spart. Ausgeführt in R sieht das wie angeführt aus. Unter Conflicts werden Funktionen genannt, die denselben Namen wie andere R Funktionen haben und die von hier an überschrieben werden. library(tidyverse) -- Attaching core tidyverse packages ------------ tidyverse 2.0.0 -- ✔ dplyr 1.1.0 ✔ readr 2.1.4 ✔ forcats 1.0.0 ✔ stringr 1.5.0 ✔ ggplot2 3.4.1 ✔ tibble 3.1.8 ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ✔ purrr 1.0.1 -- Conflicts --------------------- tidyverse_conflicts() -- ✖ dplyr::filter() masks stats::filter() ✖ dplyr::lag() masks stats::lag() ℹ Use the conflicted package to force all conflicts to become errors ggplot2: Erstellen von Visualisierungen (siehe Kapitel 8). tibble: Erweiterung des klassischen data.frames (siehe Kapitel 11.3). tidyr: Wechsel zwischen langem und breitem Datenformat (siehe Kapitel 6.6). readr: Einlesen von Dateien (siehe Kapitel 5). purrr: Wiederholtes Anwenden von Funktionen (siehe Kapitel 12). dplyr: Funktionen zur Datenvorbereitung. stringr: Veränderung von Buchstaben und Wörtern (siehe Kapitel 6.9). forcats: Manipulation von Faktoren (siehe Kapitel 6.10). lubridate: Arbeiten mit Zeitdaten (siehe Kapitel 6.11). Die in diesem Kapitel eingeführten Funktionen zur Datenaufbereitung sind in sich konsistent. Man muss das Prinzip also nur einmal verstehen, um sämtliche Funktionen anwenden zu können. Dabei sind diese durch eine ausdrucksstarke Namensgebung beinahe schon selbsterklärend. Schauen wir uns zunächst eine typische Aneinanderreihung von Befehlen an: big5 |&gt; select(Geschlecht, Extraversion) |&gt; filter(Geschlecht == &quot;m&quot;) |&gt; mutate(Extraversion_lg = log(Extraversion)) # A tibble: 82 × 3 Geschlecht Extraversion Extraversion_lg &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 m 3 1.10 2 m 3.4 1.22 3 m 3.3 1.19 4 m 3.5 1.25 # ℹ 78 more rows Gelesen würde es wie folgt: Man nehme den Datensatz big5 UND DANN wähle die Spalten Geschlecht und Extraversion UND DANN wähle die Zeilen, in denen Geschlecht gleich “m” (für männlich) ist UND DANN mutiere oder verändere die (neue) Spalte Extraversion_lg, wie in diesem Fall beschrieben, durch die logarithmierten Werte der Extraversion. Die anderen Funktionen sind ähnlich intuitiv und nahe an der englischen Sprache benannt. Besonders ist an dieser Stelle die sogenannte Pipe (|&gt;), welche für das Weiterreichen des modifizierten Datensatzes an die nächste Funktion zuständig ist. Der Name des Datensatzes muss nicht in jeder Funktion stehen, da dieser immer das erste Argument der hier behandelten Funktionen darstellt. In dem obigen Beispiel werden zuerst zwei der Spalten ausgewählt. Dann wird das Ergebnis dieses Befehls – also der Datensatz mit den zwei Spalten Geschlecht und Extraversion – im nächsten Schritt der Funktion filter() übergeben. Das Verbindungssymbol |&gt; wird als Pipe bezeichnet. Es kann mit dem Shortcut Strg + Shift + M (bzw. auf macOS mit Cmd + Shift + M) direkt eingefügt werden. Zuerst muss allerdings innerhalb von RStudio unter Tools/Global options .../Code der Haken bei Use native pipe operator, |&gt; (requires R 4.1+) gesetzt werden. Das Verwenden der in R integrierten Pipe (|&gt;) ist erst ab der R Version 4.1.0 möglich. Die Verwendung der Pipe hat zwei große Vorteile: Wir müssen nicht jedes Ergebnis der verschiedenen Funktionen einzeln zwischenspeichern. Die Verschachtelung mehrerer Funktionen ineinander wird verringert. Trotzdem müssen wir das Ergebnis dieser aneinandergeketteten Funktion irgendwann mit dem Zuweisungspfeil speichern. daten &lt;- big5 |&gt; select(Geschlecht, Extraversion) |&gt; filter(Geschlecht == &quot;m&quot;) |&gt; mutate(Extraversion_lg = log(Extraversion)) Ohne Pipe müssten wir beim Anwenden jeder Funktion ein Zwischenergebnis abspeichern, was mühsam und fehleranfällig ist. Im Kontext der Datenvorbereitung müssen wir häufig so viele verschiedene Probleme bereinigen, dass zehn bis zwanzig verschiedene Befehle aneinandergekettet werden. daten1 &lt;- select(daten, Geschlecht, Extraversion) daten2 &lt;- filter(daten1, Geschlecht == &quot;m&quot;) daten3 &lt;- mutate(daten2, Extraversion_lg = log(Extraversion)) Mit Verschachtelung ist eine Kombination mehrerer Funktion innerhalb einer Zeile gemeint. Die ersten beiden Zeilen des vorherigen Beispiels sind äquivalent zu: filter(select(daten, Geschlecht, Extraversion), Geschlecht == &quot;m&quot;) Hier haben wir den Nachteil der erheblich schlechteren Lesbarkeit. Beachte, dass sämtliche Änderungen, die du am Datensatz vollziehst, erst gespeichert werden, wenn du sie mit dem Zuweisungspfeil einer Variable zuweist (siehe Kapitel 4.2). Falls der gewählte Variablenname bereits vergeben ist (z.B. der bisherige Datensatzname), wird dieser überschrieben. Um das rückgängig zu machen, muss der Datensatz dann wieder neu eingelesen werden. Es empfiehlt sich bei einschneidenden Änderungen einen neuen Variablennamen zu wählen. Ein zentrales Konzept ist die sogenannte Pipe (|&gt;), die verschiedenste Funktionsaufrufe aneinanderbinden kann. Dabei wird der modifizierte Datensatz jeweils an die nächste Funktion weitergeben. In diesem Buch verwenden wir die Pipe (|&gt;), welche seit der Version 4.1.0 direkt in R integriert ist. Älter ist die Pipe innerhalb des tidyverse, die mit %&gt;% geschrieben wird. Die Funktionsweise der beiden Operatoren unterscheidet sich für die meisten NutzerInnen nicht nennenswert und langfristig wird die Base R Pipe (|&gt;) weiter verbreitet sein. 6.2 Spalten auswählen, umbenennen und umsortieren Die Funktionen in diesem Kapitel beschäftigen sich mit der Auswahl, Umbenennung und Umordnung von Spalten. Wir haben die Funktion select() bereits im vorherigen Abschnitt kennengelernt. Es können beliebig viele Spalten ausgewählt werden. Dies ist vor allem nützlich, wenn der Datensatz groß ist und man übersichtlich nur die Spalten haben möchte, die zur Auswertung verwendet werden. Zur Auswahl einer Spalte muss nur der Name (ohne Anführungszeichen) der Funktion übergeben werden. Man kann auch direkt in der Funktion die Spalte umbenennen. Dabei muss auf der linken Seite des Gleichheitszeichens der neue Name stehen. big5 |&gt; select(Extraversion, Neuro = Neurotizismus) # A tibble: 200 × 2 Extraversion Neuro &lt;dbl&gt; &lt;dbl&gt; 1 3 1.9 2 3.1 3.4 3 3.4 2.4 4 3.3 4.2 # ℹ 196 more rows Zur Auswahl der Spalten von Extraversion bis O2 verwendet man einen Doppelpunkt. big5 |&gt; select(Extraversion:O2) Soll nur die Spalte Geschlecht entfernt und der Rest ausgegeben werden, erreicht man dies mit einem Minus vor dem Spaltennamen. Bei mehreren zu entfernenden Spalten müsste man diese zwischen Klammern einbetten (z.B. -(Extraversion:O2)). big5 |&gt; select(-Geschlecht) Darüber hinaus können wir sogenannte Helferfunktionen verwenden. Diese können nur in Kombination mit einer anderen Funktion verwendet werden. Ein nützliches Beispiel ist where(), womit beispielsweise alle numerischen Spalten ausgewählt werden können. big5 |&gt; select(where(is.numeric)) Eine weitere nützliche Helferfunktion ist starts_with(). So könnte man in diesem Fall beispielsweise alle Fragen zum Persönlichkeitsfaktor Offenheit auswählen, da diese alle mit dem Buchstaben O beginnen. big5 |&gt; select(starts_with(&quot;O&quot;)) Mit der Helferfunktion ends_with() können auf dieselbe Art und Weise Spalten ausgewählt werden, die mit einem bestimmten Character enden (z.B. eine Spalte endend mit dem Buchstaben A), während die Helferfunktion contains() prüft, ob ein Character im Spaltennamen enthalten ist . Darüber hinaus gibt es noch all_of(), um alle zuvor ausgewählten Variablen (Spaltennamen als Character) auszuwählen. Möchte man hingegen die Spalten nur umbenennen und dabei den gesamten Datensatz behalten, verwendet man anstelle von select() die Funktion rename(). Die Schreibweise der Argumente beim Umbenennen bleibt dabei im Vergleich zu select() gleich. big5 |&gt; rename(Sex = Geschlecht) # A tibble: 200 × 7 Alter Sex Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # ℹ 196 more rows Während beide Funktionen Spalten umbenennen können, gibt select() nur die ausgewählten Spalten und rename() hingegen alle Spalten zurück. Außerdem können Funktionen zur Umbenennung von Spalten verwendet werden. Dafür müssen wir rename_with() einfach nur die gewünschte Funktion (ohne Klammern) übergeben. In diesem Beispiel werden alle Buchstaben der Spaltennamen in Großbuchstaben umgewandelt. Alternativ könnten auch neue anonyme Funktion direkt innerhalb von rename_with() erstellt werden (siehe Kapitel 6.4.4). big5 |&gt; rename_with(toupper) # A tibble: 200 × 7 ALTER GESCHLECHT EXTRAVERSION NEUROTIZISMUS O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 30 f 3.1 3.4 5 3 5 3 23 m 3.4 2.4 3 3 5 4 54 m 3.3 4.2 2 5 3 # ℹ 196 more rows Mit dem zusätzlichen Argument .cols könnte man die Umbenennung nur auf einen Teil der Spalten anwenden. Hierbei können wir ebenfalls die zuvor kennengelernten Helferfunktionen verwenden. Möchten wir bspw. nur jene Spaltennamen großschreiben, die mit einem A beginnen, könnten wir dies mit rename_with(.fn = toupper, .cols = starts_with(\"A\")) erreichen. Bei sehr großen Datensätzen mit vielen Spalten ist die Funktion relocate() äußerst nützlich. Eine neue Spalte wird zum Beispiel immer ans Ende des Datensatzes angefügt. Um diese trotzdem direkt am Anfang betrachten zu können, übergeben wir den Spaltennamen unserer Funktion. big5 |&gt; relocate(O1) # A tibble: 200 × 7 O1 Alter Geschlecht Extraversion Neurotizismus O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 36 m 3 1.9 1 5 2 5 30 f 3.1 3.4 3 5 3 3 23 m 3.4 2.4 3 5 4 2 54 m 3.3 4.2 5 3 # ℹ 196 more rows Wenn die Spalte nicht zu Beginn, sondern nach einer bestimmten anderen Spalte eingeordnet werden soll, können wir dies mit dem .after Argument festlegen. Hier würde die Spalte O1 hinter der Spalte Alter ausgegeben werden. big5 |&gt; relocate(O1, .after = Alter) Auch hier können wir wieder Helferfunktionen wie where() verwenden. Man könnte beispielsweise alle numerischen Spalten hinter alle character Spalten einfügen. big5 |&gt; relocate(where(is.numeric), .after = where(is.character)) Eine weitere nützliche Funktion bei sehr breiten Datensätzen mit vielen Spalten ist colnames(). So können wir auf einem Blick alle Spaltennamen ausgegeben bekommen. colnames(big5) [1] &quot;Alter&quot; &quot;Geschlecht&quot; &quot;Extraversion&quot; &quot;Neurotizismus&quot; &quot;O1&quot; [6] &quot;O2&quot; &quot;O3&quot; Von Zeilennamen (rownames()) sollte hingegen grundsätzlich Abstand genommen werden. Falls der Datensatz Zeilennamen enthält, die tatsächlich von Bedeutung sind, sollte man diese mit der Funktion rownames_to_column(\"Spaltenname\") aus dem tibble Package in eine eigene Spalte befördern. Übung 6.2. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.2). 6.3 Zeilen auswählen und umsortieren Anders als im vorherigen Kapitel beschäftigen sich diese beiden Funktionen mit der Auswahl und Umordnung von Zeilen. Der Funktion filter() muss dabei ein logischer Ausdruck übergeben werden. Das Ergebnis der Abfrage muss also immer TRUE oder FALSE zurückgeben (siehe Kapitel 4.3). Zur Auswahl aller männlichen Probanden würde man Geschlecht == \"m\" schreiben. Beachte an dieser Stelle das doppelte Gleichheitszeichen. big5 |&gt; filter(Geschlecht == &quot;m&quot;) # A tibble: 82 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 2 23 m 3.4 2.4 3 3 5 3 54 m 3.3 4.2 2 5 3 4 32 m 3.5 3.1 3 1 5 # ℹ 78 more rows Um Zeilen neu anzuordnen, benutzt man arrange(). Wenn die Zeilen nach aufsteigendem Alter sortiert werden sollen, muss man lediglich den Spaltennamen übergeben. big5 |&gt; arrange(Alter) Für eine absteigende Anordnung muss der Spaltennamen innerhalb der Helferfunktion desc() geschrieben werden. big5 |&gt; arrange(desc(Alter)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1995 f 2.5 3.7 4 1 4 2 1964 f 3.2 2.3 5 1 5 3 60 f 3 2.7 5 1 5 4 59 m 2.7 2.3 5 1 5 # ℹ 196 more rows So sehen wir hier beispielsweise zwei falsch eingetragene Alterswerte. Hier haben zwei Probandinnen nicht das Alter sondern das jeweilige Geburtsjahr in den Datensatz eingetragen. Das müsste vor einer Auswertung noch entsprechend korrigiert werden. Übung 6.3. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.3). 6.4 Spalten hinzufügen und Spalteninhalte verändern 6.4.1 Einzelne Spalten Die Funktion mutate() ist eine sehr vielseitig einsetzbare Funktion zum Verändern bestehender oder zum Hinzufügen neuer Spalten. Dabei wird der neue oder bereits bestehende Spaltenname auf die linke Seite des Gleichheitszeichens geschrieben. Auf der rechten Seite kann so ziemlich alles stehen, solange die Funktion eine Spalte zurückgibt, die genauso lang ist wie der Datensatz. Einen Mittelwert über eine ganze Spalte könnte mit mutate() also nicht berechnet werden, da dabei nur ein einziger Wert zurückgeben werden würde. Mit der Funktion log() logarithmieren wir hingegen die Extraversionsausprägungen für jede Person, sodass wir 200 Werte erhalten. big5 |&gt; mutate(Extraversion_lg = log(Extraversion)) Standardmäßig wird die neue erstellte Spalte hinten als letzte Spalte zum Datensatz hinzugefügt. Möchte man die neue Spalte an einer anderen Position haben, können wir dies mit dem zusätzlichen Argument .after erreichen. So könnte man die neue Spalte namens Extraversion_lg bspw. direkt hinter der Spalte Extraversion einfügen. Dem .after Argument können ebenfalls die in Kapitel 6.2 eingeführten Helferfunktion wie starts_with() und ends_with() übergeben werden. big5 |&gt; mutate(Extraversion_lg = log(Extraversion), .after = Extraversion) # A tibble: 200 × 8 Alter Geschlecht Extraversion Extraversion_lg Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.10 1.9 5 1 5 2 30 f 3.1 1.13 3.4 5 3 5 3 23 m 3.4 1.22 2.4 3 3 5 4 54 m 3.3 1.19 4.2 2 5 3 # ℹ 196 more rows Möchten wir die bestehende Spalte Extraversion mit den logarithmierten Werten überschreiben, wählen wir auf der linken Seite des Gleichheitszeichens ebenfalls den Spaltennamen Extraversion. Es wird folglich keine neue Spalte hinzugefügt, sondern nur eine bestehende verändert. big5 |&gt; mutate(Extraversion = log(Extraversion)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 1.10 1.9 5 1 5 2 30 f 1.13 3.4 5 3 5 3 23 m 1.22 2.4 3 3 5 4 54 m 1.19 4.2 2 5 3 # ℹ 196 more rows Innerhalb eines mutate() Aufrufes können wir auch mehrere Spalten einzeln neu erstellen oder verändern. Die verschiedenen Spalten müssen dabei lediglich mit einem Komma voneinander getrennt werden. Hier logarithmieren wir exemplarisch die mittlere Ausprägung von Extraversion und Neurotizismus. Der Abstand der öffnenden Klammer oben und der schließenden Klammer unten ist aus funktioneller Sicht nicht relevant. Es ist allerdings im Sinne der Lesbarkeit bei Benutzung mehrerer Argumenten unter Umständen sinnvoller, die Argumente auf mehrere Zeilen zu verteilen. Um unsere Berechnung überprüfen zu können, bringen wir noch unsere neuen mit lg endenden Spalten an den Anfang des Datensatzes (siehe Kapitel 6.2). Dies ist vor allem beim reinen Überprüfen nützlich, da der relocate() Aufruf im Anschluss übersichtlich wieder gelöscht werden kann. Möchte man die Anordnung dauerhaft verändern, ist das zuvor erwähnte Argument .after zu bevorzugen. big5 |&gt; mutate( Extraversion_lg = log(Extraversion), Neurotizismus_lg = log(Neurotizismus) ) |&gt; relocate(ends_with(&quot;lg&quot;)) # A tibble: 200 × 9 Extraversion_lg Neurotizismus_lg Alter Geschlecht Extraversion Neurotizismus O1 O2 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.10 0.642 36 m 3 1.9 5 1 2 1.13 1.22 30 f 3.1 3.4 5 3 3 1.22 0.875 23 m 3.4 2.4 3 3 4 1.19 1.44 54 m 3.3 4.2 2 5 # ℹ 196 more rows # ℹ 1 more variable: O3 &lt;dbl&gt; Übung 6.4.1. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.4.1). 6.4.2 Mehrere Spalten Wenn man eine Funktion auf mehrere Spalten anwenden möchte, kann man die Berechnung, wie im vorherigen Kapitel gezeigt, entweder für jede Spalte separat vornehmen oder mithilfe von across() (engl. für herüber) den Prozess abkürzen. Wir wollen schließlich eine Funktion über mehrere Spalten anwenden. Innerhalb von across() erfolgt die Auswahl der Spalten dabei genau wie in select() (siehe Kapitel 6.2). So kann bspw. der Doppelpunkt zur Auswahl eines mehrere Spalten umfassenden Bereiches oder c() zur Auswahl einzelner Spalten verwendet werden. Wichtig ist das Setzen der Klammern an der richtigen Stelle, da die Funktion log() innerhalb von across() aufgerufen wird. big5 |&gt; mutate(across( .cols = Extraversion:Neurotizismus, .fns = log, .names = &quot;{.col}_lg&quot;) ) # A tibble: 200 × 9 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 Extraversion_lg &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 5 1 5 1.10 2 30 f 3.1 3.4 5 3 5 1.13 3 23 m 3.4 2.4 3 3 5 1.22 4 54 m 3.3 4.2 2 5 3 1.19 # ℹ 196 more rows # ℹ 1 more variable: Neurotizismus_lg &lt;dbl&gt; Das .names Argument verhindert dabei das Überschreiben der Spalten. Innerhalb der geschweiften Klammern wird hier der Spaltenname für jede in .cols gewählten Spalte nacheinander übergeben und die Endung _lg angehängt. Würden wir den Namen der neu zu erstellenden Spalten nicht explizit mit .names definieren, würden die ursprünglichen Spalten Extraversion und Neurotizismus mit den logarithmierten Werten überschrieben werden. Die eigentlichen Werte wären also nach Abspeichern dieses Zwischenergebnisses nicht mehr im Datensatz enthalten, sondern nur noch die Spalten mit den logarithmierten Werten. Die auf mehrere Spalten anzuwendende Funktion muss innerhalb von across() übergeben werden. Falls ein Fehler auftritt, ist dieser in der Regel auf falsche Positionierung der Klammern zurückzuführen. Ein weiterer Unterschied besteht in der manuellen Auswahl einzelner Spalten. Hier müssen wir die einzelnen Spalten im Gegensatz zur Anwendung bei select() in diesem Fall innerhalb von c() übergeben. An dieser Stelle würden die eigentlichen Werte der Spalten Extraversion und Neurotizismus mit den logarithmierten Werten überschrieben werden, da das zuvor beschriebene .names Argument nicht definiert ist. big5 |&gt; mutate(across( .cols = c(Extraversion, Neurotizismus), .fns = log) ) Auch andere bereits in Kapitel 6.2 besprochene Helferfunktionen wie starts_with(), ends_with(), contains() oder where() können zur Variablensauswahl für das .cols Argument verwendet werden. big5 |&gt; mutate(across( .cols = where(is.numeric), .fns = log) ) Übung 6.4.2. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.4.2). 6.4.3 Änderungen unter Bedingungen Häufig möchte man Funktionen nicht auf alle Zeilen innerhalb der ausgewählten Spalten gleichermaßen anwenden. Wenn wir bspw. eine bestehende Spalte auf bestimmte Art und Weise nur dann verändern wollen, wenn eine bestimmte Bedingung zutrifft, erreichen wir dies mit der Funktion if_else(). Wir haben in Kapitel 6.3 gesehen, dass zwei Probandinnen ihr Geburtsjahr anstelle des Alters in Jahren angegeben haben. Wenn unsere Bedingung (condition) zutrifft, also das Alter in Jahren größer als 120 ist, soll das Jahr der Erhebung (2020) Minus das Alter gerechnet werden. Ansonsten (else bzw. false) wird nur das unveränderte Alter zurückgegeben. Anschließend überprüfen wir noch unsere Berechnung, indem wir das Alter wieder absteigend anordnen. big5 |&gt; mutate(Alter = if_else( condition = Alter &gt; 120, true = 2020 - Alter, false = Alter) ) |&gt; arrange(desc(Alter)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 60 f 3 2.7 5 1 5 2 59 m 2.7 2.3 5 1 5 3 59 m 2.8 1.6 4 2 5 4 58 m 2.3 2.9 5 1 4 # ℹ 196 more rows Auf dieselbe Art und Weise könnte man Variablen vor dem Abspeichern umkodieren. Schließlich kann sich spätestens nach 2 Jahren niemand mehr erinnern, ob die 1 bei der Kodierung für Geschlecht nun für männlich oder weiblich stand. Deswegen ist es besser, diese direkt in m oder f abzuspeichern. Wenn das Geschlecht gleich 1 ist (also männlich), schreibe in die Spalte ein m und ansonsten ein f. big5 |&gt; mutate(Geschlecht = if_else( condition = Geschlecht == 1, true = &quot;m&quot;, false = &quot;f&quot;) ) Bei mehr als zwei Bedingungen können wir anstelle von if_else() die Funktion case_when() verwenden. Auf der linken Seite der Tilde (~) ist dabei immer die Bedingung angegeben, während auf der rechten Seite steht, was bei zutreffender Bedingung in die Spalte geschrieben werden soll. Allen Werten, auf die keine der explizit genannten Bedingungen zutrifft, wird NA (Akronym für Not Available, engl. für nicht vorhanden) zugewiesen. Dies kann vermieden werden, indem man am Ende das Argument .default als Bedingung hinzufügt. Achtung, hier wird ein Gleichheitszeichen verwendet, da es sich um ein gewöhnliches Argument handelt. Somit werden in diesem Beispiel alle ProbandInnen, die bisher keiner Bedingung zugeordnet sind, in die ältesten Altersgruppe kategorisiert. Am Ende ordnen wir unsere neu erstellte Spalte zum Kontrollieren unserer Berechnung wie gewohnt nach vorne. big5 |&gt; mutate(Gruppe = case_when( Alter &lt;= 25 ~ &quot;Jungspund&quot;, Alter &gt; 25 &amp; Alter &lt;= 45 ~ &quot;Mittel&quot;, between(Alter, 46, 65) ~ &quot;Erfahren&quot;, .default = &quot;Weise&quot;) ) |&gt; relocate(Gruppe) # A tibble: 200 × 8 Gruppe Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Mittel 36 m 3 1.9 5 1 5 2 Mittel 30 f 3.1 3.4 5 3 5 3 Jungspund 23 m 3.4 2.4 3 3 5 4 Erfahren 54 m 3.3 4.2 2 5 3 # ℹ 196 more rows Die Helferfunktion between() ist eine übersichtliche Alternative zum kombinierten logischen Begriff eine Zeile darüber. Wichtig ist an dieser Stelle, dass der Datentyp auf der linken Seite der Tilde immer logisch sein muss. Der Ausdruck der linken Seite muss also entweder TRUE oder FALSE zurückgeben. Auf der rechten Seite der Tilde muss über alle Bedingungen hinweg immer der gleiche Datentyp sein. Wenn wir also wie hier den Datentyp Character haben, muss bei allen diesen Zuweisungen auf der rechten Seite der Datentyp übereinstimmen. Ein weiterer praktischer Anwendungsfall ist die Umkodierung von Antwortoptionen bei Fragebogendaten. Angenommen, wir messen auf einer Skala von 1 (trifft gar nicht zu) bis 5 (trifft vollkommen zu) die Ausprägung der Offenheit für neue Erfahrungen. Um Verzerrungen zu vermeiden, sind in einem derartigen Fragebogen immer einige Items verneint gestellt. Normalerweise würde beispielsweise gefragt, ob man gerne neue Sportarten ausprobiert. Würden wir allerdings fragen, ob man ungerne neue Sportarten ausprobiert, trifft unsere Skala natürlich nicht mehr zu. Hier wäre 5 trifft gar nicht zu und 1 trifft vollkommen zu. Stellen wir uns vor, dies würde für die erste Frage zur Offenheit O1 zutreffen. Zum Vergleich erstellen wir eine neue Spalte namens O1_neu, welche die umkodierten Werte enthält. Um die Ausgabe zu überprüfen lassen wir die originale und neue Spalte mithilfe von relocate() zu Beginn ausgeben (siehe Kapitel 6.2). big5 |&gt; mutate(O1_neu = case_when( O1 == 1 ~ 5, O1 == 2 ~ 4, O1 == 3 ~ 3, O1 == 4 ~ 2, O1 == 5 ~ 1) ) |&gt; relocate(O1, O1_neu) # A tibble: 200 × 8 O1 O1_neu Alter Geschlecht Extraversion Neurotizismus O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 1 36 m 3 1.9 1 5 2 5 1 30 f 3.1 3.4 3 5 3 3 3 23 m 3.4 2.4 3 5 4 2 4 54 m 3.3 4.2 5 3 # ℹ 196 more rows Immer wenn die Spalte O1 den Werte 1 hat, wird eine 5 daraus gemacht und immer wenn eine 2 angekreuzt wurde, diese mit einer 4 ersetzt. Die 3 können wir so belassen und die 4 und 5 wandeln wir auf die gleiche Art und Weise um. Beachte auch hier, dass wir auf der linken Seite immer eine logische Abfrage und rechts denselben Datentyp (hier Double) vorliegen haben. Auch hier können wir die nützlichen Helferfunktionen contains(), starts_with(), ends_with() und where() auf der linken Seite der Tilde verwenden. Eine verkürzte Schreibweise für Szenarien des Umkodierens, wie zuvor gezeigt, bietet die Funktion case_match(). So könnte der vorherige Befehl wie folgt abgekürzt werden. big5 |&gt; mutate(O1_neu = case_match( O1, 1 ~ 5, 2 ~ 4, 3 ~ 3, 4 ~ 2, 5 ~ 1) ) Auch Spalten vom Typ Character, wie z.B. die Bezeichnung von Gruppen, können wir so verändern. Soll das Geschlecht bspw. nicht als \"f\" und \"m\" sondern als weiblich und maennlich gespeichert werden, müssen wir wie auch bei case_when() die alte Bedingung auf die linke Seite der Tilde und das neue Ergebnis auf die rechte Seite schreiben. big5 |&gt; mutate(Geschlecht = case_match( Geschlecht, &quot;f&quot; ~ &quot;weiblich&quot;, &quot;m&quot; ~ &quot;maennlich&quot;) ) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 maennlich 3 1.9 5 1 5 2 30 weiblich 3.1 3.4 5 3 5 3 23 maennlich 3.4 2.4 3 3 5 4 54 maennlich 3.3 4.2 2 5 3 # ℹ 196 more rows Mit dieser Funktion können auch verschiedene Schreibweisen beim Ausfüllen von Fragebögen korrigiert werden. In diesem Beispiel gab es in der Spalte Kontakt vier Arten das Wort täglich zu schreiben und zwei für wöchentlich. daten |&gt; mutate(Kontakt = case_match( Kontakt, c(&quot;taeglich&quot;, &quot;Taeglich&quot;, &quot;täglich&quot;, &quot;Täglich&quot;) ~ &quot;Taeglich&quot;, c(&quot;wöchentlich&quot;, &quot;Wöchentlich&quot;) ~ &quot;Woechentlich&quot;) ) Nach gleichem Schema könnte man auch eine neue Spalte für Medikament A erstellen, die eine 1 bei Medikamenteneinnahme enthält und sonst eine 0. In der Spalte Medikamente wären bspw. bei manchen PatientInnen der Wirkstoff und bei manchen der Markenname des Medikaments enthalten. Auch hier wird mithilfe des .default Arguments der Wert festgelegt, welcher bei nicht-zutreffen der Bedingung in die Spalte geschrieben wird. Das Beispiel kann um beliebig viele Bedingungen erweitert werden. daten |&gt; mutate(MedikamentA = case_match( Medikamente, c(&quot;A_Wirkstoff&quot;, &quot;A_Markenname&quot;) ~ 1, .default = 0) ) Übung 6.4.3. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.4.3). 6.4.4 Eigene Funktionen erstellen Obwohl in R selbst oder in zusätzlichen Packages bereits eine Vielzahl von Funktionen enthalten sind, braucht man doch immer wieder eigene Funktionen für spezifische Anwendungsfälle. Dies versuchen wir anhand einer neuen Funktion zu illustrieren, die den Logarithmus einer der Funktion übergebenen Zahl mit zwei summiert. Diese Funktion soll den Namen new_log() haben. Eine Funktion wird mit function() erstellt. Innerhalb der runden Klammern können wir mit einem Komma getrennt beliebig viele Argumente festlegen. An dieser Stelle nehmen wir nur x. Der Name dieses Arguments ist grundsätzlich egal, solange er wie hier in dem Beispiel sowohl innerhalb von function() als auch in log() miteinander übereinstimmt. Die eigentliche Berechnung findet innerhalb der geschweiften Klammern statt. Es ist wichtig, dass wir einmal vor Benutzung diese Funktion durch Ausführen (strg + enter) lokal als Variable speichern. new_log &lt;- function(x) { log(x) + 2 } Eigene Funktionen müssen genau wie Packages nach Neustart von R immer wieder neu geladen werden. Dies erreicht man durch einmaliges Ausführen des obigen Befehls zu Beginn der Auswertung. Es gibt in dieser Hinsicht also keinen Unterschied zum Speichern gewöhnlicher Variablen. Eine Funktion mit zwei Argumenten, wenn wir beispielsweise zusätzlich noch die zu addierende Zahl innerhalb der Funktion anpassen möchten, könnte wie folgt aussehen. new_log &lt;- function(x, zahl = 2) { log(x) + zahl } Durch das zweite Argument könnte man mit new_log(c(2, 4, 1), zahl = 5) den Logarithmus der drei Zahlen jeweils mit 5 (anstelle von 2) addieren. Da innerhalb der Funktion bereits ein Standardwert (hier 2) angegeben ist, ist die explizite Angabe des Arguments zahl optional. So könnte man zum Logarithmieren und addieren mit 2 einfach new_log(c(2, 4, 1)) verwenden. Einmal erstellt und abgespeichert, können wir die eigene Funktion, wie in Kapitel 6.4.1 bereits gelernt, direkt innerhalb von mutate() anwenden. big5 |&gt; mutate(across(Extraversion:Neurotizismus, new_log)) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3.10 2.64 5 1 5 2 30 f 3.13 3.22 5 3 5 3 23 m 3.22 2.88 3 3 5 4 54 m 3.19 3.44 2 5 3 # ℹ 196 more rows Eine Kurzschreibweise zum Definieren eigener Funktionen sind sogenannte Lambda Funktionen. Dies sind anonyme Funktionen, die keinen Funktionsnamen erhalten und daher auch nur einmalig bei Verwendung aufgerufen werden können. Dabei sind zwei Sachen hervorzuheben: Auf der einen Seite muss man immer einen führenden Backslash gefolgt von runden Klammern (\\()) hinzufügen. Innerhalb der Klammern wird der Name des Arguments (hier innerhalb von log()) übergeben. Im vorherigen Beispiel haben wir die Werte mit x und die zu addierende Zahl mit zahl bezeichnet. Bei anonymen Funktionen können wir entweder ebenfalls x oder einen beliebigen anderen Namen (z.B. wert) verwenden. Die folgenden zwei Funktionsaufrufe sind äquivalent. big5 |&gt; mutate(across(Extraversion:Neurotizismus, \\(x) log(x) + 2)) big5 |&gt; mutate(across(Extraversion:Neurotizismus, \\(wert) log(wert) + 2)) Als zweites Beispiel verwenden wir die z-Transformation bzw. Standardisierung einer Variable. Die dafür in R integrierte Funktion namens scale() gibt noch zusätzliche Informationen wieder, weswegen der Funktionsaufruf innerhalb von as.numeric() (Umwandlung in einen rein numerischen Datentypen) stehen sollte. Da wir die Spalten Extraversion und Neurotizismus nicht überschreiben, sondern zwei neue Spalten erstellen wollen, wird zusätzlich das .names Argument verwendet. Die neuen Spalten mit den standardisierten Werten würden so die Endung z erhalten. big5 |&gt; mutate(across( .cols = Extraversion:Neurotizismus, .fns = \\(wert) as.numeric(scale(wert)), .names = &quot;{.col}_z&quot;) ) Innerhalb des tidyverse kann eine alternative, nicht mehr empfohlene Schreibweise mit einer führenden Tilde verwendet werden. Dabei muss das Argument der Funktion immer mit .x benannt werden. big5 |&gt; mutate(across( .cols = Extraversion:Neurotizismus, .fns = ~ as.numeric(scale(.x)), .names = &quot;{.col}_z&quot;) ) Anonyme Funktionen sind eine praktische Möglichkeit, schnell eigene wenig komplexe Funktionen zu erstellen, die man nur an einer Stelle benötigt. So spart man sich das eigenständige Erstellen einer neuen Funktion. Für komplexere Anwendungen ist jedoch das Erstellen einer eigenen Funktion mit function() {} der übersichtlichere und damit empfohlene Weg. Seit der R Version 4.1.0 sind anonyme Funktionen mit der \\() Syntax ohne zusätzliche Packages direkt in R integriert. Für welche der beiden Optionen man sich letzten Endes entscheidet, hängt von der persönlichen Präferenz ab. Mithilfe der Lambda Funktionen könnten wir auf einen Schlag anders als zuvor in Kapitel 6.4.3 nicht nur eine Spalte, sondern so viele wie wir wollen, umkodieren. Wir erinnern uns, eine Spalte könnten wir mithilfe von case_when() umkodieren. big5 |&gt; mutate(O1_neu = case_when( O1 == 1 ~ 5, O1 == 2 ~ 4, O1 == 3 ~ 3, O1 == 4 ~ 2, O1 == 5 ~ 1) ) Möchten wir in einem Zug die Spalten O1, O2 und O3 in die richtige Reihenfolge bringen, können wir across() mit case_when() kombinieren. Durch die Lambda Funktion ändert sich der Spaltenname zum Backslash mit der Bezeichnung des Arguments (hier Auspraegung) in Klammern. Dieses Argument wird dann für alle ausgewählten Spalten umkodiert. big5 |&gt; mutate(across(c(O1, O2, O3), \\(Auspraegung) case_when( Auspraegung == 1 ~ 5, Auspraegung == 2 ~ 4, Auspraegung == 3 ~ 3, Auspraegung == 4 ~ 2, Auspraegung == 5 ~ 1) )) # A tibble: 200 × 7 Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 36 m 3 1.9 1 5 1 2 30 f 3.1 3.4 1 3 1 3 23 m 3.4 2.4 3 3 1 4 54 m 3.3 4.2 4 1 3 # ℹ 196 more rows Alternativ könnten die Variablen innerhalb von across() in diesem Fall auch mit O1:O3 oder starts_with(\"O\") ausgewählt werden. Beachte an dieser Stelle auch, dass die schließende Klammer von across() hinter dem vollständigen Funktionsaufruf von case_when() platziert wird. Übung 6.4.4. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.4.4). 6.4.5 Zeilenweise berechnen Für Berechnungen pro Beobachtung muss die Funktion rowwise() mit c_across() kombiniert werden. Durch die separate Berechnung für jede Zeile über mehrere Spalten können bspw. Mittelwerte für jede Person in einem bestimmten Merkmal berechnet werden. Um das Konzept zu illustrieren, soll der Mittelwert pro Person für den Persönlichkeitsfaktor Offenheit berechnet werden. Dieser ergibt sich aus drei einzelnen Fragen zur Offenheit (O1, O2, O3). Zuerst müssen wir die Funktion rowwise() aufrufen, um R das zeilenweise Berechnen zu signalisieren. Innerhalb von mutate() müssen unsere drei Fragen zur Offenheit nun der Funktion c_across() übergeben werden. Beachte das Präfix c_ an dieser Stelle. Nach der Berechnung muss die zeilenweise Betrachtung des Datensatzes noch mit ungroup() aufgehoben werden. Zur Kontrolle holen wir uns die neu erstellte Spalte namens Offenheit wieder an den Anfang des Datensatzes. Auf diesen Aufruf von relocate() kann natürlich beim Abspeichern verzichtet werden. big5 |&gt; rowwise() |&gt; mutate(Offenheit = mean(c_across(O1:O3))) |&gt; ungroup() |&gt; relocate(Offenheit) # A tibble: 200 × 8 Offenheit Alter Geschlecht Extraversion Neurotizismus O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.67 36 m 3 1.9 5 1 5 2 4.33 30 f 3.1 3.4 5 3 5 3 3.67 23 m 3.4 2.4 3 3 5 4 3.33 54 m 3.3 4.2 2 5 3 # ℹ 196 more rows Hinter die Spaltenauswahl mithilfe von c_across() können wir mit einem Komma getrennt wie gewohnt weitere Argumente der jeweiligen Funktion übergeben. Hier sei exemplarisch die Entfernung fehlender Werte mit na.rm = TRUE illustriert. big5 |&gt; rowwise() |&gt; mutate(Offenheit = mean(c_across(O1:O3), na.rm = TRUE)) |&gt; ungroup() Alternativ könnte man auf diese Weise mit sum() auch die Summe über bestimmte Spalten berechnen. Grundsätzlich können wir so jede Funktion aufrufen, die einen Wert pro Beobachtung zurückgibt (z.B. einen Mittelwert oder einen Median). Falls diese zusammenfassenden Berechnungen nicht pro Beobachtung sondern pro Gruppe oder über alle Beobachtungen ausgegeben werden soll, muss stattdessen summarise() verwendet werden (siehe Kapitel 7). Übung 6.4.5. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.4.5). 6.5 Umgang mit fehlenden Werten und Duplikaten Zur besseren Illustration der verschiedenen Möglichkeiten verwenden wir an dieser Stelle einen kleinen selbst erstellten Datensatz namens df. df &lt;- tibble( Alter = c(34, NA, 45, 999), Geschlecht = c(NA, &quot;m&quot;, &quot;f&quot;, &quot;&quot;), Extraversion = c(4, 3, 999, 2) ) df # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 NA &quot;m&quot; 3 3 45 &quot;f&quot; 999 4 999 &quot;&quot; 2 Enthalten sind zum einen fehlende Werte als NA und zum anderen als 999 kodiert. NA ist dabei ein besonderer Datentyp, der für Not Available (engl. für nicht verfügbar) steht. Die Kodierung als 999 ist typisch für Nutzer des alternativen Statistikprogramms SPSS, da dort kein dedizierter Datentyp für fehlende Werte existiert. Wir sind also daran interessiert, diese 999 oder andere nicht passende Werte in NAs sowie umgekehrt NAs in bestimmte Zahlen umzuwandeln. Einen ersten Überblick über die Anzahl der fehlenden Werte in allen Spalten erhalten wir mit colSums() und is.na(). Erstere Funktion bildet die Summe pro Spalte und letztere fragt ab, ob der Wert fehlend ist. colSums(is.na(df)) Alter Geschlecht Extraversion 1 1 0 Hier sehen wir richtiger Weise, dass ein Wert in der Altersspalte und zwei Werte in der Geschlechtsspalte fehlen. Möchten wir sehen, auf welche Beobachtungen das genau zutrifft, können wir das in Kapitel 6.3 kennengelernte filter() verwenden. Zum Beispiel könnte man so sehen, wer keine Geschlechtsangabe gemacht hat. df |&gt; filter(is.na(Geschlecht)) # A tibble: 1 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 Zum Umwandeln von Werten in NA können wir die Funktion na_if() aus dem dplyr Package verwenden. Wenn bspw. in der Spalte Alter die Zahl 999 vorkommt, soll stattdessen NA geschrieben werden. Das ganze müssen wir natürlich innerhalb von mutate() verwenden (siehe Kapitel 6.4). df |&gt; mutate(Alter = na_if(Alter, 999)) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 NA &quot;m&quot; 3 3 45 &quot;f&quot; 999 4 NA &quot;&quot; 2 Dasselbe können wir natürlich mit across() auch gleich auf mehrere Spalten anwenden (siehe Kapitel 6.4.2). Hierbei müssen jedoch die Datentypen gleich sein. df |&gt; mutate(across(c(Alter, Extraversion), \\(x) na_if(x, 999))) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 NA &quot;m&quot; 3 3 45 &quot;f&quot; NA 4 NA &quot;&quot; 2 So könnte man auch ein NA in jene Zellen schreiben, die einen leeren Character beinhalten. df |&gt; mutate(across(where(is.character), \\(x) na_if(x, &quot;&quot;))) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 NA m 3 3 45 f 999 4 999 &lt;NA&gt; 2 Umgekehrt zur Umwandlung von NAs z.B. in die Zahl 999, können wir replace_na() aus selbigen Package benutzen. Wenn in der Spalte Alter ein NA steht, soll dieses mit 999 ersetzt werden. df |&gt; mutate(Alter = replace_na(Alter, 999)) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 999 &quot;m&quot; 3 3 45 &quot;f&quot; 999 4 999 &quot;&quot; 2 Man könnte NAs auch in Abhängigkeit einer Bedingung mithilfe von if_else() oder case_when() zuweisen. So würde bspw. bei allen Personen mit einer Altersangabe über 120 ein fehlender Wert mit NA eingetragen werden. df |&gt; mutate(Alter = if_else(condition = Alter &gt; 120, true = NA, false = Alter)) # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 NA &quot;m&quot; 3 3 45 &quot;f&quot; 999 4 NA &quot;&quot; 2 Die meisten Statistikfunktionen haben im Regelfall ein Argument namens na.rm (Akronym für not available remove), welches die fehlenden Werte der entsprechenden Spalte direkt entfernt. Genauer gesagt verwenden diese Funktionen an dieser Stelle die sogenannten Pairwise complete observations. Im letzten Kapitel haben wir die Anwendung bereits im Kontext von zeilenweisen Mittelwertsberechnungen kennengelernt. big5 |&gt; rowwise() |&gt; mutate(Offenheit = mean(c_across(O1:O3), na.rm = TRUE)) |&gt; ungroup() Eine weitere Möglichkeit ist das Entfernen von Zeilen, die fehlende Werte enthalten. Dies erreichen wir mit drop_na() aus dem dplyr Package. Allerdings entfernt diese Funktion die gesamte Zeile von allen Beobachtungen, in denen auch nur ein NA vorkommt. Wenn du also zwei Spalten auswerten möchtest und in einer dritten für die Auswertung irrelevanten Spalte ist ein fehlender Wert, würde die entsprechende Zeile trotzdem entfernt werden. Hier ist also Vorsicht geboten, um keine Informationen zu verlieren. df |&gt; drop_na() # A tibble: 2 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 45 &quot;f&quot; 999 2 999 &quot;&quot; 2 Alternativ können wir mithilfe von filter() und der logischen Abfrage is.na() das Entfernen von NAs auch auf eine bestimmte Spalte begrenzen (hier Geschlecht). Der Unterschied zum Aufruf zuvor ist das Ausrufezeichen vor is.na(). Es sollen schließlich jene Zeilen ausgegeben werden, die keinen fehlenden Wert in der Spalte Geschlecht haben. df |&gt; filter(!is.na(Geschlecht)) # A tibble: 3 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 NA &quot;m&quot; 3 2 45 &quot;f&quot; 999 3 999 &quot;&quot; 2 Exemplarische sei eine doppelte Zeile in den Datensatz df hinzugefügt. df &lt;- tibble( Alter = c(34, NA, 45, 45), Geschlecht = c(NA, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;), Extraversion = c(4, 3, 999, 999) ) df # A tibble: 4 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 NA m 3 3 45 f 999 4 45 f 999 Mit distinct() können derartige Duplikate entfernt werden. df |&gt; distinct() # A tibble: 3 × 3 Alter Geschlecht Extraversion &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 34 &lt;NA&gt; 4 2 NA m 3 3 45 f 999 Der Funktion können auch Spaltennamen übergeben werden. Dies ist nützlich, wenn zwei Beobachtungen von derselben Person vorhanden sind, man aber bspw. nur die Erstdiagnose behalten möchte. Für dieses Beispiel würde mit arrange() erst die Reihenfolge so verändert, dass die Erstdiagnose bei doppelten Personeneinträgen an erster Stelle steht und anschließend nur die erste Zeile behalten wird. daten |&gt; arrange(Name, Vorname, Erstdiagnose) |&gt; distinct(Name, Vorname, .keep_all = TRUE) Das Argument .keep_all sorgt dafür, dass jeweils die erste Spalte von Duplikaten behalten wird. 6.6 Breites und langes Datenformat Grundsätzlich unterscheidet man ein sogenanntes breites Datenformat von einem langen Datenformat. Im breiten Datensatz ist jede Spalte eine Variable, jede Zeile eine Beobachtung und jede Zelle ein Wert. Für die meisten Fälle ist das unser gewünschtes Datenformat. In Abbildung 6.1 ist ein einfaches Beispiel für einen breiten Datensatz mit drei Personen und zwei Variablen illustriert. Abbildung 6.1: Breites Datenformat mit drei Personen und drei Variablen. Für das Erstellen mehrfaktorieller Abbildungen und hierarchischer statistischer Modellierung benötigen wir allerdings das lange Datenformat. In Abbildung 6.2 ist der zuvor gezeigte Datensatz in ein langes Format umgewandelt. Abbildung 6.2: Langes Datenformat mit Persönlichkeitsfaktor als Innersubjektfaktor. Im tidyr Package sind zwei Funktionen für genau diese Umwandlungen enthalten. Mit pivot_longer() (engl. für Drehpunkt länger) können wir ein breites Datenformat in ein langes verändern. Die Funktion pivot_wider() fungiert umgekehrt für die Transformation vom langen ins breite Datenformat. Erstere Funktion findet deutlich häufiger Anwendung, da die Daten häufig initial im breiten Format vorhanden sind. Für das Umformatieren ins lange Datenformat ist es essentiell, einen eindeutigen Personenidentifikator im breiten Datensatz zu haben (z.B. eine Kombination aus Vor- und Nachname mit dem Geburtsjahr). Ansonsten könnten doppelte Zeilen vorkommen, die der Integrität des Datensatzes schaden.. Hier entscheiden wir uns einfach für die Zeilennummer, die wir mit der Funktion row_number() in die Spalte VPN (Akronym für Versuchspersonennummer) schreiben. wide_big5 &lt;- big5 |&gt; mutate(VPN = row_number()) |&gt; select(VPN, Geschlecht, Extraversion, Neurotizismus) wide_big5 # A tibble: 200 × 4 VPN Geschlecht Extraversion Neurotizismus &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 m 3 1.9 2 2 f 3.1 3.4 3 3 m 3.4 2.4 4 4 m 3.3 4.2 # ℹ 196 more rows Nun müssen wir in der Funktion pivot_longer() nur noch die gewünschten Spalten auswählen. Im obigen Beispiel wären das Extraversion und Neurotizismus. Beachte, dass genau wie bei across() auch hier die Spalten bei einzelner Auswahl der Funktion innerhalb von c() übergeben werden müssen. Es werden zwei neue Spalten erstellt, die erst noch benannt werden müssen. Wie man diese benennt, ist einem selbst überlassen. Der Name für die Spalte mit den Werten wird mit dem Argument values_to und die Spalte mit den Spaltennamen mit names_to festgelegt. Hier entscheiden wir uns für die neuen Spaltennamen \"Auspraegung\" und \"Faktor\". Die Namen müssen hier unbedingt in Anführungszeichen geschrieben werden, da die Spalten noch nicht existieren. Das Ergebnis speichern wir an dieser Stelle als long_big5 ab. long_big5 &lt;- wide_big5 |&gt; pivot_longer( cols = c(Extraversion, Neurotizismus), values_to = &quot;Auspraegung&quot;, names_to = &quot;Faktor&quot; ) long_big5 # A tibble: 400 × 4 VPN Geschlecht Faktor Auspraegung &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 m Extraversion 3 2 1 m Neurotizismus 1.9 3 2 f Extraversion 3.1 4 2 f Neurotizismus 3.4 # ℹ 396 more rows Zur Auswahl der Spalten können dieselben Helferfunktionen verwendet werden, die in Kapitel 6.2 beschrieben sind (z.B. starts_with(), ends_with() oder everything()). Umgekehrt können wir mithilfe von pivot_wider() den Datensatz long_big5 wieder ins breite Datenformat bringen. Dafür müssen wir hier nur festlegen, aus welcher Spalte die Werte (values_from) und woher die Spaltennamen (names_from) kommen sollen. Hier benötigen wir keine Anführungszeichen, da die Spalten bereits in unserem Datensatz enthalten sind. long_big5 |&gt; pivot_wider( values_from = Auspraegung, names_from = Faktor ) # A tibble: 200 × 4 VPN Geschlecht Extraversion Neurotizismus &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 m 3 1.9 2 2 f 3.1 3.4 3 3 m 3.4 2.4 4 4 m 3.3 4.2 # ℹ 196 more rows Als grobe Daumenregel kann man sich merken, dass man nicht vorhandene Spalten mit Anführungszeichen übergeben muss. Auf bereits im Datensatz vorhandene Spalten kann man hingegen im Regelfall ohne Anführungszeichen zugreifen. Übung 6.6. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.6). 6.7 Spalten trennen Mit den Funktionen namens separate_wider_*() aus dem tidyr Package können Spalten getrennt werden. Dabei unterscheiden wir drei Szenarien, für die jeweils eine eigene Funktion existiert. Die Informationen innerhalb einer Spalte sind getrennt durch einen Trenner (z.B. Unterstrich, Komma, Punkt): separate_wider_delim(), eine genaue Position: separate_wider_position(), einen Regex (siehe Kapitel 6.9): separate_wider_regex(). Falls die Spalten anstelle des weiten Datenformates in ein langes gebracht werden sollen, existieren äquivalent dazu die Funktionen separate_longer_delim(), separate_longer_position() und separate_longer_regex(). Exemplarisch sei hier der im remp Package enthaltene Datensatz big5_zeit geladen. big5_zeit # A tibble: 5 × 5 VPN Extrav_T1 Extrav_T2 NeurotFA NeurotFB &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 3.2 3.3 2.8 3.2 2 2 1.7 1.5 4.1 3.2 3 3 2.8 2.7 3.2 2.8 4 4 4.7 4.2 1.7 2.4 # ℹ 1 more row Um das hier bestehende Problem klarer zu machen, wandeln wir diesen erst einmal in ein langes Datenformat um (siehe Kapitel 6.6). zeit1 &lt;- big5_zeit |&gt; select(-NeurotFA, -NeurotFB) |&gt; pivot_longer( cols = Extrav_T1:Extrav_T2, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) zeit1 # A tibble: 10 × 3 VPN Faktor Auspraegung &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Extrav_T1 3.2 2 1 Extrav_T2 3.3 3 2 Extrav_T1 1.7 4 2 Extrav_T2 1.5 # ℹ 6 more rows Die Spalte Faktor enthält hier zwei Informationen: den Persönlichkeitsfaktor (Extrav) und den entsprechenden Messzeitpunkt (T1, T2). Es muss mit dem names Argument festgelegt werden, wie die neuen Spalten mit den getrennten Informationen heißen sollen. Die beiden Informationen sind durch einen Unterstrich (_) getrennt, weswegen wir dem delim Argument innerhalb der Funktion separate_wider_delim() diesen Unterstrich als Character übergeben. Würde die Spalte Faktor mehr als zwei Informationen getrennt durch mehrere Unterstriche enthalten, müssten wir dem Argument names entsprechend drei Spaltennamen übergeben. zeit1 |&gt; separate_wider_delim( cols = Faktor, names = c(&quot;Faktor&quot;, &quot;Zeitpunkt&quot;), delim = &quot;_&quot; ) # A tibble: 10 × 4 VPN Faktor Zeitpunkt Auspraegung &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Extrav T1 3.2 2 1 Extrav T2 3.3 3 2 Extrav T1 1.7 4 2 Extrav T2 1.5 # ℹ 6 more rows Falls die Informationen innerhalb einer Spalten nicht mit einem Unterstrich, sondern durch unterschiedliche Wortlängen, getrennt sind, verwendet man stattdessen die Funktion separate_wider_position(). Hierbei muss dem widths Argument innerhalb von c() die Anzahl der Buchstaben der ersten und zweiten Information übergeben werden. Der Persönlichkeitsfaktor Neurot hat sechs Buchstaben und die Information über den Messzeitpunkt zwei. big5_zeit |&gt; select(-Extrav_T1, -Extrav_T2) |&gt; pivot_longer( cols = NeurotFA:NeurotFB, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) |&gt; separate_wider_position( cols = Faktor, widths = c(Faktor = 6, Zeitpunkt = 2) ) # A tibble: 10 × 4 VPN Faktor Zeitpunkt Auspraegung &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Neurot FA 2.8 2 1 Neurot FB 3.2 3 2 Neurot FA 4.1 4 2 Neurot FB 3.2 # ℹ 6 more rows Übung 6.7. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.7). 6.8 Datensätze zusammenführen In der Praxis hat man häufig nicht nur einen Datensatz vorliegen, der alle notwendigen Informationen enthält. Stattdessen sind oft die eigentlichen Daten der Studie in einem Datensatz, die demographischen Daten wie Geschlecht und Alter in einem zweiten und Laborwerte wiederum in einem anderen Datensatz. Andere mögliche Szenarien sind mehrere Untersucher oder verschiedenen Messzeitpunkte. Welcher Grund auch immer für separate Datensätze verantwortlich ist, vor der Auswertung müssen diese zusammengeführt werden. Um dieses Prinzip zu illustrieren, haben wir zwei Datensätze mit Informationen über den Puls, Vorhandensein einer Blasenentleerungsstörung und die Anzahl an Infektionen innerhalb von zwei Jahren. Diese wurden von zwei Untersuchern erhoben, weswegen diese in dem Datensatz df_oben und df_unten vorliegt. df_oben # A tibble: 3 × 4 ID Puls Blasenstoerung Infekt_2j &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 AX161095 83 0 1 2 NM020683 108 1 7 3 IO240576 60 0 2 df_unten # A tibble: 3 × 4 ID Puls Blasenstoerung Infekt_2j &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 EW180265 53 1 5 2 CB280682 92 0 0 3 JH051199 65 0 1 Der Datensatz demogr enthält darüber hinaus demographische Daten in Form des biologischen Geschlechts und Alters. demogr # A tibble: 4 × 3 ID Sex Alter &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 AX161095 m 28 2 NM020683 f 47 3 IO240576 f 40 4 JH051199 m 24 Solange in beiden Datensätzen dieselben Spalten vorhanden sind, könnte man beide zeilenweise zusammenfügen. In Kapitel 5.1 haben wir bereits gesehen, dass mehrere Datensätze mit gleichem Format mithilfe des zusätzlichen Arguments rbind (row bind) verbunden werden können. import_list(dateien, rbind = TRUE) Mit der Funktion bind_rows() kann diese Operation auch mit bereits in R geladenen Datensätzen durchgeführt werden. Die einzigen Argumente sind dabei die Namen der Datensätze. bind_rows(df_oben, df_unten) # A tibble: 6 × 4 ID Puls Blasenstoerung Infekt_2j &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 AX161095 83 0 1 2 NM020683 108 1 7 3 IO240576 60 0 2 4 EW180265 53 1 5 5 CB280682 92 0 0 6 JH051199 65 0 1 Problematisch ist bei dieser Funktion, dass nicht überprüft wird, ob Beobachtungen mehrfach vorkommen. Mit einem sogenannten Join (engl. für aneinanderfügen) können zwei Datensätze kontrolliert zusammengefügt werden. Die Funktion full_join() kombiniert alle Informationen aus beiden Datensätzen. full_join(df_oben, df_unten) Joining with `by = join_by(ID, Puls, Blasenstoerung, Infekt_2j)` # A tibble: 6 × 4 ID Puls Blasenstoerung Infekt_2j &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 AX161095 83 0 1 2 NM020683 108 1 7 3 IO240576 60 0 2 4 EW180265 53 1 5 5 CB280682 92 0 0 6 JH051199 65 0 1 Da in diesem Beispiel alle Spalten aus beiden Datensätzen integriert werden sollen, muss zusätzlich das by Argument nicht spezifiziert werden. Es wird eine Benachrichtigung ausgegeben, dass nach den Spalten ID, Puls, Blasenstoerung und Infekt_2j zusammengefügt wurde. Vorsicht ist geboten, wenn es Überschneidung in den Datensätzen gibt (z.B. die gleiche Person in beiden Datensätzen aber mit unterschiedlichen Werten in einer gleichnamigen Spalte). Daher sollte bspw. ein zweiter Messzeitpunkt direkt innerhalb des Spaltennamens entsprechend gekennzeichnet werden (z.B. Puls_T1 und Puls_T2). Sollen in den linken Datensatz (1. Argument, hier df_oben) die Informationen eines zweiten Datensatzes (2. Argument, hier demogr) in Abhängigkeit der Übereinstimmung einer dritten Spalte (by Argument, hier ID) eingefügt werden, verwenden wir left_join(). left_join(df_oben, demogr, by = &quot;ID&quot;) # A tibble: 3 × 6 ID Puls Blasenstoerung Infekt_2j Sex Alter &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 AX161095 83 0 1 m 28 2 NM020683 108 1 7 f 47 3 IO240576 60 0 2 f 40 Obwohl in demogr die demographischen Informationen von vier Personen enthalten sind, werden an dieser Stelle nur jene drei an df_oben angehängt, deren ID mit der in df_oben übereinstimmt. Wenn der linke Datensatz in den rechten integriert werden soll, kann man äquivalent dazu right_join() benutzen. Wenn hingegen alle IDs vorhanden sind, allerdings einige demographische Informationen fehlen, werden diese mit NA (not available, engl. für nicht vorhanden) angegeben. Um das zu illustrieren, werden erst df_oben und df_unten zusammengefügt. df_alle &lt;- full_join(df_oben, df_unten) Joining with `by = join_by(ID, Puls, Blasenstoerung, Infekt_2j)` Anschließend joinen wir wie zuvor eingeführt in Abhängigkeit der ID. left_join(df_alle, demogr, by = &quot;ID&quot;) # A tibble: 6 × 6 ID Puls Blasenstoerung Infekt_2j Sex Alter &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 AX161095 83 0 1 m 28 2 NM020683 108 1 7 f 47 3 IO240576 60 0 2 f 40 4 EW180265 53 1 5 &lt;NA&gt; NA 5 CB280682 92 0 0 &lt;NA&gt; NA 6 JH051199 65 0 1 m 24 Wenn wir nur diejenigen Werte integrieren möchten, die in beiden Datensätzen enthalten sind, verwenden wir inner_join(). Abschließend gibt es noch zwei Funktionen, die nicht direkt zusammenführen, sondern nur eine Bedingung prüfen und davon abhängig den ersten (linken) Datensatz zurückgeben. Die Funktion semi_join() gibt nur jene Werte aus dem ersten Datensatz zurück, welche im ersten (linken) und zweiten (rechten) vorkommen. Die Funktion anti_join() hingegen gibt nur die Werte aus dem ersten (linken) Datensatz zurück, die nicht im zweiten (rechten) Datensatz enthalten sind. Das einfache Zusammenfügen von mehreren Zeilen mit bind_rows() ist risikoreich und sollte genau überprüft werden. Kontrollierter ist das Kombinieren mithilfe der Joins. Diese fügen Datensätze nur dann zusammen, wenn es Übereinstimmungen in weiteren Spalten gibt. Übung 6.8. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.8). 6.9 Buchstaben und Wörter bearbeiten Oft muss man entweder die Spaltennamen oder die Inhalte verschiedener Spalten, die Characters enthalten, in irgendeiner Form anpassen. In diesem Kapitel schauen wir uns an, wie man mit Funktionen aus stringr Character ersetzt (str_replace()), extrahiert (str_extract()) und entdeckt (str_detect()). Das Präfix str steht dabei für String – einem anderen Wort für Character. Ein häufiges Ärgernis im Kontext von Programmiersprachen sind Umlaute, da verschiedene Zeichenkodierungen diese intern unter Umständen anders übersetzen, was auf anderen Betriebssystemen zu Kauderwelsch führen kann. Schauen wir uns im Folgenden an, wie man Umlaute ersetzt. Es ist folgender Satz in der Variable char gespeichert. char &lt;- &quot;Österreich hat 28610 schräge Berge&quot; Möchte man nun das eine ä mit ae ersetzen, verwendet man str_replace(). str_replace(string = char, pattern = &quot;ä&quot;, replacement = &quot;ae&quot;) [1] &quot;Österreich hat 28610 schraege Berge&quot; In dem Satz ist allerdings nicht nur ein ä sondern auch ein ö enthalten. Für mehr als eine Anpassung verwendet man str_replace_all(). Die Syntax ist hierbei etwas anders als bisher kennengelernt, da in diesem Fall die alte Bezeichnung auf der linken Seite der jeweiligen Gleichung steht. Alle Änderungen müssen innerhalb von c() übergeben werden. str_replace_all(string = char, c(&quot;ä&quot; = &quot;ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;)) [1] &quot;Oesterreich hat 28610 schraege Berge&quot; Beachte, dass ein Umlaut großgeschrieben ist und die Funktionen case sensitive sind. Das bedeutet, dass wir mit den Befehlen oben nur den kleinen Buchstaben ä und den großen Buchstaben Ö ersetzen. Aber wie geht man vor, wenn Spaltennamen Umlaute enthalten? Um dem auf den Grund zu gehen, erstellen wir uns einen neuen Datensatz, der die Preise für drei verschiedene Sägen in Österreich enthält. umlaut &lt;- tibble( Säge = c(&quot;Häxler&quot;, &quot;Sünde3000&quot;, &quot;Lölf4&quot;), Österreich = c(10.45, 4.60, 9.70) ) umlaut # A tibble: 3 × 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 2 Sünde3000 4.6 3 Lölf4 9.7 Die Namen enthalten jeweils einen Umlaut. Möchten wir alle Spaltennamen von Umlauten befreien, könnten wir dies mithilfe von rename_with() und str_replace_all() erreichen. An dieser Stelle verwenden wir eine im Kapitel 6.4.4 bereits eingeführte Lambda Funktion, um die Funktion auf alle Spalten anzuwenden. umlaut |&gt; rename_with(\\(x) str_replace_all( string = x, c(&quot;ä&quot; = &quot;ae&quot;, &quot;ö&quot; = &quot;oe&quot;, &quot;ü&quot; = &quot;ue&quot;, &quot;Ä&quot; = &quot;Ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;, &quot;Ü&quot; = &quot;Ue&quot;) )) # A tibble: 3 × 2 Saege Oesterreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 2 Sünde3000 4.6 3 Lölf4 9.7 Zum Ändern von Umlauten innerhalb von Spalten muss str_replace_all(), wie in Kapitel 6.4.2 kennengelernt, innerhalb von mutate() in Kombination von across() verwendet werden. So könnte man alle Spalten, die vom Datentyp Character sind, von Umlauten befreien. umlaut |&gt; mutate(across(where(is.character), \\(x) str_replace_all( string = x, c(&quot;ä&quot; = &quot;ae&quot;, &quot;ö&quot; = &quot;oe&quot;, &quot;ü&quot; = &quot;ue&quot;, &quot;Ä&quot; = &quot;Ae&quot;, &quot;Ö&quot; = &quot;Oe&quot;, &quot;Ü&quot; = &quot;Ue&quot;) ))) # A tibble: 3 × 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Haexler 10.4 2 Suende3000 4.6 3 Loelf4 9.7 Für das Extrahieren von Buchstaben oder Zahlen können wir str_extract() verwenden. Wir nehmen wieder unseren Beispielsatz von oben, der als char gespeichert ist. Es ist möglich, die Zahl mithilfe eines sogenannten Regex zu erkennen (Akronym für Regular Expression). Regex sind grundsätzlich sehr komplex selbst zu schreiben. In der Praxis muss man in der Regel nur online nach dem gewünschten Regex suchen, ohne die genaue Syntax zu verstehen. Um eine Zahl mit mehr als einer Ziffer herauszuholen, könnte man nach dem Regex \"\\d+\" suchen. In R muss noch ein zusätzlicher Backslash verwendet werden, wodurch wir den Ausdruck \"\\\\d+\" erhalten. Das d steht für digit (engl. für Ziffer) und das Plus für eine oder mehrere Ziffern. str_extract(char, &quot;\\\\d+&quot;) [1] &quot;28610&quot; Angenommen, die Spalten eines Datensatzes mit den Antworten eines Fragebogens starten mit \"Q\" folgend von Nummer, Namen und der genauen Beschreibung. Ein Beispiel hierfür wäre die 12. Frage zur Risikowahrnehmung mit Antwortschema in Klammern \"Q12_Risikowahrnehmung (0 = \"Trifft nicht zu\")\". Für die Auswertung möchten wir aufgrund der Leerzeichen und der redundanten Information den hinteren in Klammern geschriebenen Teil löschen. In anderen Worten soll der gesamte Spaltenname bis zum ersten Leerzeichen extrahiert werden. Ein möglicher Regex dafür wäre ([^\\\\s]+). Beachte auch in diesem Fall, dass bei jedem Backslash für einen Regex aus dem Internet innerhalb von R ein zweiter Backslash hinzugefügt werden muss. daten |&gt; rename_with( .cols = starts_with(&quot;Q&quot;) .fn = \\(x) str_extract(x, &quot;([^\\\\s]+)&quot;), ) Die Funktion str_detect() entdeckt bestimmte Buchstaben, Wörter oder ganze Regex. Dabei gibt die Funktion einen logischen Wert aus (TRUE, FALSE), wenn das Gesuchte gefunden oder nicht gefunden wurde. Das ist daher praktisch, da man diese Funktion für logische Bedingungen innerhalb von if_else(), case_when() oder filter() verwenden kann. str_detect(char, &quot;\\\\d+&quot;) [1] TRUE So ermöglicht str_detect() bspw. genauere Abfragen innerhalb von filter() (siehe Kapitel 6.3. Alle Käufer der Säge namens Häxler auszugeben, benötigt nur die Funktion filter(). umlaut |&gt; filter(Säge == &quot;Häxler&quot;) # A tibble: 1 × 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 Möchte man alle gekauften Sägen mit dem Buchstaben ä auswählen, könnte man hingegen str_detect() verwenden. Dies ist das zeilen-bezogene Äquivalent zur der bereits kennengelernten Helferfunktion contains(), welche auf der Auswahl von Spalten fokussiert ist (siehe Kapitel 6.2). umlaut |&gt; filter(str_detect(Säge, &quot;ä&quot;)) # A tibble: 1 × 2 Säge Österreich &lt;chr&gt; &lt;dbl&gt; 1 Häxler 10.4 In der Praxis müssen die Funktionen aus dem stringr Package in der Regel in Kombination mit mutate() oder rename_with() verwendet werden. Einen weiteren Anwendungsfall stellt der Umbruch langer Achsenbeschriftung durch str_wrap() bei der Erstellung von Visualisierungen dar. Übung 6.9. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.9). 6.10 Faktoren verändern Falls du nicht mehr im Kopf hast, was genau Faktoren sind, schaue dir noch einmal Kapitel 4.3.2 an. Faktoren sind vor allem zur automatischen Erstellung von Dummy Variablen für Regressionsmodelle und das Umbenennen oder Ändern der Reihenfolge beim Erstellen von Visualisierungen nützlich. Dummy Variablen stellen binäre (0, 1) Variablen dar, welche für nominale Merkmale mit mehr als zwei Ausprägungen im Kontext von Regressionsmodellen erstellt werden müssen. Es wird so eine Referenzkategorie festlegt, mit der die anderen Kategorien verglichen werden können. Im Folgenden schauen wir uns Beispiele an, wie man mit Funktionen aus dem forcats Package Faktoren umbenennen (fct_recode()) und deren Reihenfolge ändern (fct_relevel(), fct_reorder()) kann. Dafür verwenden wir die Spalte Gruppe aus dem big5_mod Datensatz mit den Faktorstufen (oder Level) Jung, Mittel und Weise. big5_mod |&gt; relocate(Gruppe) # A tibble: 200 × 6 Gruppe Alter Geschlecht Extraversion Neurotizismus ID &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 Mittel 36 m 3 1.9 1 2 Jung 30 f 3.1 3.4 2 3 Jung 23 m 3.4 2.4 3 4 Weise 54 m 3.3 4.2 4 # ℹ 196 more rows Um die Veränderungen der Faktorstufen besser darstellen zu können, ziehen wir uns die Spalte Gruppe aus dem Datensatz heraus (siehe Kapitel 4.5). Zuvor müssen wir die Spalte allerdings noch zum Datentyp Faktor umwandeln. Dafür übergibt man der Funktion factor() den Spaltennamen sowie die Faktorstufen (levels). big5_mod &lt;- big5_mod |&gt; mutate(Gruppe = factor(Gruppe, levels = c(&quot;Jung&quot;, &quot;Mittel&quot;, &quot;Weise&quot;))) faktoren &lt;- big5_mod$Gruppe Alternativ könnte auch as.factor(Gruppe) aufgerufen werden, allerdings werden so nur bestehende Stufen zu Faktoren umgewandelt. Wenn bspw. eine Erkrankung in der eigenen Stichprobe nie vorkommt, würde das später nicht angezeigt werden. Mit dem expliziten Festlegen aller grundsätzlich möglichen Faktorstufen mithilfe des levels Arguments innerhalb von factor() würde bei weiterer Auswertung eine Häufigkeit von 0 für die fehlende Kategorie ausgegeben werden. Zum Anzeigen der Faktorstufen verwenden wir die Funktion levels(). faktoren |&gt; levels() [1] &quot;Jung&quot; &quot;Mittel&quot; &quot;Weise&quot; Dass die Reihenfolge schon der Altersreihenfolge entspricht, liegt nur daran, dass wir die Faktorstufen oben genau spezifiziert haben. Ansonsten können durchaus unerwartete Reihenfolgen der Faktorstufen auftreten. Es lohnt sich also in jedem Fall ein Blick in die Faktorstufen zu werfen, bevor man sie verwendet. Möchte man einzelne Faktoren umbenennen, verwendet man fct_recode(). faktoren |&gt; fct_recode(Alt = &quot;Weise&quot;) |&gt; levels() [1] &quot;Jung&quot; &quot;Mittel&quot; &quot;Alt&quot; Mit der Funktion fct_relevel() kann ein Faktor als erste Stufe definiert werden. faktoren |&gt; fct_relevel(&quot;Mittel&quot;) |&gt; levels() [1] &quot;Mittel&quot; &quot;Jung&quot; &quot;Weise&quot; Zum Ändern der gesamten Reihenfolge kann man beliebig viele weitere Faktorstufen der Funktion übergeben. faktoren |&gt; fct_relevel(&quot;Weise&quot;, &quot;Mittel&quot;, &quot;Jung&quot;) |&gt; levels() [1] &quot;Weise&quot; &quot;Mittel&quot; &quot;Jung&quot; In der Praxis werden sämtlich Funktion aus dem forcats Package meistens innerhalb von mutate() verwendet. So könnte man die Referenzgruppe, also die erste Faktorstufe, für die ältestes Altersgruppe festlegen. big5_mod |&gt; mutate(Gruppe = fct_relevel(Gruppe, &quot;Weise&quot;)) Wenn die Reihenfolge der Faktoren in absteigender (.desc = TRUE) oder aufsteigender (.desc = FALSE) Reihenfolge z.B. in Abhängigkeit des Mittelwertes einer anderen Spalte (wie dem Ausmaß an Extraversion) sortiert werden soll, verwendet man fct_reorder(). Dabei kann man mit dem Argument .fun die gewünschte Funktion zur Auswertung der zweiten Variable (hier Extraversion) festlegen. extraversion &lt;- big5_mod$Extraversion faktoren |&gt; fct_reorder(extraversion, .fun = mean, .desc = TRUE) |&gt; levels() [1] &quot;Mittel&quot; &quot;Jung&quot; &quot;Weise&quot; In unserem Beispiel ist die Ausprägung der Extraversion in der mittleren Altersklasse am höchsten gefolgt von der jüngsten und der ältesten. Zur Verwendung direkt am Datensatz wird auch hier der Befehl innerhalb von mutate() aufgerufen (siehe Kapitel 6.4). Die erste Spalte ist der umzugruppierende Faktor und die zweite (hier Alter) jene, nach der gereiht werden soll. big5_mod |&gt; mutate(Gruppe = fct_reorder(Gruppe, Alter, .fun = mean, .desc = FALSE)) # A tibble: 200 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # ℹ 196 more rows Faktoren sollten erst unmittelbar vor Verwendung erstellt und verändert werden. Es kann im Umgang von Faktoren zu seltsamen Fehlermeldungen kommen, da diese innerhalb von R als Integer (Zahl) und nicht als Character (Buchstabenfolge) behandelt werden. Übung 6.10. (Noch nicht enthalten) Starte die Übung mit uebung_starten(6.10). 6.11 Mit Zeitdaten arbeiten Für das Arbeiten mit Zeitdaten muss das lubridate Package installiert und geladen sein. library(lubridate) In Kapitel 4.3.3 wurden bereits die Datentypen POSIXct, Date und Difftime vorgestellt. Dabei ist POSIXct ein selten erwünschter Datentyp. Wir können mit mutate() in Kombination mit across() aus Kapitel 6.4.2 alle Spalten vom Datentyp POSIXct in den Datentyp Date umwandeln. Im Datensatz videostream aus dem remp Package liegt die Spalte Watchdate im falschem Format vor. Die Abkürzung &lt;dttm&gt; steht dabei für den Datentyp POSIXct. videostream # A tibble: 1,493 × 4 Titel Staffel Folge Watchdate &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; 1 Suits Staffel 7 Neues Spiel 2019-07-22 00:00:00 2 Suits Staffel 1 Patente und andere Unwaegbarkeiten 2019-07-22 00:00:00 3 Death Note Staffel 1 Ein Geschaeft 2019-07-21 00:00:00 4 Haus des Geldes Teil 3 Verloren 2019-07-21 00:00:00 # ℹ 1,489 more rows Innerhalb von mutate() können wir den Datentyp der Spalte nun verändern (siehe Kapitel 6.4.1). videostream |&gt; mutate(Watchdate = as.Date(Watchdate)) # A tibble: 1,493 × 4 Titel Staffel Folge Watchdate &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; 1 Suits Staffel 7 Neues Spiel 2019-07-22 2 Suits Staffel 1 Patente und andere Unwaegbarkeiten 2019-07-22 3 Death Note Staffel 1 Ein Geschaeft 2019-07-21 4 Haus des Geldes Teil 3 Verloren 2019-07-21 # ℹ 1,489 more rows In größeren Datensätzen möchten wir in der Regel alle falsch formatierten Spalten ändern, welches wir mithilfe von across() erreichen (siehe Kapitel 6.4.2). videostream |&gt; mutate(across(where(is.POSIXct), as.Date)) Die Funktion as.Date() stößt an ihre Grenzen, wenn das Datum nicht im Jahr-Monat-Tag Format ist (z.B. \"2002-02-15\"). Daher gibt es im lubridate Package neben ymd() (year month date) für denselben Anwendungsfall wie as.Date() zusätzlich die Funktion dmy() (day month year). Möchte man nur den Tag, Monat oder das Jahr separat aus dem Datum extrahieren, können wir dies mit day(), month() und year() erreichen. videostream |&gt; mutate( Watchdate = ymd(Watchdate), Jahre = 2023 - year(Watchdate) ) Wenn das genaue Datum bekannt ist, können die Daten auch direkt voneinander subtrahiert werden. Dabei wird die Differenz in Tagen angegeben. Möchte man die Zeitdifferenz in Jahren angegeben haben (z.B. beim Alter), muss man durch die Funktion dyears() teilen. Als Argument wird die Anzahl an zu teilenden Jahren übergeben. Äquivalent dazu können wir auch durch Monate (dmonths()) und Tage (ddays()) teilen. So können wir auch auch die Überlebenszeit (OS_Zeit) bei bekanntem Datum der Erstdiagnose berechnen. daten |&gt; mutate( Alter_Diagnose = (ymd(Erstdiagnose) - ymd(Geburtsdatum)) / dyears(1), OS_zeit = ymd(OS_datum) - ymd(Erstdiagnose), ) 6.12 Binäre Antwortmatrix erstellen Im Kontext von Fragebögen ist man häufig an richtiger oder falscher Beantwortung der ProbandInnen interessiert. Mit data_binary() aus dem remp Package kann man den Datensatz in die gewünschte binäre Antwortmatrix umwandeln. Für jede Frage pro Person wird also zurückgegeben, ob das Item richtig (1) oder falsch (0) beantwortet wurde. Exemplarisch nehmen wir die ersten drei Spalten zur Offenheit für neue Erfahrungen aus dem big5 Datensatz. df2 &lt;- big5 |&gt; select(O1:O3) df2 # A tibble: 200 × 3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5 1 5 2 5 3 5 3 3 3 5 4 2 5 3 # ℹ 196 more rows Stell dir vor, bei Frage 1 ist die Antwort 3 richtig, bei Frage 2 die Antwort 2 und bei Item 3 die Antwort 4. Dann würden diese richtigen Antworten dem answers Argument kombiniert übergeben werden. df2 |&gt; data_binary(answers = c(4, 1, 5)) # A tibble: 200 × 3 O1 O2 O3 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 1 1 2 0 0 1 3 0 0 1 4 0 0 0 # ℹ 196 more rows "],["descr.html", "Kapitel 7 Deskriptive Statistik 7.1 Lagemaße und Streuungsmaße 7.2 Häufigkeiten und Kontingenztafeln 7.3 Zusammenhangsmaße", " Kapitel 7 Deskriptive Statistik Die beschreibende Statistik bietet neben Informationen über die Verteilungen und Zusammenhänge der interessierenden Merkmale ebenfalls die Möglichkeit, unplausible Werte zu identifizieren, die mit den im vorherigen Kapitel gelernten Methoden entfernt oder korrigiert werden müssen. Die Angabe der in diesem Kapitel eingeführten Lage- und Streuungsmaße, Häufigkeiten und Zusammenhangsmaße dürfen in keiner wissenschaftlichen Arbeit fehlen. 7.1 Lagemaße und Streuungsmaße Für dieses Kapitel muss das tidyverse installiert und geladen werden. library(tidyverse) Wir werden in diesem Kapitel den big5_mod Datensatz ohne die Spalte namens ID verwenden und bezeichnen diesen als big5_mod1. big5_mod1 &lt;- big5_mod |&gt; select(-ID) big5_mod1 # A tibble: 200 × 5 Alter Geschlecht Extraversion Neurotizismus Gruppe &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 36 m 3 1.9 Mittel 2 30 f 3.1 3.4 Jung 3 23 m 3.4 2.4 Jung 4 54 m 3.3 4.2 Weise # ℹ 196 more rows Einen ersten Überblick über die Daten können wir mit der direkt in R integrierten Funktion summary() erhalten, der wir lediglich den Namen des Datensatzes übergeben müssen. summary(big5_mod1) Alter Geschlecht Extraversion Neurotizismus Gruppe Min. :13.00 Length:200 Min. :2.300 Min. :1.400 Jung :147 1st Qu.:18.75 Class :character 1st Qu.:2.800 1st Qu.:2.700 Mittel: 39 Median :23.00 Mode :character Median :3.000 Median :3.100 Weise : 14 Mean :26.48 Mean :3.076 Mean :3.133 3rd Qu.:31.25 3rd Qu.:3.300 3rd Qu.:3.600 Max. :60.00 Max. :4.300 Max. :4.600 Hier sehen wir einen Vorteil von Faktoren. Während wir die Häufigkeiten der Altersgruppe ausgegeben bekommen, kann R für die Spalte mit Geschlecht lediglich die Anzahl an Character Werten zählen. Für weitere Informationen über Faktoren und deren Umgang schaue dir erneut Kapitel 4.3.2 und 6.10 an. Falls wir gruppierte Lage- und Streuungsmaße erhalten oder andere Werte wie den Standardfehler ausrechnen möchten, müssen wir auf die summarise() Funktion aus dem Package dplyr (enthalten im tidyverse) zurückgreifen. Auch hier werden wir einzelne Befehle mit der Pipe (|&gt;) aneinanderketten (siehe Kapitel 6.1). Exemplarisch soll zunächst der Mittelwert mit der Funktion mean() und die Standardabweichung mit sd() berechnet werden. Beide Funktionen müssen wir dabei innerhalb der Funktion summarise() verwenden. Auf der linken Seite des Gleichheitszeichens stehen auch hier wieder die Namen der neu erstellten Spalten. big5_mod1 |&gt; summarise( M = mean(Extraversion), SD = sd(Extraversion) ) # A tibble: 1 × 2 M SD &lt;dbl&gt; &lt;dbl&gt; 1 3.08 0.347 Zum Gruppieren der Variablen wird die Funktion group_by() verwendet. big5_mod1 |&gt; group_by(Geschlecht) |&gt; summarise( M = mean(Extraversion), SD = sd(Extraversion) ) # A tibble: 2 × 3 Geschlecht M SD &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 f 3.05 0.358 2 m 3.11 0.328 Dabei können der Funktion beliebig viele Spalten übergeben werden. So könnte man bspw. nicht nur nach Geschlecht, sondern auch nach der Altersgruppe gruppieren. big5_mod1 |&gt; group_by(Geschlecht, Gruppe) |&gt; summarise( M = mean(Extraversion), SD = sd(Extraversion) ) # A tibble: 6 × 4 # Groups: Geschlecht [2] Geschlecht Gruppe M SD &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 f Jung 3.07 0.373 2 f Mittel 3.07 0.299 3 f Weise 2.83 0.269 4 m Jung 3.12 0.324 # ℹ 2 more rows Alternativ können wir zum Gruppieren anstelle von group_by() auch das Argument .by hinzufügen. Während der Datensatz nach group_by() gruppiert ist, was bei weiterer Verwendung mit ungroup() aufgelöst werden müsste, gruppiert das .by Argument nur einmalig. Das Argument kann auch mit der in Kapitel 6.4 kennengelernten Funktion mutate() verwendet werden. Da die Ausgabe von summarise() in der Regel bereits ein Endergebnis darstellt, ist die Wahl zwischen group_by() oder .by größtenteils abhängig von persönlicher Präferenz. big5_mod1 |&gt; summarise( M = mean(Extraversion), SD = sd(Extraversion), .by = c(Geschlecht, Gruppe) ) # A tibble: 6 × 4 Geschlecht Gruppe M SD &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 m Mittel 3.13 0.281 2 f Jung 3.07 0.373 3 m Jung 3.12 0.324 4 m Weise 2.96 0.546 # ℹ 2 more rows Neben der Funktionen für den Mittelwert und der Standardabweichung gibt es noch diverse weitere: n() für die Anzahl an Beobachtungen, min() und max() für Minimum und Maximum, var() für die Varianz, sqrt() für die Quadratwurzel, median() für den Median und quantile() zur Berechnung der jeweiligen Quantile. Für die Berechnung des Standardfehlers teilen wir direkt innerhalb des Funktionsaufrufes die Standardabweichung durch die Wurzel aus der Anzahl der Personen. big5_mod1 |&gt; summarise( N = n(), Min = min(Alter), Mean = mean(Alter), Median = median(Alter), Max = max(Alter), SD = sd(Alter), SE = SD / sqrt(N) ) Grundsätzlich kann jede Funktion summarise() übergeben werden, die einen einzelnen Wert berechnet. Somit unterscheidet sich die Anwendung maßgeblich vom bereits kennengelernten mutate(). Dort musste die Ausgabe immer eine Reihe von Werten umfassen, die der Anzahl der Zeilen im Datensatz entspricht. Auch hier können wir mehrere Spalten gleichzeitig mithilfe von across() auswerten (siehe Kapitel 6.4.2). Die Syntax ändert sich in dem Fall im Vergleich zu vorher. Hier müssen wir die verschiedenen Funktionen mit entsprechendem Namen innerhalb einer Liste übergeben. Listen als solche werden erst später eingeführt und müssen uns an dieser Stelle nicht weiter interessieren (siehe Kapitel 11.4). big5_mod1 |&gt; summarise(across( .cols = Extraversion:Neurotizismus, .fns = list(M = mean, SD = sd)) ) # A tibble: 1 × 4 Extraversion_M Extraversion_SD Neurotizismus_M Neurotizismus_SD &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.08 0.347 3.13 0.682 Bei vielen Lagemaßen kann es schnell umständlich werden, jede einzeln aufzurufen. Wenn man keine Lust hat, dies jedes Mal manuell abzutippen, kann man auch direkt vereinfachte Funktionen zur deskriptiven Statistik verwenden. Das remp Package bietet die Funktion descriptive() an. Dieser muss man kein weiteres Argument übergeben. Es wird die Anzahl, das Minimum, das erste Quartil, der Mittelwert, der Median, das zweite Quartil, die Standardabweichung und der Standardfehler für sämtliche numerische Spalten zurückgegeben. Alle anderen Datentypen werden von dieser Funktion ignoriert. big5_mod1 |&gt; descriptive() # A tibble: 3 × 10 Variable N Min Q1 Mean Median Q3 Max SD SE &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Alter 200 13 18.8 26.5 23 31.2 60 11.4 0.8 2 Extraversion 200 2.3 2.8 3.08 3 3.3 4.3 0.35 0.02 3 Neurotizismus 200 1.4 2.7 3.13 3.1 3.6 4.6 0.68 0.05 Auch hier können wir die Berechnungen auf dieselbe Art und Weise gruppieren. big5_mod1 |&gt; group_by(Geschlecht) |&gt; descriptive() Alternativen für einen schnellen Überblick bieten beispielsweise auch das skimr Package mit der Funktion skim() oder describe() aus dem psych Package. Beide können auch nicht-numerische Spalten auswerten und erstere gibt zu jeder Spalte sogar ein kleines Histogramm aus. Wie dir vielleicht bereits aufgefallen ist, sieht die Ausgabe von descriptive() anders aus als die von summarise(). Während erstere die Variablen untereinander in unterschiedliche Zeilen übersichtlich auflistet, fügt summarise() die Ergebnisse spaltenweise hinzu. Wenn wir denselben Output wie in descriptive() erreichen möchten, müssen wir zuerst den Datensatz in ein langes Format bringen (siehe Kapitel 6.6). Nun gruppieren wir nach der neuen Spalte namens Variable. Anschließend können wir wie gewohnt mit summarise() die deskriptiven Statistiken berechnen. Nichts anderes macht die Funktion descriptive() hinter den Kulissen. big5_mod1 |&gt; pivot_longer( cols = c(Alter, Extraversion, Neurotizismus), names_to = &quot;Variable&quot;, values_to = &quot;Wert&quot; ) |&gt; group_by(Variable) |&gt; summarise( Q1 = quantile(Wert, 0.25), Mean = mean(Wert), Q3 = quantile(Wert, 0.75), Schiefe = skewness(Wert), Kurtosis = kurtosis(Wert) ) # A tibble: 3 × 6 Variable Q1 Mean Q3 Schiefe Kurtosis &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Alter 18.8 26.5 31.2 1.34 3.97 2 Extraversion 2.8 3.08 3.3 0.761 3.95 3 Neurotizismus 2.7 3.13 3.6 -0.132 2.56 Beachte an dieser Stelle, dass für die Funktionen skewness() und kurtosis() das moments Package installiert und geladen sein muss. Übung 7.1. (Noch nicht enthalten) Starte die Übung mit uebung_starten(7.1). 7.2 Häufigkeiten und Kontingenztafeln Auch in diesem Unterkapitel verwenden wir unter anderem die Packages aus dem tidyverse. library(tidyverse) Eine Möglichkeit Häufigkeiten zu zählen, haben wir bereits mit n() innerhalb von summarise() kennengelernt. Eine Alternative stellt die count() Funktion aus selbigem Package dar. Hier können wir uns group_by() sparen und stattdessen die gruppierenden Spalten direkt in count() schreiben. big5_mod |&gt; count(Geschlecht) # A tibble: 2 × 2 Geschlecht n &lt;chr&gt; &lt;int&gt; 1 f 118 2 m 82 Dabei können beliebig viele Spalten übergeben werden. Um die höchste Anzahl zu Beginn auszugeben, würden wir zusätzlich das sort Argument auf TRUE setzen. big5_mod |&gt; count(Geschlecht, Gruppe, sort = TRUE) # A tibble: 6 × 3 Geschlecht Gruppe n &lt;chr&gt; &lt;fct&gt; &lt;int&gt; 1 f Jung 89 2 m Jung 58 3 f Mittel 20 4 m Mittel 19 # ℹ 2 more rows Zum Darstellen der Anteile der jeweiligen Gruppen, müssen wir innerhalb von mutate() die einzelnen Häufigkeiten durch Anzahl in der jeweiligen Gruppe teilen (siehe Kapitel 6.4). big5_mod |&gt; count(Geschlecht, Gruppe) |&gt; mutate(Prop = n / sum(n)) # A tibble: 6 × 4 Geschlecht Gruppe n Prop &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; 1 f Jung 89 0.445 2 f Mittel 20 0.1 3 f Weise 9 0.045 4 m Jung 58 0.29 5 m Mittel 19 0.095 6 m Weise 5 0.025 Hier werden die Verhältnisse über alle Häufigkeiten berechnet, da count() keinen gruppierten Datensatz zurückgibt. Sollen die Häufigkeiten innerhalb einer Gruppe berechnet werden, muss zusätzlich mithilfe von group_by() explizit neu gruppiert werden. big5_mod |&gt; count(Geschlecht, Gruppe) |&gt; group_by(Geschlecht) |&gt; mutate(Prop = n / sum(n)) |&gt; ungroup() # A tibble: 6 × 4 Geschlecht Gruppe n Prop &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; 1 f Jung 89 0.754 2 f Mittel 20 0.169 3 f Weise 9 0.0763 4 m Jung 58 0.707 5 m Mittel 19 0.232 6 m Weise 5 0.0610 Für die Erstellung von Kontingenztafeln, die wir mit statistischen Tests auswerten können, benötigen wir allerdings eine andere Funktion namens table(). Damit können wir ähnlich wie mit count() die Häufigkeiten kategorialer Merkmale abbilden. Die Werte müssen dabei als Wertereihe (bzw. Vektor) mit dem Dollar-Operator aus dem Datensatz extrahiert werden (siehe Kapitel 4.5). table(big5_mod$Geschlecht) f m 118 82 Zur Erstellung einer klassischen Vierfeldertafel wird eine zusätzliche Variable in die Funktion geschrieben. Wenn wir also untersuchen möchten, ob es Unterschiede in der Häufigkeit extrovertierter Menschen zwischen den Geschlechtern gibt, wählen wir erst die Spalte Extraversion des Datensatzes big5_mod und anschließend die Spalte Geschlecht. Beachte die logische Abfrage bei der Extraversion: wenn Extraversion größer als 3 ist, wird eine 1 und ansonsten eine 0 zurückgegeben. tb &lt;- table(big5_mod$Extraversion &gt; 3, big5_mod$Geschlecht) tb f m FALSE 66 43 TRUE 52 39 Es können beliebig viele Häufigkeiten von Merkmalen miteinander in Beziehung gesetzt werden. Allerdings sind mehr als drei Dimensionen nur selten sinnvoll zu interpretieren. table(big5_mod$Extraversion &gt; 3, big5_mod$Geschlecht, big5_mod$Neurotizismus &gt; 3.5) , , = FALSE f m FALSE 47 39 TRUE 33 23 , , = TRUE f m FALSE 19 4 TRUE 19 16 Nach Erstellen der Kontingenztafeln mithilfe von table() können die Verhältnisse der Merkmale mit der Funktion prop.table() ausgegeben werden. Dies stellt eine Alternative zur bereits kennengelernten Kombination aus count() und mutate() dar. prop.table(tb) f m FALSE 0.330 0.215 TRUE 0.260 0.195 Der Vorteil ist hier, dass wir zusätzlich bestimmen können, ob wir auf die Zeilen oder Spalten bedingen möchten. Zum Konditionieren auf die Zeilen, muss nur eine 1 hinzugefügt werden. prop.table(tb, 1) f m FALSE 0.6055046 0.3944954 TRUE 0.5714286 0.4285714 Äquivalent dazu wird mit einer zusätzlichen 2 auf die Spalten bedingt. prop.table(tb, 2) f m FALSE 0.5593220 0.5243902 TRUE 0.4406780 0.4756098 Möchte man die Kontingenztafel für jede Spalte ausgeben lassen, kann table() ohne Klammern in der Funktion map() genutzt werden. Diese wendet table() auf jeden Spalte der Reihe nach an und speichert diese in Form einer Liste ab. big5_mod |&gt; map(table) Zur besseren Übersicht schalten wir an dieser Stelle noch ein select() davor, um nur das Geschlecht und die Altersgruppe auszuwählen. Wir könnten aber auf dieselbe Art und Weise sämtlich Spalten in einer Kontingenztafel ausgeben lassen. big5_mod |&gt; select(Geschlecht, Gruppe) |&gt; map(table) $Geschlecht f m 118 82 $Gruppe Jung Mittel Weise 147 39 14 So können auch alle Verhältnisse ausgegeben werden. big5_mod |&gt; select(Geschlecht, Gruppe) |&gt; map(table) |&gt; map(prop.table) $Geschlecht f m 0.59 0.41 $Gruppe Jung Mittel Weise 0.735 0.195 0.070 Diese Art des wiederholten Berechnens einer Funktion wird in Kapitel 12 genauer erläutert. Vom Prinzip her ist die Anwendung von map() in diesem Fall wie das Nutzen von across() innerhalb von mutate() (siehe Kapitel 6.4.2). Allerdings muss die Ausgabe in mutate() oder auch in summarise() das gleiche Format für alle Spalten haben. Wenn wir Kontingenztafeln mit unterschiedlich vielen Kategorien in den jeweiligen Spalten erstellen, ist diese Bedingung jedoch nicht erfüllt. Daher müssen wir in diesem Fall auf map() zurückgreifen. Übung 7.2. (Noch nicht enthalten) Starte die Übung mit uebung_starten(7.2). 7.3 Zusammenhangsmaße Zwei der wichtigsten Zusammenhangsmaße sind die Kovarianz und die darauf basierende Korrelation. Eine Korrelation berechnen wir mit der cor() Funktion, welche standardmäßig die Produkt-Moment-Korrelation berechnet. cor(big5_mod$Extraversion, big5_mod$Neurotizismus) [1] 0.06972529 Für die Rangkorrelation nach Spearman oder die Kendall-Tau Korrelation muss das Argument method auf \"spearman\" oder kendall gesetzt werden. cor(big5_mod$Extraversion, big5_mod$Neurotizismus, method = &quot;spearman&quot;) [1] 0.04874732 Um mehrere Korrelationen auf einmal zu berechnen, übergeben wir der Funktion cor() mehr als zwei Spalten. Dabei können wir der Funktion grundsätzlich unbegrenzt viele numerische Spalten übergeben. Erst wählen wir die gewünschten Spalten aus und bezeichnen das Zwischenergebnis als big5_sub. big5_sub &lt;- big5_mod |&gt; select(Alter, Extraversion, Neurotizismus) Diesen Datensatz mit ausschließlich numerischen Spalten übergeben wir dann der Funktion cor(). cor(big5_sub) Alter Extraversion Neurotizismus Alter 1.0000000 -0.12250136 -0.23019948 Extraversion -0.1225014 1.00000000 0.06972529 Neurotizismus -0.2301995 0.06972529 1.00000000 Auf dieselbe Art und Weise kann die Kovarianz mithilfe der Funktion cov() berechnet werden. cov(big5_sub) Alter Extraversion Neurotizismus Alter 128.8837940 -0.48201005 -1.78158291 Extraversion -0.4820101 0.12012462 0.01647437 Neurotizismus -1.7815829 0.01647437 0.46473467 Zusammenhangsmaße für Kontingenztafeln werden in Kapitel 9.6 besprochen. Übung 7.3. (Noch nicht enthalten) Starte die Übung mit uebung_starten(7.3). "],["visual.html", "Kapitel 8 Visualisierungen 8.1 Einführung 8.2 Histogramm und Dichte 8.3 Streudiagramm 8.4 Boxplot 8.5 Violin Plot 8.6 Balkendiagramm 8.7 Liniendiagramm 8.8 Quantil-Quantil Plot 8.9 Mehrfaktorielle Abbildungen 8.10 Anpassen des Aussehens 8.11 Anordnen mehrerer Graphen 8.12 Speichern von Abbildungen 8.13 Exemplarische Erweiterungen 8.14 Anwendungsbeispiel", " Kapitel 8 Visualisierungen Eine der informativsten und zugänglichsten Form der Darstellung von Unterschieden oder Zusammenhängen ist ein Diagramm. Optisch ansprechende Abbildungen zu erstellen, die dann auch noch für wissenschaftliche Publikationen geeignet sind, war bisher allerdings oftmals ein Krampf. Innerhalb dieses Kapitels wird ein konsistentes Schema zur Erstellung und Anpassung einfacher bis komplexer Abbildungen verschiedener Art vorgestellt und an vielen praktischen Beispielen gefestigt. 8.1 Einführung Mit R können komplexe und dabei optisch ansprechende Abbildungen erstellt werden, die durch die umfangreichen Anpassungsmöglichkeiten gleichermaßen für wissenschaftlichen Publikationen oder in Unternehmen verwendet werden. Durch das Package ggplot können auf konsistente Art und Weise Histogramme, Streudiagramme, Boxplots, Balkendiagramme, Liniendiagramme und viele mehr erstellt werden. Die zwei g des im tidyverse enthaltenen Packages ggplot stehen für grammar of graphics, was frei als Grammatik der Abbildungen übersetzt werden kann. Für alle Unterkapitel muss folglich das tidyverse installiert und geladen sein. library(tidyverse) Wir werden auch hier mit der leicht modifizierten Variante des Big Five Datensatzes arbeiten. big5_mod # A tibble: 200 × 6 Alter Geschlecht Extraversion Neurotizismus Gruppe ID &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; 1 36 m 3 1.9 Mittel 1 2 30 f 3.1 3.4 Jung 2 3 23 m 3.4 2.4 Jung 3 4 54 m 3.3 4.2 Weise 4 # ℹ 196 more rows In Abbildung 8.1 sind die grundsätzlichen Komponenten einer ggplot Abbildung links und je ein entsprechendes Beispiel rechts abgebildet. Jeder ggplot besteht aus verschiedenen Layern, die untereinander gelegt werden. Um eine Abbildung zu erstellen, muss auf jeden Fall Data, Aestetics und Geometries vorhanden sein. Die Layer namens Scales und Theme sind hingegen optional und passen lediglich das Erscheinungsbild an. Wir werden uns in Kapitel 8.2 bis 8.8 die verschiedenen Geometries Layer anschauen. Erst in Kapitel 8.10 werden die Anpassungsmöglichkeiten mithilfe der Scales und Theme Layer umfangreich erklärt. Abbildung 8.1: Vereinfachte Anordnung der Layer im Rahmen der Grammar of Graphics mit Beispielen. Es ist wichtig zu verstehen, dass die Layer untereinander gelegt werden und man am Ende von unten auf die kreierte Abbildung schaut. Wenn man bspw. mehrere Geometries hintereinander in einen Plot einbaut, kann der zuletzt hinzugefügte den vorherigen vollständig oder teilweise überdecken. Bleiben wir bei dem Beispiel der Erstellung eines Histogramms aus Abbildung 8.1. ggplot(data = big5_mod, mapping = aes(x = Extraversion)) + geom_histogram() Innerhalb der Funktion ggplot() wird dem data Argument der Datensatz big5_mod übergeben. Aus diesem Datensatz möchten wir die Spalte Extraversion auf der x-Achse abbilden. Die ersten beiden Layer sind somit bereits vorhanden. Der Aesthetics Layer wird durch das mapping Argument hinzugefügt. Allerdings können wir die Spalte nicht einfach mit x = Extraversion hinzufügen, sondern benötigen die Helferfunktion aes(), der wir unsere Spalte übergeben. Dabei steht aes() für aesthetics (engl. für Ästhetik). Neben der Spalte, die auf der x-Achse abgebildet werden soll, kann auf dieselbe Art und Weise die y-Achse definiert werden. Welche Variable dabei auf welcher Achse angezeigt werden soll, kann von dir frei entschieden werden. Auch Argumente zur Veränderung des Aussehens wie color (Außenfarbe) oder fill (Füllfarbe) können hier innerhalb von aes() der Funktion ggplot übergeben werden. Der eigentliche Graph wird erst mit den geom_*() Funktionen hinzugefügt (z.B. geom_histogram()). Das Präfix geom ist dabei für jeden Geometry Layer derselbe. Wichtig ist an dieser Stelle noch das Pluszeichen (+), welches sämtliche Layer zusammenbindet und untereinander verbindet. Dies unterscheidet sich grundlegend von den restlichen Funktion innerhalb des tidyverse, die mithilfe der Pipe (|&gt;) kombiniert werden (siehe Kapitel 6). Im Gegensatz zu anderen Funktionen innerhalb des tidyverse können Funktionen aus dem Package ggplot2 nicht mit einer Pipe aneinander gebunden werden. Das hat ausschließlich historische Gründe, da zu der Zeit der Erstellung von ggplot2 die Pipe noch nicht existiert hat. Dies wird sich in Zukunft voraussichtlich auch nicht mehr ändern. Schauen wir uns die einzelnen Befehle einmal genauer an. Der Data Layer kreiert nur eine leere Fläche (siehe Abbildung 8.2 (a)). ggplot(data = big5_mod) Durch das Hinzufügen des Aesthetics Layers wird ein Raster angefertigt und die x-Achse mit der gewünschten Spalte Extraversion beschriftet (siehe Abbildung 8.2 (b)). Allerdings werden noch immer keine Werte angezeigt, da wir dafür den Geometry Layer benötigen. ggplot(data = big5_mod, mapping = aes(x = Extraversion)) Die meisten Funktionen des Geometry Layers beginnen mit dem Präfix geom_ und enden mit dem Namen der Abbildungsart (z.B. geom_histogram() für Histogramme oder geom_boxplot() für Boxplots). Das Ergebnis ist in Abbildung 8.2 (c)) illustriert. ggplot(data = big5_mod, mapping = aes(x = Extraversion)) + geom_histogram() Abbildung 8.2: (a) Nur Data Layer (b) Data und Aesthetics Layer und (c) Data, Aesthetics und Geometry Layer. Die beiden Namen der Argumente data und mapping schreiben wir im Folgenden nicht mehr explizit aus, weil die Reihenfolge dieser Argumente im Verlaufe des Buches dieselbe ist und es ohne übersichtlicher ist. Da das standardmäßige Aussehen mit dem hellgrauen Hintergrund in dieser Form nicht für wissenschaftliche Publikationen geeignet ist, werden wir in Kapitel 8.10 die notwendigen Anpassungsmöglichkeiten besprechen. 8.2 Histogramm und Dichte Das Histogramm wurde exemplarisch bereits zur Erklärung des Aufbau eines ggplots in der Einführung beschrieben. Histogramme verwendet man zur Abbildung von Häufigkeitsverteilungen kontinuierlicher Variablen. Zur Erstellung eines Histograms in R bedarf es nur der Zuweisung der interessierenden Variable für die x-Achse, da auf der y-Achse die Häufigkeiten dargestellt werden. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram() Für ein schlichteres Aussehen fügen wir in Abbildung 8.3 (b) noch eine schwarze Rahmenfarbe mit dem color Argument und eine weiße Füllungsfarbe mit fill hinzu (siehe Kapitel 8.10.1). Histogramme sind allerdings maßgeblich von der gewählten Breite der Balken abhängig. Bei zu wenigen Balken können Informationen der Verteilung verloren gehen, bei zu vielen hingegen irrelevante Trends erscheinen. Dieses kann entweder direkt mit der Anzahl der Balken (bins Argument) oder mit der Breite (binwidth Argument) verändert werden. Wir werden hier das Argument binwidth verwenden, da diesem auch eine Funktion übergeben werden kann. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = 0.2 ) Abbildung 8.3: Histogramme im Vergleich. Es gibt verschiedene Arten, eine möglichst optimale binwidth herauszufinden. Exemplarisch sei hier die Freedman-Diaconis Regel angewandt. Diese können wir mithilfe einer neuen anonymen Funktion dem binwidth Argument übergeben (siehe Kapitel 6.4.4). Das Ergebnis ist in Abbildung 8.4 (a) gezeigt. ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram( color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = \\(x) (max(x) - min(x)) / nclass.FD(x) ) Eine weitere Möglichkeit, die Verteilung der Extraversion unserer Population darzustellen, ist die Wahrscheinlichkeitsdichte. Dazu müssen wir lediglich das Suffix des Geometry Layers zu density (engl. für Dichte) verändern (siehe Abbildung 8.4 (b)). ggplot(big5_mod, aes(x = Extraversion)) + geom_density() Um das Histogramm gemeinsam mit der Wahrscheinlichkeitsdichte abzubilden, müssen sich beide auf derselben Skala befinden. Zum Beispiel könnte man die Häufigkeiten des Histogramms ebenfalls als Dichte ausdrücken. Dafür muss dem height und y Argument jeweils die berechnete Dichte (density) übergeben werden. Diese wird mithilfe der Helferfunktion after_stat() berechnet. Anschließend muss lediglich mit einem weiteren Pluszeichen die Dichtefunktion hinzugefügt werden. Für eine ansprechendere optische Darstellung sei eine graue Füllfarbe ergänzt, welche mithilfe des alpha Argument etwas durchsichtig wird. Das Ergebnis kann in Abbildung 8.4 (c) betrachtet werden. ggplot(big5_mod, aes(x = Extraversion, height = after_stat(density))) + geom_histogram( mapping = aes(y = after_stat(density)), binwidth = 0.2, color = &quot;black&quot;, fill = &quot;white&quot; ) + geom_density(fill = &quot;grey&quot;, alpha = 0.7) Abbildung 8.4: Histogramm und Wahrscheinlichkeitsdichte separat und kombiniert. 8.3 Streudiagramm Das Erstellen eines Streudiagramms in R benötigt das Festlegen der x-Achse und y-Achse (auch Punktdiagramm oder Scatter Plot genannt). Im Vergleich zum bereits kennengelernten Histogramm ändert sich sonst nur der Geometry Layer zu geom_point(). Exemplarisch sei hier die mittlere Extraversion gegen das Lebensalter in Jahren aufgetragen (siehe Abbildung 8.5 (a)). Auf weitere Parameter wie das Ändern der Farbe (color) oder Form (shape) verzichten wir an dieser Stelle. ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point() Bei größeren Datensätzen kann es passieren, dass die Punkte sich überlappen. Um das zu verhindern, kann die Position zu jitter verändert werden. Dies bewirkt eine leichte zufällige Variation jedes Datenpunktes (siehe Abbildung 8.5 (b)). ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) Mit geom_smooth() wird eine am besten passende Linie durch die Punkte gezogen. Wir entscheiden uns an dieser Stelle für eine lineare Regressionsgeraden (method = lm). Außerdem färben wir die Gerade schwarz und fügen ein 95%iges Konfidenzintervall mit se = TRUE hinzu. Damit die Regressionsgerade nicht nur in dem Bereich, in dem Daten beobachtet wurden, abgebildet wird, kann zusätzlich das fullrange Argument auf TRUE gesetzt werden. Um den Effekt dieser Funktion zu illustrieren, greifen wir etwas voraus und definieren mit xlim(c(2, 5)) die untere Grenze der x-Achse mit 2 und die obere mit 5 (siehe Abbildung 8.5 (c)). ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) + geom_smooth( color = &quot;black&quot;, method = lm, se = TRUE, fullrange = TRUE ) + xlim(c(2, 5)) Abbildung 8.5: Streudiagramm mit und ohne Regressionsgerade. 8.4 Boxplot Würde man nur einen Boxplot für die mittlere Ausprägung von Extraversion erstellen wollen, könnte man dies genau wie in den beiden zuvor besprochenen Kapiteln durch das Auswechseln des Geometry Layers mit geom_boxplot() erreichen (siehe Abbildung 8.6 (a)). ggplot(big5_mod, aes(y = Extraversion)) + geom_boxplot() Dies stellt allerdings eine seltene Situation dar. Meistens ist man am Vergleich mehrerer Variablen interessiert, die auf der x-Achse aufgetragen werden. An dieser Stelle sollen die Verteilungen von Extraversion und Neurotizismus miteinander verglichen werden. Um dies zu erreichen, müssen wir den Datensatz vom breiten ins lange Datenformat transformieren (siehe Kapitel 6.6). Dafür wird die Funktion pivot_longer() verwendet. big5_long &lt;- big5_mod |&gt; pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) big5_long # A tibble: 400 × 7 Alter Geschlecht Gruppe ID Faktor Auspraegung Zeitpunkt &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 36 m Mittel 1 Extraversion 3 T1 2 36 m Mittel 1 Neurotizismus 1.9 T1 3 30 f Jung 2 Extraversion 3.1 T1 4 30 f Jung 2 Neurotizismus 3.4 T1 # ℹ 396 more rows Nach der Umwandlung sind in big5_long die Persönlichkeitsfaktoren in der Spalte Faktor und die Extraversions- und Neurotizismusausprägung in der Spalte Auspraegung. Einen Boxplot erstellt man mit geom_boxplot(). Auf der x-Achse möchten wir die Persönlichkeitsfaktoren und auf der y-Achse die mittleren Ausprägungen darstellen (siehe Abbildung 8.6 (b)). ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_boxplot() Um zusätzlich Fehlerbalken zu erhalten, müssen diese mit stat_boxplot() berechnet werden. Das Argument geom muss auf \"errorbar\" (engl. für Fehlerbalken) gesetzt werden. Die Breite des Fehlerbalkens kann durch das optionale Argument width kontrolliert werden. Zum Ausblenden der Ausreißer setzt man innerhalb von geom_boxplot() die outlier.shape auf NA (Akronym für Not Available, engl. für nicht vorhanden). Das Ergebnis ist in Abbildung 8.6 (c) illustriert. ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + stat_boxplot(geom = &quot;errorbar&quot;, width = 0.4) + geom_boxplot(outlier.shape = NA) Abbildung 8.6: Boxplots im Vergleich. Beim Hinzufügen von Fehlerbalken ist die Reihenfolge der Funktionsaufrufe entscheidend, da die verschiedenen Layer untereinander gezeichnet werden. Würden wir also zunächst geom_boxplot() und erst anschließend stat_boxplot() zum ggplot hinzufügen, würde die Linie des Fehlerbalkens über dem Boxplot abgebildet werden. 8.5 Violin Plot Im Vergleich zu Boxplots ändert sich zum einen der Geometry Layer, welcher nun geom_violin() heißt und zum anderen muss noch eine willkürliche Koordinate für die x-Achse gewählt werden (hier 0) (siehe Abbildung 8.7 (a)). ggplot(big5_mod, aes(x = 0, y = Extraversion)) + geom_violin() Für mehrere Violin Plots nutzen wir genau wie zuvor bei den Boxplots den Datensatz im langen Datenformat namens big5_long (siehe Kapitel 8.4 und 6.6). Auch hier werden auf der x-Achse die Persönlichkeitsfaktoren und auf der y-Achse die jeweilige Ausprägung ausgegeben (siehe Abbildung 8.7 (b)). ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_violin() Optional kann zusätzlich das Argument trim auf FALSE gesetzt werden, um das Abschneiden der Enden des Violin Plots zu verhindern. Mithilfe des Arguments draw_quantiles können wir explizit beliebige Quantile (hier Quartile) einzeichnen lassen. Das Ergebnis ist in Abbildung 8.7 (c) zu sehen. ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75) ) Abbildung 8.7: Violin Plots im Vergleich. 8.6 Balkendiagramm In einem Balkendiagramm werden im Regelfall entweder Häufigkeiten oder Mittelwerte miteinander verglichen (auch Säulendiagramm oder Barplot genannt). Zum Darstellen absoluter Häufigkeiten kategorealer Variablen wird die Funktion geom_bar() verwendet. Wir können dabei frei entscheiden, ob die Balken vertikal oder horizontal angeordnet werden sollen. Für eine vertikale Illustration auf der x-Achse übergeben wir die Spalte Geschlecht dem x Argument (siehe Abbildung 8.8) (a)). ggplot(big5_mod, aes(x = Geschlecht)) + geom_bar() Äquivalent dazu können wir horizontale Balkendiagramme durch Abbildung der entsprechenden Spalte auf der y-Achse mit dem y Argument erstellen (siehe Abbildung 8.8) (b)). ggplot(big5_mod, aes(y = Geschlecht)) + geom_bar() Anstelle von absoluten Häufigkeiten können ebenfalls relative Häufigkeiten verglichen werden. Hierzu müssen wir innerhalb der Helferfunktion after_stat() das Argument prop übergeben und zeitgleich das group Argument auf 1 setzen (siehe Abbildung 8.8) (c)). Diese Helferfunktion haben wir bereits im Kontext der Histogramme in Kapitel 8.2 kennengelernt. ggplot(big5_mod, aes(x = Geschlecht, y = after_stat(prop), group = 1)) + geom_bar() Möchten wir Mittelwerte miteinander vergleichen, müssen wir auf die Funktion geom_col() zurückgreifen. Beim Gegenüberstellen mehrerer Merkmale, wird zunächst, wie bereits in den Kapiteln 8.4 und 8.5 bei den Boxplots und Violin Plots kennengelernt, der Datensatz in ein langes Format gebracht (siehe Kapitel 6.6). big5_long &lt;- big5_mod |&gt; pivot_longer( cols = Extraversion:Neurotizismus, names_to = &quot;Faktor&quot;, values_to = &quot;Auspraegung&quot; ) big5_long # A tibble: 400 × 7 Alter Geschlecht Gruppe ID Faktor Auspraegung Zeitpunkt &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 36 m Mittel 1 Extraversion 3 T1 2 36 m Mittel 1 Neurotizismus 1.9 T1 3 30 f Jung 2 Extraversion 3.1 T1 4 30 f Jung 2 Neurotizismus 3.4 T1 # ℹ 396 more rows Die abzubildenden Mittelwerte und Standardabweichungen berechnen wir vor der Visualisierung und speichern das Zwischenergebnis als big5_means ab. Die Standardabweichungen benötigen wir für die Fehlerbalken. Alternativ könnte man durch Teilen der Standardabweichung durch die Stichprobengröße auch den Standardfehler abbilden. Wie man diese Lage- und Streuungsmaße berechnet, wurde bereits in Kapitel 7.1 eingeführt. big5_means &lt;- big5_long |&gt; group_by(Faktor) |&gt; summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means # A tibble: 2 × 3 Faktor Mean SD &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion 3.08 0.347 2 Neurotizismus 3.13 0.682 Die Balken werden mit geom_col() erstellt. Auf der x-Achse sind demnach wie zuvor auch die Persönlichkeitsfaktoren und auf der y-Achse die Mittelwerte aufgetragen (siehe Abbildung 8.8 (d)). ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col() Verwechsle geom_col() nicht mit geom_bar(). Erstere Funktion stellt genau das dar, was man ihr übergibt (z.B. Mittelwerte). Letztere Funktion hingegen erstellt Balken mit absoluten oder relativen Häufigkeiten. Um das Balkendiagramm zu verschönern, können wir auch hier die Füllfarbe (fill) und die Rahmenfarbe (color) entsprechend anpassen (siehe Abbildung 8.8 (e)). ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col(fill = &quot;white&quot;, color = &quot;black&quot;) Zusätzlich bilden wir mit der Funktion geom_errorbar() die Standardabweichung (SD) ab, indem das Minimum des Fehlerbalken als Mittelwert minus der Standardabweichung und das Maximum als Mittelwert plus der Standardabweichung festlegt wird. Die Breite der Fehlerbalken wird mit dem Argument width verändert. Beachte an dieser Stelle, dass die Grenzen der Fehlerbalken (ymin und ymax) im Gegensatz zur Fehlerbalkenbreite innerhalb der Helferfunktion aes() definiert werden müssen. Das Ergebnis ist in Abbildung 8.8 (f) illustriert. ggplot(big5_means, aes(x = Faktor, y = Mean)) + geom_col(fill = &quot;white&quot;, color = &quot;black&quot;) + geom_errorbar( mapping = aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4 ) Falls du mehr als eine Gruppe innerhalb eines Balkendiagramms vergleichen möchtest, musst du ein Gruppierungsargument verwenden, welches in Kapitel 8.9 eingeführt wird. Abbildung 8.8: Verschiedene Balkendiagramme mit und ohne Fehlerbalken. 8.7 Liniendiagramm Bei Mittelwertvergleichen in Form von Liniendiagrammen ändert sich im Vergleich zu den Balkendiagrammen nur wenig. Auch hier verwenden wir wieder den Datensatz big5_means, der unsere Mittelwerte und Standardabweichungen für Extraversion und Neurotizismus enthält (siehe Kapitel 8.6. Zum Erstellen der Verbindungslinie zwischen den beiden Persönlichkeitsfaktoren muss das group Argument auf 1 gesetzt werden. Abschließend müssen wir noch die Linie mit geom_line(), die Mittelwerte als Punkte mit geom_point() und die Fehlerbalken mit geom_errorbar() erstellen. Auch bei den Fehlerbalken ändert sich nichts im Vergleich zu den Balkendiagrammen. Das Ergebnis ist in Abbildung 8.9 (a) illustriert. ggplot(big5_means, aes(x = Faktor, y = Mean, group = 1)) + geom_line() + geom_point() + geom_errorbar( mapping = aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2 ) Ein weiteres klassisches Beispiel eines Liniendiagramms ist die Abbildung von Zeitreihen. Dafür schauen wir uns den Kurs der Bitcoin-Aktie an. bitcoin # A tibble: 731 × 2 Datum Preis &lt;date&gt; &lt;dbl&gt; 1 2019-01-01 3844. 2 2019-01-02 3943. 3 2019-01-03 3837. 4 2019-01-04 3858. # ℹ 727 more rows Auf der x-Achse soll das Datum und auf der y-Achse der Preis bei geschlossener Börse in USD abgebildet werden. Die Zeitreihe wird wie zuvor mit geom_line() visualisiert. Wichtig ist hierbei, dass das Datum vom Datentyp Date ist (siehe Kapitel 6.11). Zusätzlich können wir, wie beim Streudiagramm in Kapitel 8.3, mit stat_smooth() eine am besten passendste Kurve zur Kursbeschreibung hinzufügen. Abschließend greifen wir an dieser Stelle etwas vor und verändern noch die Benennung der x-Achse mithilfe von scale_x_date(). Dabei gibt es verschiedene Möglichkeiten der Anzeige, die jeweils mit einem Prozentzeichen angeführt werden müssen. Hier zeigen wir den abgekürzten Monatsnamen (%b) und das entsprechende Jahr (%Y). Das Ergebnis ist in Abbildung 8.9 (b) illustriert. ggplot(bitcoin, aes(x = Datum, y = Preis)) + geom_line() + stat_smooth(color = &quot;black&quot;) + scale_x_date(date_labels = &quot;%b %Y&quot;) Eine weitere Anwendung finden Liniendiagramme bei sogenannten Scree Plots zur Auswahl der Anzahl der Faktoren für explorative Faktorenanalysen. Dafür benötigen wir den kompletten Big 5 Datensatz mit den einzelnen Fragen zu den Persönlichkeitsfaktoren namens big5_comp. Mit der im remp Package enthaltenen Funktion data_eigen() können die entsprechenden Eigenvalues berechnet werden, die wir im Scree Plot abbilden wollen. big5_erg &lt;- select(big5_comp, -Geschlecht) big5_scree &lt;- data_eigen(big5_erg) big5_scree # A tibble: 51 × 2 Eigenwerte Dimension &lt;dbl&gt; &lt;int&gt; 1 8.25 1 2 4.59 2 3 3.62 3 4 3.57 4 # ℹ 47 more rows Auf der x-Achse haben wir unsere verschiedenen Dimensionen und auf der y-Achse die Eigenwerte. Zusätzlich modifizieren wir die Funktionen geom_point() und geom_line() optisch leicht. Neu ist an dieser Stelle die Funktion geom_hline() (für horizontal line), welche eine horizontale Linie beim Schnittpunkt mit der y-Achse von 1 einzeichnet (siehe Abbildung 8.9 (c)). ggplot(big5_scree, aes(x = Dimension, y = Eigenwerte)) + geom_point(shape = 19, size = 2) + geom_line(linewidth = 0.6) + geom_hline( mapping = aes(yintercept = 1), linewidth = 0.8, linetype = &quot;longdash&quot; ) Abbildung 8.9: Abbildung von Liniendiagrammen als (a) Mittelwertsvergleich (b) Zeitreihe und (c) Scree Plot 8.8 Quantil-Quantil Plot Um mithilfe eines Q-Q Plots die Quantile zweier Verteilungen zu überprüfen, verwenden wir die Funktionen geom_qq() und geom_qq_line(). Ein häufiger Anwendungsfall ist die graphische Überprüfung auf Vorliegen einer Normverteilung eines intervallskalierten Merkmals oder der Residuen eines Regressionsmoodells. Daher ist der Vergleich mit einer Normalverteilung auch die Standardeinstellung innerhalb der Funktionen. Die interessierende Variable könnte bspw. die Leukozytenanzahl nach 6 Monaten Therapie sein (Leukos_t6), welche im chemo Datensatz enthalten ist. Diese übergeben wir dem sample Argument (engl. für Stichprobe) (siehe Abbildung 8.10). ggplot(chemo, aes(sample = Leukos_t6)) + geom_qq() + geom_qq_line() Abbildung 8.10: Q-Q Plots Möchte man die Verteilung eines Merkmals mit einer anderen Verteilung vergleichen, könnten wir mit dem distribution Argument die Quantile einer anderen Verteilung wie der Binomialverteilung (qbinom) oder der t-Verteilung (qt) festlegen. Beachte, dass hierbei keine Anführungszeichen verwendet werden. Bei der Überprüfung von Residuen werden dem sample Argument stattdessen die geschätzten Residuen übergeben (siehe Kapitel 8.13.2, 9.7.1 und 9.8). 8.9 Mehrfaktorielle Abbildungen In den bisherigen Abbildungen haben wir bislang nur jeweils zwei Variablen in Zusammenhang gesetzt, indem eine Variable auf der x-Achse und eine auf der y-Achse visualisiert wurde. Eine dritte Variable kann mit dem Gruppierungselement hinzugefügt werden (siehe Kapitel 8.9.1). Für das zeitgleiche Vergleichen einer vierten und fünften Variable innerhalb einer Abbildungen können sogenannte Facetten erstellt werden (siehe Kapitel 8.9.2). 8.9.1 Gruppierungsargumente Gruppierungen können in Abhängigkeit der gewünschten Darstellung durch die Argumente color, fill, linetype, size oder shape innerhalb der Helferfunktion aes() erstellt werden. Anders als bisher wird diesen Argumenten in diesem Fall kein Character (z.B. color = \"black\"), sondern das gruppierende Argument ohne Anführungszeichen übergeben (z.B. color = Geschlecht). Für einige Abbildungen wie bei Mittelwertsvergleichen in Form von Balken- oder Liniendiagrammen benötigen wir ein zusätzliches position Argument. Dieses spezifiziert, wie die Gruppen zueinander in Verhältnis zu setzen sind. Eine nützliche Wahl ist hierfür die Funktion position_dodge(0.95) (engl. für ausweichen), welche die Gruppen direkt nebeneinander darstellt. Die Zahl in der Klammer steht für den genauen Abstand zwischen den Elementen. Alternativ könnte man bei Balkendiagrammen auch position = \"stack\" für eine aufeinander gestapelte Ansicht pro Kategorie verwenden. Wenn man stattdessen position = \"fill\" verwendet, werden diese übereinander gestapelten Anteile auf 1 standardisiert, sodass man die Verhältnisse besser vergleichen kann. Zum Abbilden der mittleren Ausprägung von Extraversion und Neurotizismus unterteilt nach Geschlecht in Form eines Boxplots verwenden wir den Datensatz big5_long aus dem remp Package. big5_long # A tibble: 400 × 7 Alter Geschlecht Gruppe ID Faktor Auspraegung Zeitpunkt &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 36 m Mittel 1 Extraversion 3 T1 2 36 m Mittel 1 Neurotizismus 1.9 T1 3 30 f Jung 2 Extraversion 3.1 T1 4 30 f Jung 2 Neurotizismus 3.4 T1 # ℹ 396 more rows Die Füllfarbe (fill) machen wir abhängig vom Geschlecht und das Positionsargument wird ebenfalls entsprechend angepasst werden. Das Ergebnis ist in Abbildung 8.11 (a) illustriert. ggplot(big5_long, aes(x = Faktor, y = Auspraegung, fill = Geschlecht)) + geom_boxplot(outlier.shape = NA, position = position_dodge(0.95)) Falls zusätzlich ein Fehlerbalken angezeigt werden soll, muss auch in stat_boxplot() das Positionsargument verwendet werden (siehe Abbildung 8.11 (b)). ggplot(big5_long, aes(x = Faktor, y = Auspraegung, fill = Geschlecht)) + stat_boxplot( geom = &quot;errorbar&quot;, width = 0.4, position = position_dodge(0.95) ) + geom_boxplot(outlier.shape = NA, position = position_dodge(0.95)) Abbildung 8.11: Boxplots mit und ohne Fehlerbalken nach drei Variablen gruppiert. Wenn Mittelwerte in Form von Balken- oder Liniendiagrammen unterteilt nach einer dritten Variable miteinander verglichen werden sollen, müssen diese zuerst berechnet werden (siehe Kapitel 7). Dabei müssen wir zusätzlich nach der dritten Variable (hier Geschlecht) gruppieren. big5_means2 &lt;- big5_long |&gt; group_by(Faktor, Geschlecht) |&gt; summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means2 # A tibble: 4 × 4 # Groups: Faktor [2] Faktor Geschlecht Mean SD &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f 3.05 0.358 2 Extraversion m 3.11 0.328 3 Neurotizismus f 3.25 0.633 4 Neurotizismus m 2.96 0.718 Ansonsten ändert sich im Vergleich zum vorherigen Beispiel nichts. Auch hier muss die Füllfarbe (fill = Geschlecht) und die Position der gruppierten Balken definiert werden. Das Ergebnis ist in Abbildung 8.12 (a) illustriert. ggplot(big5_means2, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95), color = &quot;black&quot;) + geom_errorbar( mapping = aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) Ein weiterer Anwendungsfall sind gruppierte Liniendiagramme. Hierfür ändern wir an dieser Stelle das Argument fill zu linetype. Zusätzlich muss das Gruppierungsargument (hier Geschlecht) den einzelnen Funktionen übergeben werden, da sonst keine Linien zwischen den Gruppen gezeichnet werden würden (siehe Abbildung 8.12 (b)). ggplot(big5_means2, aes(x = Faktor, y = Mean, linetype = Geschlecht)) + geom_line( mapping = aes(group = Geschlecht), position = position_dodge(0.2) ) + geom_point(position = position_dodge(0.2)) + geom_errorbar( mapping = aes(group = Geschlecht, ymin = Mean - SD, ymax = Mean + SD), width = 0.2, position = position_dodge(0.2) ) Abbildung 8.12: Balken- und Liniendiagramme mit drei Variablen. 8.9.2 Facetten als weitere Dimensionen Für das Hinzufügen einer vierten Variable ändert sich im Vergleich zu Kapitel 8.9.1 nur die zusätzliche Funktion facet_wrap(), welche die vierte Variable in Form einer sogenannten Facette abbildet. Auch hier verwenden wir den big5_long Datensatz aus dem remp Package. big5_long # A tibble: 400 × 7 Alter Geschlecht Gruppe ID Faktor Auspraegung Zeitpunkt &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 36 m Mittel 1 Extraversion 3 T1 2 36 m Mittel 1 Neurotizismus 1.9 T1 3 30 f Jung 2 Extraversion 3.1 T1 4 30 f Jung 2 Neurotizismus 3.4 T1 # ℹ 396 more rows Innerhalb von facet_wrap() wird erst eine Tilde und anschließend der gewünschte Spaltenname geschrieben. Zusätzliche könnte man mit dem Argument ncol die Anzahl der Spalten festlegen. ggplot(big5_long, aes(x = Faktor, y = Auspraegung, fill = Geschlecht)) + geom_boxplot(outlier.shape = NA, position = position_dodge(0.95)) + facet_wrap(~ Gruppe) Abbildung 8.13: Boxplot mit vier Variablen. Wir sehen in Abbildung 8.13, dass die Altersgruppen in einem Raster als separate Graphen angezeigt werden. Die Anzahl der Spalten in der Anordnung können mit dem ncol Argument angepasst werden (z.B. ncol = 2). Mit dem scales Argument ist es möglich, die x-Achse (scales = \"free_x\") oder y-Achse (scales = \"free_y\") auf unterschiedlichen Skalen anzeigen zu lassen. Äquivalent zum Abbilden von drei Variablen in Kapitel 8.9.1, müssen beim Mittelwertsvergleich in Form von Balken- oder Liniendiagrammen auch hier erst die entsprechenden Werte berechnet werden. Der Unterschied ist die zusätzlich gruppierende Variable (hier Gruppe) innerhalb von group_by() (siehe Kapitel 7). big5_means3 &lt;- big5_long |&gt; group_by(Faktor, Geschlecht, Gruppe) |&gt; summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means3 # A tibble: 12 × 5 # Groups: Faktor, Geschlecht [4] Faktor Geschlecht Gruppe Mean SD &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f Jung 3.07 0.373 2 Extraversion f Mittel 3.07 0.299 3 Extraversion f Weise 2.83 0.269 4 Extraversion m Jung 3.12 0.324 # ℹ 8 more rows Anschließend können auf dieselbe Art und Weise mithilfe von facet_wrap() die Altersgruppen eingefügt werden. ggplot(big5_means3, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95), color = &quot;black&quot;) + geom_errorbar( mapping = aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) + facet_wrap(~ Gruppe) Auch beim Hinzufügen einer fünften Variable finden wir das gleiche Prinzip vor. Für fünf Variablen ändert sich die Funktion zu facet_grid(). Die Variable, welche in den Zeilen des Rasters (hier Zeitpunkt) abgebildet werden soll, wird auf die linke Seite der Tilde und die Variable für die Spalten (hier Gruppe) auf die rechte Seite geschrieben. ggplot(big5_long, aes(x = Faktor, y = Auspraegung, fill = Geschlecht)) + geom_boxplot(outlier.shape = NA, position = position_dodge(0.95)) + facet_grid(Zeitpunkt ~ Gruppe) Abbildung 8.14: Boxplot mit fünf Variablen. Bei Mittelwertsvergleichen müssen diese mit group_by() und summarise() durch das Hinzufügen einer vierten gruppierenden Variable zunächst berechnet werden. big5_means4 &lt;- big5_long |&gt; group_by(Faktor, Geschlecht, Gruppe, Zeitpunkt) |&gt; summarise( Mean = mean(Auspraegung, na.rm = TRUE), SD = sd(Auspraegung, na.rm = TRUE) ) big5_means4 # A tibble: 24 × 6 # Groups: Faktor, Geschlecht, Gruppe [12] Faktor Geschlecht Gruppe Zeitpunkt Mean SD &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion f Jung T1 3.08 0.391 2 Extraversion f Jung T2 3.06 0.362 3 Extraversion f Mittel T1 2.93 0.121 4 Extraversion f Mittel T2 3.13 0.336 # ℹ 20 more rows Anschließend kann auch hier ein Raster mithilfe von facet_grid() erstellt werden. ggplot(big5_means4, aes(x = Faktor, y = Mean, fill = Geschlecht)) + geom_col(position = position_dodge(0.95), color = &quot;black&quot;) + geom_errorbar( mapping = aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.4, position = position_dodge(0.95) ) + facet_grid(Zeitpunkt ~ Gruppe) 8.10 Anpassen des Aussehens Die Standardeinstellungen von ggplot sind praktisch für einen ersten Überblick, aber eignen sich nicht zur Veröffentlichung der Abbildungen in einer wissenschaftlichen Publikation. In den folgenden Kapiteln wird erklärt, wie man Farben, Texte, die Legende, die Achsen und vieles mehr verändert. Wir werden uns anhand der Abbildung aus Kapitel 8.9.1 die verschiedenen Anpassungsmöglichkeiten anschauen. Zusätzlich fügen wir bereits an dieser Stelle das alpha Argument hinzu, welches die Deckkraft mit 0.8 auf 80% hinuntersetzt. Dies sorgt für ein hochwertigeres Aussehen der Farben. Der gruppierte Boxplot wird als p gespeichert und in den folgenden Kapiteln verwendet. p &lt;- ggplot(big5_long, aes(x = Faktor, y = Auspraegung, fill = Geschlecht)) + geom_boxplot(outlier.shape = NA, position = position_dodge(0.95), alpha = 0.8) In Abbildung 8.15 ist die Standardeinstellung von ggplot unserer modifizierten Abbildungen gegenübergestellt. Die verschiedenen Anpassungen können ebenfalls mit einem Pluszeichen (+) aneinandergekettet werden. Wofür welche Funktion genau zuständig ist, wird im weiteren Verlauf kleinschrittig eingeführt. p + scale_fill_brewer( palette = &quot;Dark2&quot;, name = &quot;Geschlecht (Auswahl):&quot;, labels = c(&quot;Weiblich&quot;, &quot;Männlich&quot;) ) + scale_y_continuous(expand = c(0, 0), limits = c(1, 5.2), breaks = 1:5) + labs(x = &quot;Persönlichkeitsfaktor&quot;, y = &quot;Mittlere Ausprägung&quot;) + theme_classic(base_size = 14, base_family = &quot;sans&quot;) + theme( legend.position = c(0.4, 0.9), axis.title.y = element_text(vjust = 2), axis.title.x = element_text(vjust = 0.5), axis.text = element_text(size = 13) ) Abbildung 8.15: Standardausgabe im Vergleich mit einer publikationsreifen Abbildung. 8.10.1 Farben Beim Verändern der Farben innerhalb einer Abbildung unterscheiden wir zwischen der Rahmenfarbe (color) und Füllfarbe (fill). Während die Rahmenfarbe häufig als schwarz gewählt werden sollte, hat man mit der Füllfarbe mehr Freiheiten. Das Farbschema kann durch verschiedene Farbpaletten adaptiert werden. Häufig sind im wissenschaftlichen Kontext jedoch nur Graustufen erwünscht, welche wir mit scale_fill_grey() erstellen können. Wichtig ist an dieser Stelle das start und end Argument der Farbpalette, da die Grautöne sonst zu dunkel für das Erkennen von Fehlerbalken sein können. Hätten wir in unserem ggplot nicht die Füllfarbe (fill), sondern die Rahmenfarbe (color) verändert, würde man stattdessen die Funktion scale_color_grey() verwenden. Die brewer Paletten enthalten verschiedene Farben wie blau (\"Blues\") oder rot (\"Reds\"). Diese werden mit der Funktion scale_fill_brewer() erstellt. Mithilfe von scale_fill_manual() können Farben in Form von Hexadezimal Farbkodierungen als einzelne Werte übergeben werden. a &lt;- p + scale_fill_grey(start = 0.3, end = 0.7) b &lt;- p + scale_fill_brewer(palette = &quot;Blues&quot;) c &lt;- p + scale_fill_manual(values = c(&quot;#A50F15&quot;, &quot;#FC9272&quot;)) Abbildung 8.16: Vergleich verschiedener Farbpaletten. Bei der Auswahl einzelner Werte sollte ebenfalls auf Farben innerhalb von Paletten zurückgegriffen werden, da diese aufeinander abgestimmt sind. Es ist generell davon abzuraten, eigene Farbkombinationen zu wählen, da diese oft einen unprofessionellen Eindruck erwecken. Die Hexadezimal-Farbkodierung erhält man entweder durch Nachschlagen in einer Suchmaschine oder durch Nachschauen in den Farbpaletten. Dafür müssen die Packages scales und RColorBrewer installiert und geladen werden. library(scales) library(RColorBrewer) Die Rottöne aus Abbildung 8.16 wurden bspw. aus der Brewer Farbpalette ausgewählt, welche von ggplot2 direkt zur Verfügung gestellt werden. Zum Anzeigen von 9 Farben der roten Farbpalette kombinieren wir die Funktion brewer.pal() mit show_col(). Beachte die Trennung der Funktionsnamen einmal mit Punkt und einmal mit Unterstrich. Neben den Rottönen könnte man so auch die genauen Farben der Dark2 Palette herausfinden. show_col(brewer.pal(9, &quot;Reds&quot;)) show_col(brewer.pal(8, &quot;Dark2&quot;)) Abbildung 8.17: Farbpaletten mit Hexadezimalcodes der Rottöne und Dark2. Eine ansprechende Farbpalette mit Berücksichtigung von Farbblindheit ist durch die Viridis Palette gegeben. Diese Farbpalette erstellt man mit scale_fill_viridis_d(begin = 0.27, end = 0.72, option = \"C\"). Wie bei den Graustufen zuvor können auch hier die Start- und Entwerte angepasst werden. Das optionale opt Argument wählt hier die dritte von acht möglichen Viridis Paletten aus. Für kontinuierliche Skalen muss stattdessen auf die Funktion scale_fill_viridis_c() zurückgegriffen werden. 8.10.2 Themen und Achsen Der hellgraue Standardhintergrund und die fehlende Visualisierung der Achsen ist in der Wissenschaft in der Regel nicht erwünscht. Ein gutes minimales Thema ist theme_classic(). Innerhalb der theme_*() Funktionen kann die Textgröße und -art direkt angepasst werden. Relativ zur Basisgröße (base_size) sind andere Elemente wie Überschriften entsprechend größer. Welches die richtige Größe ist, hängt maßgeblich von den Dimensionen der Abbildung und somit der Auflösung ab (siehe Kapitel 8.12). Weitere Themen sind z.B. theme_minimal() oder theme_bw(). a &lt;- p + theme_classic(base_size = 14, base_family = &quot;sans&quot;) b &lt;- p + theme_minimal() c &lt;- p + theme_bw() Abbildung 8.18: Vergleich verschiedener Themen aus ggplot. Neben den direkt in ggplot enthaltenen Themen, stellt das ggthemes Package noch weitere Themen zur Verfügung. Die Beschriftung der x-Achse und y-Achse wird mithilfe der Funktion labs() (Akronym für labels, engl. für Beschriftung) angepasst. p + labs(x = &quot;Persönlichkeitsfaktor&quot;, y = &quot;Mittlere Ausprägung&quot;) Zum Verändern der Textgröße, -ausrichtung und adjustierung, verwenden wir die Funktion theme(). Jedem Argument innerhalb von theme() müssen die Werte (wie z.B. die Textgröße) innerhalb der Helferfunktion element_text() übergeben werden. Das Argument vjust schafft etwas Raum zwischen der Achsenbeschriftung und dem Text der Achsen. p + theme( axis.title.y = element_text(vjust = 2), axis.title.x = element_text(vjust = 0.5), axis.text = element_text(size = 13) ) Mit der Funktion scale_x_discrete() verändert man die Reihenfolge diskreter Merkmale auf der x-Achse. So könnte mithilfe des limits Arguments bspw. zuerst der Boxplot für Neurotizismus angezeigt werden. Mit dem labels Argument gibt es die Möglichkeit, die Namen der Merkmale zu ändern. Dabei befindet sich auf der rechten Seite des Gleichheitszeichens der neue Name (hier Extra und Neuro) und auf der linken Seite die alte Bezeichnung. Äquivalent dazu existiert die Funktion scale_y_discrete(). p + scale_x_discrete( limits = c(&quot;Neurotizismus&quot;, &quot;Extraversion&quot;), labels = c(&quot;Extraversion&quot; = &quot;Extra&quot;, &quot;Neurotizismus&quot; = &quot;Neuro&quot;) ) Für kontinuierliche Skalen gibt es die Funktionen scale_x_continuous() und scale_y_continuous(). Durch das expand Argument wird der zusätzliche Raum zwischen y-Achse und x-Achse entfernt. Außerdem können hier der Anfang und das Ende der Achse (limits) sowie die Anzahl der Beschriftungen eingestellt werden (breaks). p + scale_y_continuous( expand = c(0, 0), limits = c(1, 5.2), breaks = 1:5 ) Eine nützliche Funktion für das breaks Argument stellt seq() dar, welche die Abstände der Unterteilungen mit by festlegt. seq(from = 0, to = 10, by = 2) [1] 0 2 4 6 8 10 Ein häufiges Problem sind zu lange Achsenbeschriftung, die sich überschneiden. Um das Problem zu lösen, gibt es drei Möglichkeiten. Man kann die Beschriftungen mit angle (engl. für Winkel) in Kombination mit hjust (Akronym für horizontal adjustment) in einer 45 Grad Winkel bringen. p + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Eine weitere Möglichkeit ist potentielle Überlappungen innerhalb der Funktion guide_axis() zu überprüfen (check.overlap = TRUE) und die Anzahl der verwendeten Zeilen festzulegen (n.dodge = 2). Dies wurde in mehreren Abbildungen dieses Buches verwendet (z.B. in Abbildung 8.18). p + scale_x_discrete(guide = guide_axis(n.dodge = 2, check.overlap = TRUE)) Als dritte Option können lange Beschriftungen mit str_wrap(), einer Funktion aus dem stringr Package, in mehrere Zeilen gebrochen werden (siehe Kapitel 6.9). Wenn bspw. mehr als zehn Buchstaben (width = 10) vorhanden sind, werden die Beschriftungen in mehrere Zeilen gebrochen. Diese Option ist vor allem für Beschriftungen mit mehr als einem Wort nützlich. p + scale_x_discrete(labels = \\(text) str_wrap(text, width = 10)) Abschließend fokussiert man mit coord_cartesian() einen Ausschnitt der Abbildung. Dies ist bspw. zur explorativen Betrachtung ohne Ausreißer sinnvoll. Im Gegensatz zu scale_y_continuous() wird in diesem Fall kein Wert gelöscht. Es wird lediglich der neu definierte Ausschnitt vergrößert. p + coord_cartesian(xlim = c(1, 4)) 8.10.3 Legende und Facetten Der Titel und Text der Legende wird am besten über die Farbfunktionen verändert. Beim vorherigen Anpassen der Füllfarbe in Abhängigkeit des Geschlechts, können dabei noch das name Argument für den Titel und das labels Argument für die Merkmalsbezeichnungen hinzugefügt werden. p + scale_fill_viridis_d( begin = 0.27, end = 0.72, option = &quot;C&quot;, name = &quot;Geschlecht (Auswahl):&quot;, labels = c(&quot;Weiblich&quot;, &quot;Männlich&quot;) ) Die Reihenfolge der gruppierenden Variable, die in der Legende angezeigt wird, verändern wir mithilfe von lims() (siehe Abbildung 8.19 (b) im Vergleich zur Ausgangslage (a)). p + lims(fill = c(&quot;m&quot;, &quot;f&quot;)) Zum Ändern der Reihenfolge der Füllfarben für die jeweiligen Merkmale müssen die Faktorstufen vor Erstellung der Abbildung verändert werden (siehe Kapitel 6.10). big5_long_rev &lt;- big5_long |&gt; mutate(Geschlecht = fct_relevel(Geschlecht, &quot;m&quot;)) Beim anschließenden Visualisieren verändert sich hingegen nichts im Vergleich zur vorherigen Reihenfolge. Das Ergebnis ist in Abbildung 8.19 (c) illustriert. ggplot(big5_long, aes(x = Faktor, y = Auspraegung, fill = Geschlecht)) + geom_boxplot(outlier.shape = NA, position = position_dodge(0.95), alpha = 0.8) Abbildung 8.19: Verschiedene Reihenfolgen der gruppierenden Variable. Die Textgröße von Legendentitel und -text werden in der bereits eingeführten theme() Funktion angepasst (siehe Kapitel 8.10.2). Ausgeblendet wird der Titel der Legende mit legend.title = element_blank(). Die Position wird am besten über x- und y- Koordinaten festgelegt. Welche Werte hierfür am passendsten sind, hängt direkt von der gewählten Dimension und somit Auflösung der Abbildung beim Speichern ab (siehe Kapitel 8.12). p + theme( legend.title = element_text(size = 14), legend.text = element_text(size = 13), legend.position = c(0.25, 0.9) ) Dem legend.position Argument können außerdem \"top\" und \"bottom\" übergeben werden. Eine horizontale Legende wird mit legend.direction = \"horizontal\" erstellt. Das Anpassen der Facetten erfolgt innerhalb von theme() mit den strip Argumenten. Dabei kann etwa die Textgröße der Überschrift verändert oder der Hintergrund ausgeblendet werden. p + theme( strip.text = element_text(size = 15), strip.background = element_blank() ) 8.11 Anordnen mehrerer Graphen Innerhalb dieses Buches gibt es diverse Graphen, die nebeneinander in einer Abbildung dargestellt wurden. Dafür muss das patchwork Package installiert und geladen sein. library(patchwork) Der erste Schritt ist das Abspeichern der jeweiligen Abbildungen. Exemplarisch nutzen wir an dieser Stelle das Histogramm, das Streudiagramm, den Boxplot und den Q-Q Plot aus den vorherigen Kapiteln und speichern diese jeweils als a, b, c und d. a &lt;- ggplot(big5_mod, aes(x = Extraversion)) + geom_histogram(color = &quot;black&quot;, fill = &quot;white&quot;, binwidth = 0.2) b &lt;- ggplot(big5_mod, aes(x = Extraversion, y = Alter)) + geom_point(position = &quot;jitter&quot;) c &lt;- ggplot(big5_long, aes(x = Faktor, y = Auspraegung)) + geom_boxplot() d &lt;- ggplot(big5_mod, aes(sample = Alter)) + geom_qq() + geom_qq_line() Anschließend addieren wir die vier Graphen in gewünschter Reihenfolge. Die Funktion plot_layout() spezifiziert unter anderem die Anzahl der Spalten (ncol) und plot_annotation() ergänzt mit dem tag_levels Argument Beschriftungen zu jeder Abbildung (siehe Abbildung 8.20). a + b + c + d + plot_layout(ncol = 2) + plot_annotation(tag_levels = &quot;A&quot;) Abbildung 8.20: Anordnung mehrerer Graphen Neben \"A\" kann der Funktion plot_annotation() außerdem \"i\", \"I\", \"a\" und \"1\" übergeben werden. Die tag_levels können durch tag_prefix und tag_suffix weiter an die eigenen Bedürfnisse angepasst werden. Außerdem können Abbildungen auch in unterschiedlicher Anzahl neben- und untereinander gesetzt werden. Dafür muss man lediglich die oben stehenden Abbildungen mit vertikalen Linien unterteilen und diese dann durch die Abbildung, die unten stehen soll, teilen (siehe Abbildung 8.21). (a | b | c) / d + plot_annotation( tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot; ) Abbildung 8.21: Alternative Anordnung und Benennung mehrerer Graphen. Falls alle addierten Graphen dieselbe Legende hätten, könnte diese mit guide_area() und dem guides Argument gesammelt angezeigt werden. Dabei wird mit guide_area() die Position der gemeinsamen Legende festgelegt. plots &lt;- a + b + c + guide_area() + plot_layout(ncol = 2, guides = &quot;collect&quot;) Eine weitere nützliche Funktion ist plot_spacer() zum Freilassen eines Areals in der kombinierten Abbildung. Wenn man kombinierte Abbildungen (hier als plots gespeichert) im Nachhinein verändern möchte, müssen diese einzeln angesprochen werden. Da diese als Liste gespeichert sind, erreichen wir dies mit einer doppelten eckigen Klammern (siehe Kapitel 11.4). Mit plots[[1]] extrahiert man so Abbildung a. plots[[1]] + theme_minimal() plots[[2]] + theme_minimal() plots[[3]] + theme_minimal() Falls dieselbe Änderung alle kombinierten Graphen betrifft, können wir diese mithilfe des &amp;-Operators (anstelle von +) umsetzen. a + b + c &amp; theme_minimal() 8.12 Speichern von Abbildungen Alle Abbildungen der vorherigen Unterkapitel können auf dieselbe Art und Weise gespeichert werden. Die Funktion ggsave() benötigt dafür nur die Argument filename für den vollständigen Dateinamen mit Endung und plot für den Namen der Abbildung, die gespeichert werden soll. Abbildungen können bspw. als .jpg oder .png gespeichert werden. Wir sollten mit den Argumenten width und height zusätzlich die genauen Dimensionen in der Einheit Zoll festlegen. Hiermit bestimmen wir das Verhältnis von Breite und Höhe sowie die Auflösung. An dieser Stelle möchten wir die Boxplots aus Abbildung 8.15 in einer Datei abspeichern, welche in der Variable p gespeichert sind (siehe Kapitel 8.10). Wir speichern die Abbildung als PNG Dateien namens plotA (Abbildung 8.22 links) und plotB (rechts). Dabei sollen beide Abbildungen dasselbe Verhältnis zwischen Breite und Höhe haben. Allerdings hat die Datei plotB eine doppelt so hohe Auflösung. ggsave(filename = &quot;plotA.png&quot;, plot = p, width = 4, height = 5) ggsave(filename = &quot;plotB.png&quot;, plot = p, width = 8, height = 10) Abbildung 8.22: Auflösungsunterschiede je nach Größe der gespeicherten Abbildung. Während die Schriftgrößen und Boxplots links angemessen groß erscheint, ist auf der rechten Seite alles etwas klein. Welches Verhältnis und welche Auflösung in deinem Anwendungsfall die Richtige ist, kannst du nur durch ausprobieren herausfinden. Die Größe der Abbildung wird durch die Breite (width) und Höhe (height) festgelegt. Unterschiedliche Größen beeinflussen ebenfalls die Auflösung der Abbildung, sodass man häufig im Nachhinein die Textgrößen der Achsenbeschriftungen anpassen und erneut abspeichern muss. Die gespeicherte Abbildung hat nicht dasselbe Format wie die angezeigte Ausgabe innerhalb von RStudio. 8.13 Exemplarische Erweiterungen Wir erinnern uns, dass die beiden Gs in ggplot für grammar of graphics stehen. Daraus resultiert nicht nur eine konsistente Anwendung, wie wir es in den bisherigen Kapiteln kennengelernt haben. Zusätzlich gibt es diverse darauf aufbauende Erweiterungen. Durch die gleiche Basis können wir auch die Graphen der Erweiterungen, wie zuvor gelernt, anpassen. Die meisten Erweiterungen halten sich an eine einheitliche Namensgebung. Vor dem Zweck des Packages steht also auch bei den Erweiterungspackages meistens ein gg (z.B. ggfortify oder ggridges). Allerdings halten sich nicht alle an eine konsistente Namensgebung (z.B. beim Erstellen von Kaplan-Meier-Kurven mit dem survival Package). 8.13.1 Kaplan-Meier-Kurve Für dieses Kapitel müssen die Packages survival, survminer und patchwork installiert und geladen werden. library(survival) library(survminer) library(patchwork) Wir möchten das Überleben der PatientInnen mit einer Krebserkrankung zuerst insgesamt und anschließend unterteilt nach Behandlungsart graphisch darstellen. Im chemo Datensatz aus dem remp Package interessieren uns dabei vor allem die Spalten Beob_zeit, Status und Behandlung. Die Variable Beob_zeit gibt die Untersuchungszeit in Tagen an und Status beinhaltet die Information über den Tod. Die Variable Behandlung umfasst drei verschiedene Therapiearten in Form der Radiochemotherapie, Chemotherapie und Radiotherapie. # A tibble: 450 × 3 Beob_zeit Status Behandlung &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 0.833 1 Radio 2 4.80 1 Radiochemo 3 1.46 1 Radio 4 0.922 1 Radio # ℹ 446 more rows Gesamte Überlebenszeit. Bevor wir eine Kaplan-Meier-Kurve zur Visualisierung der Überlebenszeiten erstellen können, müssen wir zunäcgst mithilfe der Funktion survfit() aus dem survival Package die Überlebenswahrscheinlichkeiten schätzen (siehe Kapitel 9.5.6). Mit der Funktion ggsurvplot() aus dem survminer Package können wir anschließend die Überlebenszeiten über alle Gruppen hinweg abbilden. res1 &lt;- survfit(Surv(time = Beob_zeit, event = Status) ~ 1, data = chemo) ggsurvplot(res1) Abbildung 8.23: Kaplan-Meier-Kurve ohne Strata. Überlebenszeit nach gruppierender Variable. Bei Gruppierung der Überlebenszeiten (auch Strata genannt) ergänzen wir das Modell um die unabhängige VariableBehandlung. res2 &lt;- survfit(Surv(time = Beob_zeit, event = Status) ~ Behandlung, data = chemo) Zum Erstellen der eigentlichen Kaplan-Meier-Kurve verwenden wir erneut ggsurvplot(). Es gibt diverse Anpassungsmöglichkeiten. Eine Tabelle mit den Häufigkeiten der PatientInnen zu den Zeitpunkten erhalten wir mit dem Argument risk.table. Die anderen Argumente sollten nach durcharbeiten von Kapitel 8.10 selbsterklärend sein. p1 &lt;- ggsurvplot( fit = res2, xlab = &quot;Jahre&quot;, ylab = &quot;Überlebenswahrscheinlichkeit&quot;, break.x.by = 1, risk.table = TRUE, risk.table.y.text = FALSE, tables.theme = theme_cleantable(), legend = c(0.7, 0.7), legend.title = &quot;Behandlung:&quot;, legend.labs = c(&quot;Radiochemotherapie&quot;, &quot;Chemotherapie&quot;, &quot;Radiotherapie&quot;), palette = &quot;Dark2&quot; ) Durch das zusätzliche Erstellen der Risikotabelle enthält p1 eine eine Liste mit zwei Teilen, auf die wir mithilfe des Dollar-Operators zugreifen können (siehe Kapitel 4.5). In p1$plot ist die Kaplan-Meier-Kurve und in p1$table die Tabelle gespeichert. Dadurch können wir die entstandenen Abbildungen, wie in Kapitel 8.10 kennengelernt, nach belieben anpassen. Exemplarisch sei hier der Titel der Risikotabelle entfernt und das Ergebnis in derselben Variable abgespeichert. p1$table &lt;- p1$table + ggtitle(&quot;&quot;) Zum Abspeichern verwenden wir das patchwork Package, welches die Funktion plot_layout() bereitstellt. Diese bietet durch das heights Argument die Möglichkeit des Festlegens der Verhältnisse zwischen Plot und Tabelle (siehe Kapitel 8.11). Außerdem garantiert das Argument ncol = 1, dass beide Teile der Abbildung untereinander abgebildet werden. und das Verhältnis der Abbildungen mit dem heights Argument. kaplan_meier &lt;- p1$plot + p1$table + plot_layout(ncol = 1, heights = c(8, 1.5)) kaplan_meier Abbildung 8.24: Kaplan-Meier-Kurve mit Tabelle unterteilt nach Behandlungsart. Gespeichert wird das Ergebnis wie gewohnt mit der Funktion ggsave() (siehe Kapitel 8.12). ggsave(&quot;Kaplan_meier.png&quot;, kaplan_meier, height = 7, width = 9) Möchte man nun im Nachhinein noch Veränderungen des Aussehens vornehmen, können die beiden Bestandteile mit kaplan_meier[[1]] und kaplan_meier[[2]] angesprochen werden. 8.13.2 Residuen überprüfen Für das schnelle graphische Überprüfen der notwendigen Voraussetzungen in Bezug auf die Residuen, müssen wir erst das ggfortify Package installieren und laden. library(ggfortify) library(patchwork) Zunächst greifen wir an dieser Stelle etwas vor und erstellen ein lineares Regressionsmodell, welches die Variation in der mittleren Extraversionsausprägung durch Geschlecht und Alter erklären soll (siehe Kapitel 9.5.1). model &lt;- lm(Leukos_t6 ~ Alter + Geschlecht, data = chemo) Dieses Modell kann im Anschluss der Funktion autoplot() übergeben werden. Dies funktioniert auf dieselbe Art und Weise wie für die meisten Regressionsmodelle. Als weiteres Argument kann mit which ausgewählt werden, welche Abbildungen ausgegeben werden (hier alle 6). Das Argument label.repel sorgt dafür, dass der Text, der die Ausreißer beschriftet, die Linien nicht überschneidet. Als letztes wird mit smooth.colour noch die Farbe der Regressionsgeraden auf schwarz gesetzt. res &lt;- autoplot( model, which = 1:6, label.repel = TRUE, smooth.colour = &quot;black&quot; ) Zum Speichern müssen, wie in Kapitel 8.13.1, die einzelnen Abbildungen ausgewählt und mithilfe des patchwork Packages zusammengefügt werden. Da die Plots mit ggplot erstellt wurden, können wir das Aussehen wie gewohnt anpassen (siehe Kapitel 8.10). Auf die einzelnen Plots wird mit doppelten eckigen Klammern zugegriffen, da diese auch hier als Listen gespeichert wurden (siehe Kapitel 11.4). Anschließend fügen wir die Abbildungen zusammen und verändern das Thema sowie die Schriftgröße für alle Teile der Abbildungen gleichzeitig (siehe Kapitel 8.11). res[[1]] + res[[2]] + res[[3]] + res[[4]] + res[[5]] + res[[6]] + plot_layout(ncol = 2) &amp; theme_classic(base_size = 14) Abbildung 8.25: Graphische Überprüfung der Residuen. 8.13.3 Ridgeline Plot Für dieses Kapitel muss das ggridges Package installiert und geladen sein. library(ggridges) Sogenannte Ridgeline Plots erlauben unter anderem den Vergleich von mehreren Dichtefunktionen innerhalb einer Abbildung. In Kapitel 8.2 haben wir die Wahrscheinlichkeitsdichte und somit die Verteilung der Werte für eine Variable angezeigt. Um mehrere Dichten untereinander innerhalb einer Abbildung zu vergleichen, müssen wir den Datensatz zuerst in das lange Datenformat bringen (siehe Kapitel 6.6). big5_mod2_long &lt;- big5_mod2 |&gt; pivot_longer( cols = Extraversion:Gewissenhaftigkeit, values_to = &quot;Auspraegung&quot;, names_to = &quot;Faktor&quot; ) |&gt; relocate(Faktor, Auspraegung) big5_mod2_long # A tibble: 800 × 14 Faktor Auspraegung Alter Geschlecht O1 O2 O3 O4 O5 O6 O7 O8 O9 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extravers… 3 36 m 5 1 5 1 4 1 5 5 4 2 Neurotizi… 1.9 36 m 5 1 5 1 4 1 5 5 4 3 Vertraegl… 3.4 36 m 5 1 5 1 4 1 5 5 4 4 Gewissenh… 3.3 36 m 5 1 5 1 4 1 5 5 4 # ℹ 796 more rows # ℹ 1 more variable: O10 &lt;dbl&gt; Durch die Funktion geom_density_ridges() werden die Verteilungen von Extraversion, Neurotizismus, Verträglichkeit und Gewissenhaftigkeit gegeneinander aufgetragen. Zusätzlich übergeben wir das Argument alpha, welches die Deckkraft auf 70% reduziert. ggplot(big5_mod2_long, aes(x = Auspraegung, y = Faktor, height = after_stat(density))) + geom_density_ridges(stat = &quot;density&quot;, alpha = 0.7) Etwas ungewohnt ist an dieser Stelle, dass wir hier auf der y-Achse die zu vergleichende diskrete Variable abbilden und auf der x-Achse die mittlere Ausprägung des jeweiligen Persönlichkeitsfaktors. Abschließend passen wir noch einige Kleinigkeiten für ein ansprechenderes Aussehen an. ggplot(big5_mod2_long, aes(x = Auspraegung, y = Faktor, height = after_stat(density))) + geom_density_ridges(stat = &quot;density&quot;, alpha = 0.7, fill = &quot;steelblue&quot;) + labs(x = &quot;Mittlere Ausprägung&quot;, y = &quot;&quot;) + scale_x_continuous(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme_classic(base_size = 14, base_family = &quot;sans&quot;) + theme( axis.text.y = element_text(hjust = 0, color = &quot;black&quot;, size = 12), axis.title.x = element_text(hjust = 1, vjust = 0.2, size = 13) ) Abbildung 8.26: Ridgeline plots mit angepasstem Thema. 8.14 Anwendungsbeispiel Wir werden uns abschließend in diesem Kapitel anhand eines Eye-Tracking Datensatzes namens eye_tracking anschauen, wie wir mithilfe der bisher gelernten Funktionen, eine komplexere und optische ansprechende Abbildung kreieren können. In diesem Datensatz wurden zwei Studien miteinander verglichen, die mithilfe von Eye-Tracking die Anzahl der angeschauten Gesichter in Abhängigkeit der gezeigten Informationsdichte gezählt haben. eye_tracking # A tibble: 100 × 3 Gesichter Dichte Studie &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 142 3.35 Study 1 2 91 3.44 Study 2 3 140 2.81 Study 2 4 56 2.20 Study 2 # ℹ 96 more rows Die Ergebnisse der beiden Studien sollen anhand eines gruppierten Streudiagramms mit Regressionsgeraden verglichen werden (siehe Kapitel 8.3). Zusätzlich werden gruppierte Wahrscheinlichkeitsdichten zur Darstellung der Verteilungen abgebildet (siehe Kapitel 8.2). Das gewünschte Resultat ist in Abbildung 8.27 illustriert. Abbildung 8.27: Anzahl fixierter Gesichter in Abhängigkeit der Informationsdichte. Diese Abbildung besteht im Prinzip aus vier Teilen, welche nacheinander erstellt und dann miteinander kombiniert werden müssen. Links unten ist unsere Hauptabbildung, mit bereits bekannten Komponenten. Um den notwendigen Code besser zu verstehen, gehen wir erst durch, was wir zur Visualisierung benötigen. Erstelle den ggplot mit der Informationsdichte auf der x-Achse und den fixierten Gesichtern auf der y-Achse, wobei die Farbe gruppiert nach der jeweiligen Studie gewählt werden soll. Füge ein nach Studienart gruppiertes Streudiagramm mit angepasster Größer der Punkte und reduzierter Deckkraft hinzu. Zeichne einen schwarzen Ring um jeden Punkt des Streudiagramms. Berechne die linearen Regressionsgeraden, welche sich über die komplette Abbildung erstrecken sollen und blende dabei die Konfidenzintervalle aus. Anschließend soll die Abbildung noch verschönert werden. Ändere die x-Achsen und y-Achsen Beschriftung. Verwende anstelle der Standardfarben eine Farbpalette (z.B. viridis), welche auch von Menschen mit Farbenblindheit unterschieden werden kann. Im Zuge dessen sollen der Titel der Legende sowie die Namen der Merkmale angepasst werden. Passe die Begrenzungen der x-Achse und y-Achse an. Verändere das generelle Thema sowie die Schriftgrößen. Verschiebe die Legende und ordne sie horizontal an. Die Hauptabbildung wird schließlich als plot1 gespeichert. plot1 &lt;- ggplot(eye_tracking, aes(x = Dichte, y = Gesichter, color = Studie)) + geom_point(size = 3, alpha = 0.8) + geom_point(shape = 1, color = &quot;black&quot;, size = 3) + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, se = FALSE) + labs( x = &quot;Informationsdichte&quot;, y = &quot;Anzahl fixierter Gesichter&quot;, ) + scale_color_viridis_d( begin = 0.2, end = 0.7, option = &quot;D&quot;, name = &quot;Studie:&quot;, labels = c(&quot;1&quot;, &quot;2&quot;) ) + scale_y_continuous( limits = c(0, 225), expand = c(0, 0), breaks = seq(from = 0, to = 225, by = 25) ) + scale_x_continuous( limits = c(0.5, 4), expand = c(0, 0), breaks = seq(from = 0.5, to = 4, by = 0.5) ) + theme_classic(base_size = 14) + theme(legend.position = c(0.25, 0.95), legend.direction = &quot;horizontal&quot;) Bei den beiden Wahrscheinlichkeitsdichten wird erst die Informationsdichte auf der x-Achse in der einen und die Anzahl fixierter Gesichter auf der y-Achse in der anderen Abbildung übergeben. Beide Abbildungen sollen eine etwas niedriger Deckkraft als die Hauptabbildung haben, da sich beide Verteilungen überschneiden werden. Abschließend müssen wir das Thema (auch die Achsen) mithilfe von theme_void() vollständig ausblenden. Auch die Legende muss manuell entfernt werden. Gespeichert werden die beiden Ergebnisse als dens1 und dens2. dens1 &lt;- ggplot(eye_tracking, aes(x = Dichte, fill = Studie)) + geom_density(alpha = 0.6) + theme_void() + theme(legend.position = &quot;none&quot;) dens2 &lt;- ggplot(eye_tracking, aes(y = Gesichter, fill = Studie)) + geom_density(alpha = 0.6) + theme_void() + theme(legend.position = &quot;none&quot;) Als letzten Schritt fügen wir die einzelnen Teile zusammen (siehe Kapitel 8.11). Die Funktion plot_spacer() füllt dabei den leeren Raum oben rechts in der Abbildung aus. Innerhalb von plot_layout() verwenden wir die Argumente widths und heights, welche die Verhältnisse zwischen der Hauptabbildung und den Dichten festlegen. Schließlich soll die Wahrscheinlichkeitsdichte oben links und unten rechts kleiner dargestellt werden als unsere Hauptabbildung unten links. Abschließend muss noch die Füllfarbe für dens1 und dens2 angepasst werden. Beachte an dieser Stelle das Hinzufügen der Funktion mithilfe von &amp; anstelle von +. dens1 + plot_spacer() + plot1 + dens2 + plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4)) &amp; scale_fill_viridis_d(begin = 0.2, end = 0.7, option = &quot;D&quot;) "],["inductive.html", "Kapitel 9 Inferenzstatistik 9.1 Einführung 9.2 Ein- und Zweistichprobenszenarien 9.3 Unterschiede mehrerer Gruppen 9.4 Korrelationsanalysen 9.5 Regressionsmodelle 9.6 Kontingenztafeln 9.7 Voraussetzungen überprüfen 9.8 Ergebnisse formatieren", " Kapitel 9 Inferenzstatistik In diesem Kapitel wird die Umsetzung der gängigsten statistischen Modelle in R gezeigt. Dabei liegt der Fokus klar auf der Programmierung und nicht auf dem für die Interpretation notwendigen Statistikwissen. Letzteres sollte aus einem der angegeben Statistikbüchern entnommen werden. Durch diese strikte Trennung können wir in den Unterkapiteln auch fortgeschrittenere Modelle betrachten, die in anderen Programmierbüchern leider häufig ausgelassen werden. Nach diesem Kapitel kannst du (fast) alle deiner Forschungsfragen mit R beantworten. 9.1 Einführung Welches statistische Modell zur Beantwortung der eigenen Forschungsfragen in Form von Hypothesen geeignet ist, hängt im Wesentlichen von der Art der Hypothese, der zugrunde liegenden Verteilung und dem Skalenniveau der Merkmale ab. Die Ausgabe der Ergebnisse sieht dabei immer ähnlich aus und enthält die wesentlichen Informationen, die in einer wissenschaftlichen Arbeit berichtet werden müssen. Während wir hauptsächlich direkt in R integrierte Funktionen verwenden werden, sind in Einzelfällen zusätzliche Packages notwendig, auf die an geeigneter Stelle hingewiesen wird. In den folgenden Kapiteln werden wir einen Datensatz namens chemo verwenden, der im remp Package enthalten ist. 450 PatientInnen mit einer bösartigen Krebserkrankung (genauer gesagt, einer malignen Neoplasie) haben entweder nur Chemotherapie, nur eine Bestrahlung oder eine Kombination aus beidem erhalten. Neben der Beobachtungszeit (Beob_zeit) unter jeder dieser Behandlungen, wurden ebenfalls weitere Merkmale untersucht, die wir in den folgendenen Kapiteln als abhängige Variablen verwenden werden. # A tibble: 450 × 8 Stationaer Komorb Schmerzen Lebensqualitaet Infektionen Leukos_t0 Leukos_t6 Beob_zeit &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 Keine 6 2 4 5.74 5.68 0.833 2 0 Lunge 2 2 3 6.71 6.64 4.80 3 0 Keine 7 2 3 6.03 5.97 1.46 4 1 Keine 2 4 1 5.6 5.55 0.922 # ℹ 446 more rows Der stationäre Aufenthalt aufgrund einer schweren Infektion Stationaer ist nominal mit zwei Ausprägungsgraden aus der Binomialverteilung. Die Information über weitere Erkrankungen Komorb (kurz für Komorbiditäten) ist nominal skaliert aus der Multinomialverteilung. Die Variablen Schmerzen und Lebensqualitaet wurden durch einen Fragebogen auf jeweils 10 Stufen erfasst und haben daher ein ordinales Skalenniveau. Die Häufigkeit von Infektionen innerhalb von zwei Jahren hat eine negative Binomial Verteilung. Schließlich wurde die Anzahl der Leukozyten zu Beginn der Therapie (Leukos_t0) und 6 Monate später (Leukos_t6) erhoben, die jeweils normalverteilt und intervallskaliert sind. Leukozyten sind weiße Blutzellen, die einen integralen Bestandteil des Immunsystems darstellen. Die beschriebenen interessierenden Größen können möglicherweise Unterschiede bzw. Zusammenhänge im Bezug auf das biologische Geschlecht, die Behandlungsart und dem Alter haben. Diese stellen unsere unabhängigen Variablen in sämtlichen Unterkapiteln dar. # A tibble: 450 × 3 Alter Geschlecht Behandlung &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; 1 49 f Radio 2 46 f Radiochemo 3 41 m Radio 4 43 f Radio # ℹ 446 more rows Es handelt sich beim chemo Datensatz nicht um reale Daten. Stattdessen wurden die einzelnen Variablen simuliert, was den Vorteil bietet, dass die wahren Verteilungen und Effekte bekannt sind. Genaue Informationen über die zur Simulation verwendeten Parameter sei auf die Dokumentation verwiesen. Die Statistikfunktionen in R haben zwei verschiedene Schreibweisen. Entweder müssen die interessierenden Variablen in Form von Vektoren (oder Wertereihen) aus dem Datensatz mithilfe des Dollar-Operators extrahiert werden (siehe Kapitel 4.5). Oder es wird auf eine Formelschreibweise zurückgegriffen, bei der auf der linken Seite einer Tilde ~ die abhängige Variable und auf der rechten Seite eine oder mehrere unabhängige Variablen, getrennt von einem Pluszeichen, geschrieben werden. Interaktionsterme können mithilfe eines Doppelpunktes hinzugefügt werden. In diesem Buch konzentrierten wir uns auf univariate Fragestellungen mit einer abhängigen Variable und einer oder mehreren unabhängigen Variablen. Die Ergebnislisten sind zwar übersichtlich, allerdings nicht geeignet zum Weiterverarbeiten in Tabellenform zum Exportieren nach Word oder in ein PDF. Dafür gibt es ein dediziertes Package, welches abschließend in Kapitel 9.8 vorgestellt wird. 9.2 Ein- und Zweistichprobenszenarien 9.2.1 t Test und Welch Test Zum Berechnen der Effektstärken muss das effectsize Package installiert und geladen sein. library(effectsize) Im Einstichproben t Test untersuchen wir ein normalverteiltes, intervallskaliertes Merkmal in Hinblick auf einen unter der Nullhypothese festgelegten Mittelwert (\\(\\mu\\)). In Bezug auf unseren chemo Datensatz fragen wir uns, ob die mittlere Leukoyztenanzahl nach sechs Monaten Behandlung statistisch signifikant von dem Normwert von 6 E9/L abweicht. Wir würden unter der Nullhypothese demnach einen Mittelwert der Spalte Leukos_t6 in Höhe von 6 E9/L erwarten. In R setzen wir dies mit der Funktion t.test() um, welcher wir beim Untersuchen einer Stichprobe zunächst die interessierende Variable Leukos_t6 als Vektor (bzw. Wertereihe) mithilfe des Dollar-Operators übergeben (siehe Kapitel 4.5). Anschließend müssen wir noch den Mittelwert \\(\\mu\\) festlegen, welchen wir unter der Nullhypothese, dass es keinen Unterschied zwischen beiden Mittelwerten gibt, erwarten würden. t.test(chemo$Leukos_t6, mu = 6) One Sample t-test data: chemo$Leukos_t6 t = -1.3637, df = 449, p-value = 0.1734 alternative hypothesis: true mean is not equal to 6 95 percent confidence interval: 5.947976 6.009402 sample estimates: mean of x 5.978689 In der Ausgabe sehen wir den t-Wert (-1.3637), die Freiheitsgrade (449), den p-Wert (0.1734), die Alternativhypothesen, das 95% Konfidenzintervall (5.95; 6.01) und den Mittelwert der Leukozyten (5.98). Bei einem in der Wissenschaft üblichen \\(\\alpha\\)-Niveau (Fehler 1. Art) von 5% ist der auf Basis der geschätzten Teststatistik berechnete p-Wert nicht statistisch signifikant, da 0.1734 größer als 0.05 ist. Die Nullhypothese wird demnach angenommen. Sprich, die Leukozytenanzahl nach sechs Monaten Behandlung weicht nicht signifikant vom Normwert ab. Falls die Hypothese nicht zweiseitig (Mittelwerte sind gleich oder ungleich), sondern einseitig getestet werden soll (Mittelwert ist größer oder kleiner), könnte dies mit dem zusätzlichen Argument alternative umgestellt werden. Dabei nutzt man \"less\" für kleiner und \"greater\" für größer. Die Standardeinstellung ist ein zweiseitiger Test, weswegen wir das Argument in unserem Beispiel nicht extra benennen müssen. Das \\(\\alpha\\)-Niveau kann über das Argument conf.level modifiziert werden, welches mit 0.95 (also einem \\(\\alpha\\) von 5%) voreingestellt ist. Für den t Test nehmen wir neben einer zugrunde liegenden Normalverteilung beider Merkmale außerdem gleiche Varianzen an (siehe Kapitel 9.7). Dies müssen wir explizit mit dem Argument var.equal festlegen, da die Standardeinstellung in R von ungleichen Varianzen ausgeht. Bei ungleichen Varianzen (var.equal = FALSE) wird ein Welch Test berechnet, welcher die Freiheitsgrade entsprechend korrigiert. Die Überprüfungen der Voraussetzungen der Normalverteilung und Varianzhomogenität wird in Kapitel 9.7 erläutert. Wir sind an dieser Stelle an der Hypothese interessiert, ob sich die mittlere Leukozytenzahl nach sechs Monaten zwischen den biologischen Geschlechtern signifikant voneinander unterscheidet. Um zwei Gruppen in R zu vergleichen, verwenden wir die Formelschreibweise. Auf die linke Seite der Tilde schreiben wir die intervallskalierte Variable Leukos_t6 und auf die rechten Seite die kategorisierende Variable Geschlecht. t.test(Leukos_t6 ~ Geschlecht, data = chemo, var.equal = TRUE) Two Sample t-test data: Leukos_t6 by Geschlecht t = 1.456, df = 448, p-value = 0.1461 alternative hypothesis: true difference in means between group f and group m is not equal to 0 95 percent confidence interval: -0.01591524 0.10690584 sample estimates: mean in group f mean in group m 6.000426 5.954930 Da der p-Wert auch in diesem Beispiel mit 0.1461 größer als 0.05 ist, nehmen wir auch hier bei einem \\(\\alpha\\)-Niveau von 5% die Nullhypothese an. In unserer Stichprobe gibt es folglich keine signifikanten Unterschiede in der Leukoyztenzahl zwischen den Gechlechtern. Zusätzlich zum t Test sollte eine Effektstärke berechnet werden, die im Kontext von Mittelwertsvergleichen häufig Cohens d oder Hedges g darstellt. Mit der Funktion cohens_d() respektive hedges_g() aus dem effectsize Package sind diese in R implementiert. Zur Berechnung übergeben wir die Variablen fast identisch wie zuvor innerhalb der Funktion t.test(). Es ändert sich lediglich das Argument var.equal zu pooled_sd. cohens_d(Leukos_t6 ~ Geschlecht, data = chemo, pooled_sd = TRUE) Cohen&#39;s d | 95% CI ------------------------- 0.14 | [-0.05, 0.32] - Estimated using pooled SD. Beachte, dass bei einem t Test nur zwei Gruppen miteinander verglichen werden können. Für mehr als zwei Gruppen kann auf eine Varianzanalyse zurückgegriffen werden (siehe Kapitel 9.3.1). Für einen Test auf abhängige (verbundene, gepaarte) Stichproben, müssen wir zusätzlich das paired Argument hinzufügen. Unterscheiden sich die Mittelwerte der Leukozytenanzahlen von Behandlungsbeginn im Vergleich zur Messung nach sechs Monaten? Zur Beantwortung dieser Hypothese übergeben wir der Funktion t.test() die beiden Variablen mit dem Dollar-Operator als Vektoren. t.test(chemo$Leukos_t0, chemo$Leukos_t6, var.equal = TRUE, paired = TRUE) Paired t-test data: chemo$Leukos_t0 and chemo$Leukos_t6 t = 251.83, df = 449, p-value &lt; 2.2e-16 alternative hypothesis: true mean difference is not equal to 0 95 percent confidence interval: 0.05926718 0.06019949 sample estimates: mean difference 0.05973333 Die Leukoyztenanzahl ist statistisch signifikant unterschiedlichen zwischen den beiden Messzeitpunkten, da der p-Wert kleiner als 2.2e-16 ist, was die wissenschaftliche Notation für eine Zahl mit 15 Nullen hinter dem Komma darstellt. Die Alternativhypothese wird in diesem Beispiel demnach angenommen. Auch hier können wir die Effektstärke in Form von Cohens d berechnen. An dieser Stelle ist der beobachtete Effekt deutlich größer als beim vorherigen t Test, was wir in Hinblick auf den signifikanten p-Wert auch erwarten würden. cohens_d(chemo$Leukos_t0, chemo$Leukos_t6, paired = TRUE) Cohen&#39;s d | 95% CI -------------------------- 11.87 | [11.08, 12.65] Bei einem Vergleich von Mittelwerten intervallskalierter Merkmale mit maximal zwei Gruppen können wir t-Tests verwenden. Falls die Variablen keine ähnliche Varianz haben, können wir stattdessen auf den Welch-Test zurückgreifen. Wenn allerdings zusätzlich keine Normalverteilung vorliegt, greifen wir auf nicht-parametrische Tests wie den Wilcoxon Test zurück, welcher im folgenden Kapitel eingeführt wird. 9.2.2 Wilcoxon Test Der Wilcoxon Test für unabhängige Stichproben ist ein nicht-parametrischer Hypothesentest für intervallskalierte Variablen, welcher bei Verletzungen der Annahmen von Normalverteilung und Varianzhomogenität Anwendung findet. Dieser wird auch Mann-Whitney U-Test genannt. In R ist dieser in Form der Funktion wilcox.test() enthalten, welcher äquivalent zum t-Test des vorherigen Kapitels angewandt wird. Beim Vergleich einer intervallskalierten Variable zwischen zwei Gruppen verwenden wir daher auch in diesem Fall die Formelschreibweise mit kategorisierenden Variable auf der rechten Seite der Tilde. wilcox.test(Leukos_t6 ~ Geschlecht, data = chemo) Wilcoxon rank sum test with continuity correction data: Leukos_t6 by Geschlecht W = 26825, p-value = 0.2571 alternative hypothesis: true location shift is not equal to 0 Als Ergebnis erhalten wir die geschätzte Teststatistik W und den zugehörigen p-Wert. Der p-Wert ist mit 0.2571 größer als 0.05 und somit nicht statistisch signifikant. Es wird also die Alternativhypothese abgelehnt und die Nullhypothese angenommen. Die Leukozytenverteilungen der beiden biologischen Geschlechter unterscheiden sich demnach in unseren Daten nicht nennenswert voneinander. Im Kontext von abhängigen (bzw. gepaarten) Stichproben findet stattdessen der Wilcoxon-Vorzeichen-Rang Test Anwendung. Dies ist häufig in Szenarien mit wiederholten Messungen für dieselbe Person der Fall. In unserem Datensatz haben wir die Leukozytenzahl zu zwei verschiedenen Zeitpunkten gemessen. Durch das Setzen des Arguments paired auf TRUE erhalten wir den entsprechenden Schätzer. Wie im vorherigen Kapitel übergeben wir die beiden Spalten als Vektoren mithilfe des Dollar-Operators (siehe Kapitel 4.5). wilcox.test(chemo$Leukos_t0, chemo$Leukos_t6, paired = TRUE) Wilcoxon signed rank test with continuity correction data: chemo$Leukos_t0 and chemo$Leukos_t6 V = 101475, p-value &lt; 2.2e-16 alternative hypothesis: true location shift is not equal to 0 Ähnlichen wie beim t-Test des vorherigen Kapitels finden wir auch hier einen statistisch signifikanten p-Wert. Der Wert ist in diesem Fall kleiner als \\(2.2*10^{-16}\\), was einer Zahl kleiner 0.00000000000000022 entspricht. Andere Statistikprogramm wie SPSS geben bereits bei p-Werten kleiner 0.001 keinen exakten Wert mehr aus. Die beiden Varianten des Wilcoxon Tests finden häufig zur Beantwortung derselben Fragestellungen wie der t-Test Anwendung. Die Wilcoxon Tests werden bevorzugt, wenn kleine Stichproben oder Verletzungen der Voraussetzungen der Normalverteilung und Varianzhomogenität vorliegen. Falls nur die Annahme der Varianzhomogenität verletzt ist, kann alternativ auch der Welch-Test verwendet werden (siehe Kapitel 9.2.1). 9.3 Unterschiede mehrerer Gruppen 9.3.1 Varianz- und Kovarianzanalyse Für die Funktionen dieses Kapitels müssen die Packages car und effectsize installiert und geladen werden. library(car) library(effectsize) Im Vergleich zum t Test und Wilcoxon Test der vorherigen Kapitel bietet eine Varianzanalyse drei Vorteile: Es können mehr als zwei Gruppen betrachtet werden. Mehr als eine unabhängige Variable kann in das Modell eingeschlossen werden. Interaktionsterme zwischen den unabhängigen Variablen können geschätzt werden. Ansonsten setzen wir auch hier eine Normalverteilung der intervallskalierten Merkmale und Varianzhomogenität voraus (siehe Kapitel 9.7). Im Kontext der Varianzanalyse testen wir die Nullhypothese, ob alle Mittelwerte gleich sind, gegen die Alternativhypothese mindestens eines statistisch signifikant abweichenden Mittelwertes. Wir möchten im Folgenden untersuchen, inwiefern sich die Leukozytenanzahl nach sechs Monaten Behandlung zwischen den drei Behandlungsarten und den biologischen Geschlechtern unterscheidet. Die zu überprüfende Nullhypothese der Kovariate Behandlung ist demnach die Gleichheit der Mittelwerte der PatientInnen mit Radiochemotherapie, Chemotherapie oder Radiotherapie. Falls auch nur für eine dieser Gruppen ein nennenswert unterschiedlicher Mittelwert in unserer Stichprobe geschätzt wird, könnten wir bereits einen signifikanten p-Wert dieses Haupteffektes in der Varianzanalyse erhalten. Die Berechnung einer Varianzanalyse besteht in R aus zwei Schritten. Zunächst muss das Modell mit der Funktion aov() aufgestellt werden (Akronym für analysis of variances, engl. für Varianzanalyse). Auf der linken Seite der Tilde steht dabei die abhängige Variable und auf der rechten Seite in diesem Fall unsere beiden unabhängigen Variablen, getrennt von einem Pluszeichen. Interaktionseffekte könnte man in das Modell mit einem zusätzlichen Term mit Doppelpunkt hinzufügen. Eine Interaktion (auch Moderation genannt) zwischen den Variablen Behandlung und Geschlecht würde man demnach mit Behandlung:Geschlecht erhalten. Im Rahmen von Kovarianzanalysen wird dieses Konzept später im Kapitel praktisch angewendet. Das Ausgeben der Ergebnisse erfolgt mit der Funktion Anova() aus dem car Package. Andere Statistikprogramme wie SPSS geben standardmäßig die Typ 3 Quadratsummen aus, weswegen auch wir uns mit dem Argument type dafür entscheiden. Alternativ könnten über dieses Argument ebenfalls die Typ 2 Quadratsummen berechnet werden. Typ 1 Quadratsummen erhält man, indem man das Ergebnis von aov() der Funktion summary() übergibt (z.B. summary(result)). result &lt;- aov(Leukos_t6 ~ Behandlung + Geschlecht, data = chemo) res_aov &lt;- Anova(result, type = 3) res_aov Anova Table (Type III tests) Response: Leukos_t6 Sum Sq Df F value Pr(&gt;F) (Intercept) 4225.0 1 39004.9324 &lt; 2e-16 *** Behandlung 0.8 2 3.7144 0.02513 * Geschlecht 0.2 1 2.1527 0.14302 Residuals 48.3 446 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In der Ergebnisliste erhalten wir Informationen über die Quadratsummen (Sum Sq), Freheitsgrade (df) und die Teststatistik in Form des F-Wertes mit entsprechenden p-Wert. Daraus können wir schließen, dass unsere Daten bei einem üblichen \\(\\alpha\\)-Niveau von 5% keinen nennenswerten Unterschied zwischen den biologischen Geschlechtern in Hinblick auf die Leukozytenzahl nach sechs Monaten Therapie zeigen (da 0.14302 größer als 0.05). Bei der Behandlungsart sieht es anders aus. Weil wir hier einen p-Wert von 0.02513 geschätzt haben und dieser kleiner als 0.05 ist, weicht mindestens eine mittleren Leukozytenzahlen der Gruppen statistisch signifikant von den anderen ab. Ob der Unterschied nun zwischen Radiochemotherapie und Chemotherapie, Radiochemotherapie und Radiotherapie oder Chemotherapie und Radiotherapie besteht, können wir aus diesem Ergebnis noch nicht ableiten. Für die genaue Untersuchung des Haupteffekts Behandlungsgruppe verwenden wir an dieser Stelle einen sogenannten Post-hoc Test nach Tukey, welcher innerhalb von R durch die Funktion TukeyHSD() implementiert ist (Akronym für Honest Significant Differences). TukeyHSD(result, &quot;Behandlung&quot;) Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = Leukos_t6 ~ Behandlung + Geschlecht, data = chemo) $Behandlung diff lwr upr p adj Chemo-Radiochemo -0.03151523 -0.1207342 0.05770374 0.6841799 Radio-Radiochemo -0.10123872 -0.1906078 -0.01186968 0.0217816 Radio-Chemo -0.06972349 -0.1592404 0.01979339 0.1604867 Ausgeben wird eine Tabelle mit der Mittelwertsdifferenz aller Gruppenunterschiede (diff) mit 95% Konfidenzintervall (lwr, upr) und korrigierten p-Wert. Der Haupteffekt ist in der Varianzanalyse folglich aufgrund des Unterschiedes in der Leukoyztenzahl zwischen Radiochemotherapie und Radiotherapie entstanden. Die Leukozyten sind in der Gruppe mit reiner Bestrahlung (Radiotherapie) geringer als bei der Kombinationstherapie mit einem statistisch signifikanten p-Wert (da 0.022 kleiner 0.05). Eine Alternative dazu stellen paarweise t-Tests dar, welche jeweils die Mittelwerte der Behandlungsgruppen vergleichen und für jede Kombination einen p-Wert zurückgibt. Da wir an dieser Stelle mehrfach Hypothesentests anwenden und es somit zur Inflation des Fehlers 1. Art kommt, wird standardmäßig auch hier der p-Wert korrigiert. Über das p.adjust.methodArgument können unter anderem Bonferroni (\"bonferroni\"), Benjamini-Hochberg (\"BH\") oder Holm (“holm”) verwendet werden. pairwise.t.test(chemo$Leukos_t6, chemo$Behandlung, p.adjust.method = &quot;BH&quot;) Pairwise comparisons using t tests with pooled SD data: chemo$Leukos_t6 and chemo$Behandlung Radiochemo Chemo Chemo 0.407 - Radio 0.024 0.102 P value adjustment method: BH Auch hier können wir mit einem p-Wert von 0.024 einen statistisch signifikanten Unterschied zwischen der mittleren Leukozytenzahl bei PatientInnen mit Radiochemotherapie und Radiotherapie erkennen. Die übliche Effektstärke im Kontext von Varianzanalysen ist das partielle \\(\\eta^2\\), welches Aufschluss über den korrigierten Anteil der Varianzaufklärung der jeweiligen unabhängigen Variable gibt. In R berechnen wir diese mit der Funktion eta_squared() aus dem effectsize Package und setzen das Argument partial auf TRUE. eta_squared(res_aov, partial = TRUE) # Effect Size for ANOVA (Type III) Parameter | Eta2 (partial) | 95% CI ------------------------------------------ Behandlung | 0.02 | [0.00, 1.00] Geschlecht | 4.80e-03 | [0.00, 1.00] - One-sided CIs: upper bound fixed at [1.00]. Wir erhalten das partielle \\(\\eta^2\\) für beide Haupteffekte. Beachte, dass 4.80e-03 die wissenschaftliche Notation für 0.0048 ist. Die sogenannte Kovarianzanalyse (kurz ANCOVA) korrigiert den Haupteffekt um den Einfluss einer intervallskalierten Kovariate. So könnten wir in unserem Beispiel möglicherweise die Vermutung haben, dass der Einfluss der Behandlungsmethode auf die Leukozytenzahl maßgeblich vom Alter der PatientInnen abhängt. Zum Beispiel könnten die weißen Blutzellen bei älteren Menschen generell niedriger sein. Um die Interaktion der Kovariaten zu berücksichtigen, verwenden wir einen Doppelpunkt. result2 &lt;- aov(Leukos_t6 ~ Behandlung + Geschlecht + Behandlung:Alter, data = chemo) Anova(result2, type = 3) Anova Table (Type III tests) Response: Leukos_t6 Sum Sq Df F value Pr(&gt;F) (Intercept) 46.867 1 435.2811 &lt;2e-16 *** Behandlung 0.019 2 0.0894 0.9145 Geschlecht 0.289 1 2.6805 0.1023 Behandlung:Alter 0.612 3 1.8952 0.1296 Residuals 47.698 443 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Während der Haupteffekt des Geschlechts fast identisch geschätzt wird, ist die Behandlungsart plötzlich nicht mehr statistisch signifikant. Auch wenn der Interaktionsterm selbst nicht signifikant ist, könnte dies ein Hinweis auf eine altersabhängige Leukozytenanzahl sein. Um intervallskalierte Merkmale als unabhängige Variablen direkt zu untersuchen, müssen wir auf Regressionsmodelle zurückgreifen, die in Kapitel 9.5 eingeführt werden. Die abhängige Variable muss bei der Varianzanalyse immer intervallskaliert und die unabhängigen Variablen nominal skaliert sein. Nur in der Kovarianzanalyse können wir als weitere Kovariate in Form eines Interaktionsterms eine weitere intervallskalierte unabhängige Variable hinzufügen. Die Varianzanalyse ist gegenüber Verletzungen der Voraussetzungen der Normalverteilung und Varianzhomogenität relativ robust. Solange die Verletzungen nicht zu groß sind, stellt das folglich kein großes Problem dar. Erst bei gröberen Verletzungen müssen wir auf nicht-parametrische Tests wie dem Kruskal-Wallis Test zurückgreifen (siehe Kapitel 9.3.3) 9.3.2 Varianzanalyse mit Messwiederholung In diesem Unterkapitel müssen die Packages afex und emmeans installiert und geladen sein. library(afex) library(emmeans) Die Varianzanalyse für abhängige Stichproben wird häufig auch als Varianzanalyse mit Messwiederholung bezeichnet (engl. repeated-measures ANOVA). In diesem Falle haben wir, neben den bereits im vorherigen Kapitel untersuchten Zwischensubjektfaktoren (Behandlung und Geschlecht), ebenfalls Innersubjektfaktoren vorliegen. Jede Person wird also mehrfach untersucht. In unserem Beispiel wurde die Leukozytenanzahl bei Therapiebeginn und sechs Monate später erfasst. In diesem Kapitel untersuchen wir, ob sich die mittlere Leukozytenzahl zwischen diesen Zeitpunkten in Abhängigkeit der anderen unabhängigen Variablen unterscheidet. Bevor wir zur Berechnung kommen, müssen wir den Datensatz zunächst in ein langes Format bringen (siehe Kapitel 6.6). chemo_long &lt;- chemo |&gt; select(Leukos_t0, Leukos_t6, Geschlecht, Behandlung, Pat_id, Alter) |&gt; pivot_longer( cols = c(Leukos_t0, Leukos_t6), values_to = &quot;Leukos&quot;, names_to = &quot;Zeitpunkt&quot; ) chemo_long # A tibble: 900 × 6 Geschlecht Behandlung Pat_id Alter Zeitpunkt Leukos &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 f Radio 1 49 Leukos_t0 5.74 2 f Radio 1 49 Leukos_t6 5.68 3 f Radiochemo 2 46 Leukos_t0 6.71 4 f Radiochemo 2 46 Leukos_t6 6.64 # ℹ 896 more rows Anschließend übergeben wir der Funktion aov_4() aus dem afex Package, wie gewohnt, auf der linken Seite der Tilde die abhängige Variable und auf der rechten Seite der Tilde die unabhängigen Variablen. Innerhalb runder Klammern werden die Innersubjektfaktoren (hier Zeitpunkt) definiert, gegeben (|) des Personenidentifikators (hier Pat_id). Die Ziffer 4 befindet sich im Funktionsnamen, weil die verwendete Formelschreibweise auf dem lme4 Package basiert, was uns an dieser Stelle nicht weiter interessieren muss. Zusätzlich können wir mit dem Argument anova_table() innerhalb einer Liste diverse Anpassungen vornehmen. Hier verändern wir die Effektstärke zum partiellen \\(\\eta^2\\). Standardmäßig wird ansonsten das generalisierte \\(\\eta^2\\) berechnet, welches die explizite Definition der beobachteten Variablen mithilfe des observed Arguments benötigt. Also alle Variablen, die wir nicht experimentell verändert haben (in unserem Fall das Geschlecht). result3 &lt;- aov_4( formula = Leukos ~ Behandlung + Geschlecht + (Zeitpunkt | Pat_id), data = chemo_long, anova_table = list(es = &quot;pes&quot;) ) result3 Anova Table (Type 3 tests) Response: Leukos Effect df MSE F pes p.value 1 Behandlung 2, 446 0.22 3.70 * .016 .026 2 Geschlecht 1, 446 0.22 2.18 .005 .141 3 Zeitpunkt 1, 446 0.00 63467.03 *** .993 &lt;.001 4 Behandlung:Zeitpunkt 2, 446 0.00 0.43 .002 .651 5 Geschlecht:Zeitpunkt 1, 446 0.00 3.72 + .008 .054 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Die Ergebnisse der Zwischensubjektfaktoren Behandlung und Geschlecht sind in etwa wie im vorherigen Kapitel bei der Varianzanalyse für unabhängige Stichproben. Der Innersubjektfaktor Zeitpunkt zeigt einen statistisch signifikanten mit p-Wert kleiner als 001 und somit kleiner als 0.05. Die Werte ohne Rundung würden wir mit summary(result3) erhalten. Bei mehr als zwei Stufen des Innersubjektfaktors gibt, würde summary(result3) die Ergebnisse ohne Korrektur, mit Greenhouse-Geisser Korrektur und Hyunh-Feldt Korrektur sowie das Ergebnis des Mauchly Tests für Sphärizität ausgeben. Da wir hier nur zwei Messzeitpunkte vorliegen haben, erhalten wir nur die Schätzwerte der Varianzanalyse. Als Post-hoc Test schätzen wir die Randmittel mithilfe der Funktion emmeans() aus dem gleichnamigen Package (Akronym für estimated marginal means, engl. für geschätzte Randmittelwerte). Neben dem Ergebnis der Varianzanalyse müssen wir der Funktion zusätzlich die zu untersuchende Gruppe nachfolgend an eine Tilde übergeben. post_aov &lt;- emmeans(result3, ~ Behandlung) post_aov Behandlung emmean SE df lower.CL upper.CL Radiochemo 6.05 0.0269 446 6.00 6.10 Chemo 6.02 0.0270 446 5.97 6.07 Radio 5.95 0.0271 446 5.90 6.00 Results are averaged over the levels of: Geschlecht, Zeitpunkt Confidence level used: 0.95 Um die unterschiedlichen Mittelwerte durch Hypothesentests miteinander zu vergleichen, verwenden wir an dieser Stelle die Funktion pairs(). Das Ergebnis entspricht dabei dem des vorherigen Kapitel, da Behandlung als Zwischensubjektfaktor auch im Kontext einer Varianzanalyse für unabhängige Stichproben adäquat analysiert werden kann. pairs(post_aov) contrast estimate SE df t.ratio p.value Radiochemo - Chemo 0.0315 0.0381 446 0.827 0.6868 Radiochemo - Radio 0.1015 0.0382 446 2.659 0.0221 Chemo - Radio 0.0700 0.0382 446 1.831 0.1607 Results are averaged over the levels of: Geschlecht, Zeitpunkt P value adjustment: tukey method for comparing a family of 3 estimates Die Funktion emmeans() erlaubt es auch, die Behandlungsarten nach Zeitpunkt oder Geschlecht weiter aufzutrennen. Dabei müssen die einzelnen Faktoren mit einem Pluszeichen voneinander getrennt werden. Auch hier könnte man Hypothesentests mithilfe der pairs() Funktion durchführen. emmeans(result3, ~ Behandlung + Zeitpunkt) Behandlung Zeitpunkt emmean SE df lower.CL upper.CL Radiochemo Leukos_t0 6.08 0.0270 446 6.03 6.13 Chemo Leukos_t0 6.05 0.0271 446 6.00 6.10 Radio Leukos_t0 5.98 0.0272 446 5.93 6.03 Radiochemo Leukos_t6 6.02 0.0268 446 5.97 6.07 Chemo Leukos_t6 5.99 0.0269 446 5.94 6.04 Radio Leukos_t6 5.92 0.0270 446 5.87 5.97 Results are averaged over the levels of: Geschlecht Confidence level used: 0.95 Alle unabhängigen Variablen werden von der Funktion aov_4() standardmäßig als nominale Merkmale angenommen und daher als Faktor umgewandelt, falls diese einen anderen Datentyp haben. Im Falle einer Kovarianzanalyse mit Messwiederholung würde somit ebenfalls die intervallskalierte Kovariate als Faktor formatiert, was keinen Sinn ergibt und in einem Fehler endet. Bei einer Kovarianzanalyse müssen folglich Faktoren im Voraus erstellt werden. In unserem Beispiel ist die Spalte Behandlung zwar bereits ein Faktor, zur besseren Übersicht wird die Umwandlung an dieser Stelle explizit gezeigt (siehe Kapitel 6.4 und 6.10). Die Kovariate (hier Alter) sollte darüber hinaus standardisiert werden. chemo_long1 &lt;- chemo_long |&gt; mutate( Behandlung = factor(Behandlung, levels = c(&quot;Radiochemo&quot;, &quot;Chemo&quot;, &quot;Radio&quot;)), Alter = as.numeric(scale(Alter)) ) In der Funktion aov_4() setzen wir nun zusätzlich das Argument factorize auf FALSE, um die automatische Umwandlung der Kovariate Alter in einen Faktor zu verhindern. aov_4( formula = Leukos ~ Behandlung + Alter + (Zeitpunkt | Pat_id), data = chemo_long1, anova_table = list(es = &quot;pes&quot;), factorize = FALSE ) Anova Table (Type 3 tests) Response: Leukos Effect df MSE F pes p.value 1 Behandlung 2, 446 0.22 3.56 * .016 .029 2 Alter 1, 446 0.22 4.93 * .011 .027 3 Zeitpunkt 1, 446 0.00 63370.93 *** .993 &lt;.001 4 Behandlung:Zeitpunkt 2, 446 0.00 0.41 .002 .664 5 Alter:Zeitpunkt 1, 446 0.00 1.85 .004 .175 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Neben Normalverteilung und Varianzhomogenität müssen wir bei Varianzanalysen für abhängige Stichproben mit mehr als zwei Stufen des Innersubjektfaktors (z.B. drei Messzeitpunkte) ebenfalls die Sphärizität überprüfen. Das Vorliegen der Sphärizität wird automatisch von aov_4() getestet und mit entsprechenden Korrekturen zurückgegeben. Falls zu grobe Verletzungen dieser drei Annahmen vorliegen, können wir auf nicht-parametrische Verfahren wie den Friedman Test wechseln (siehe Kapitel 9.3.4). 9.3.3 Welch Approximation und Kruskal-Wallis Test Beide in diesem Unterkapitel vorgestellte statistische Tests sind Alternativen für eine Varianzanalyse bei vermuteter grober Verletzungen der Annahmen über Normalverteilung und Varianzhomogenität, welche allerdings auf eine unabhängige Variable limitiert sind. Falls die Voraussetzung der Normalverteilung erfüllt ist, aber keine Varianzhomogenität besteht, kann die Welch Approximation der Varianzanalyse verwendet werden. Dieser kann als Generalisierung des in Kapitel 9.2.1 kennengelernten t-Tests mit Welch Korrektur verstanden werden. Die Verallgemeinerung bezieht sich hierbei auf die Anwendung bei mehr als zwei Gruppen (hier z.B. drei Behandlungsarten). In R ist diese Methode innerhalb der Funktion oneway.test() implementiert. Wir erhalten hier ein sehr ähnliches Ergebnis zu der Varianzanalyse aus Kapitel 9.3.1, da in unserem Beispiel des chemo Datensatzes sowohl Normalverteilung als auch Varianzhomogenität vorliegt (siehe Kapitel 9.7). oneway.test(Leukos_t6 ~ Behandlung, data = chemo) One-way analysis of means (not assuming equal variances) data: Leukos_t6 and Behandlung F = 3.8224, num df = 2.00, denom df = 297.59, p-value = 0.02296 Im Falle einer Verletzung der Annahme von Normalverteilung und Varianzhomogenität greifen wir stattdessen auf den Kruskal-Wallis Test zurück, der durch die Funktion kruskal.test() in R enthalten ist. Auch hier kriegen wir ein ähnliches Ergebnis zu der Varianzanalyse ausgegeben. kruskal.test(Leukos_t6 ~ Behandlung, data = chemo) Kruskal-Wallis rank sum test data: Leukos_t6 by Behandlung Kruskal-Wallis chi-squared = 5.9832, df = 2, p-value = 0.05021 Beide Verfahren bieten eine Alternative zur klassischen Varianzanalyse bei Verletzung der Annahmen von Normalverteilung und/oder Varianzhomogenität. Beachte hierbei, dass nur eine unabhängige, nominal skalierte Variable in Hinblick eines möglichen Einflusses auf ein intervallskaliertes Merkmal untersucht werden kann. 9.3.4 Friedman Test Eine nicht-parametrische Alternative zur Varianzanalyse für abhängige Stichproben bei Verletzungen von Annahmen der Normalverteilung, Varianzhomogenität oder Sphärizität stellt der Friedman Test dar. Dieses Verfahren hat die Einschränkung, nur den Innersubjektfaktor einzeln untersuchen zu können. Da auch hier häufig eine Messwiederholung die interessierende nominale unabhängige Variable darstellt, müssen wir zunächst den Datensatz in ein langes Format bringen (siehe Kapitel 6.6). chemo_long &lt;- chemo |&gt; select(Leukos_t0, Leukos_t6, Geschlecht, Behandlung, Pat_id) |&gt; pivot_longer( cols = c(Leukos_t0, Leukos_t6), values_to = &quot;Leukos&quot;, names_to = &quot;Zeitpunkt&quot; ) chemo_long # A tibble: 900 × 5 Geschlecht Behandlung Pat_id Zeitpunkt Leukos &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 f Radio 1 Leukos_t0 5.74 2 f Radio 1 Leukos_t6 5.68 3 f Radiochemo 2 Leukos_t0 6.71 4 f Radiochemo 2 Leukos_t6 6.64 # ℹ 896 more rows Die Schreibweise unterscheidet sich nur insofern von der Varianzanalyse mit Messwiederholung aus Kapitel 9.3.2, als dass wir keine Klammern um den Messwiederholungsterm auf der rechten Seite der Tilde schreiben. Getrennt sind Innersubjektfaktor und Personenidentifikator durch einen vertikalen Strich. friedman.test(Leukos ~ Zeitpunkt | Pat_id, data = chemo_long) Friedman rank sum test data: Leukos and Zeitpunkt and Pat_id Friedman chi-squared = 450, df = 1, p-value &lt; 2.2e-16 Die Ausgabe zeigt auch hier ein statistisch signifikantes Ergebnis. Die Leukozytenanzahl unterscheidet sich demnach zwischen dem Therapiebeginn und nach sechs Monaten Behandlung nennenswert. 9.4 Korrelationsanalysen 9.4.1 Produkt-Moment Korrelation Bereits in Kapitel 7.3 haben wir Korrelationen in Form im Kontext der deskriptiven Statistik als Zusammenhangsmaße berechnet. Wir können allerdings auch die Nullhypothese eines nicht vorhandenen Zusammenhangs (Korrelation von 0) gegen die Alternativhypothese eines bestehenden Zusammenhangs prüfen. Die Funktion für diese Hypothesentests heißt in R cor.test(). Für die Produkt-Moment Korrelation nach Pearson können wir dem method Argument “pearson” übergeben. Wir müssen allerdings nicht, da dies die Standardeinstellung der Funktion darstellt. Mit der Produkt-Moment Korrelation vergleichen wir zwei intervallskalierte Merkmale miteinander. An dieser Stelle möchten wir herausfinden, ob es einen Zusammenhang zwischen der Leukoszytenanzahl nach sechs Monaten Behandlung und dem Alter gibt. Die Merkmale übergeben wir dabei als Vektoren (bzw. Wertereihen) mithilfe des Dollar-Operators (siehe Kapitel 4.5). cor.test(chemo$Leukos_t6, chemo$Alter, method = &quot;pearson&quot;) Pearson&#39;s product-moment correlation data: chemo$Leukos_t6 and chemo$Alter t = 2.2785, df = 448, p-value = 0.02317 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.01473737 0.19751435 sample estimates: cor 0.10703 Die Ausgabe enthält die Teststatistik in Form eines t-Wertes mit Freiheitsgraden und entsprechendem statistisch signifikanten p-Wert, welcher mit 0.02317 kleiner als das in der Wissenschaft übliche \\(\\alpha\\)-Niveau von 0.05 ist. Außerdem wird die Korrelation in Höhe von 0.10703 mit 95% Konfidenzintervall ausgegeben. Je älter die PatientInnen sind, desto mehr Leukozyten haben wir nach sechs Monaten in unserer Stichprobe gemessen (positive Korrelation). Vielleicht haben die Behandlungen eine einschneidenere Auswirkung auf das Immunsystem jüngerer PatientInnen. Das ist allerdings reine Mutmaßung, da ein vorhandener Zusammenhang nicht zwingend auch eine Kausalität bedeutet. Ein Sonderfall der Pearson Korrelation ist die Punkt-biseriale Korrelation, welche den Zusammenhang zwischen einem intervallskaliertes Merkmal und einem nominalen Merkmal mit zwei Ausprägungen quantifiziert. Hierfür müssen wir zunächst die Inhalte der Spalte Geschlecht in Zahlen umwandeln (siehe Kapitel 6.4.3). In diesem Fall steht eine 1 für weiblich und eine 0 für männlich. chemo1 &lt;- chemo |&gt; mutate(Geschlecht = case_match(Geschlecht, &quot;f&quot; ~ 1, &quot;m&quot; ~ 0)) Anschließend übergeben wir die beiden Spalten auf dieselbe Art und Weise wie zuvor der Funktion cor.test(). cor.test(chemo1$Leukos_t6, chemo1$Geschlecht, method = &quot;pearson&quot;) Pearson&#39;s product-moment correlation data: chemo1$Leukos_t6 and chemo1$Geschlecht t = 1.456, df = 448, p-value = 0.1461 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.02396547 0.16004821 sample estimates: cor 0.06862502 Als Ergebnis erhalten wir eine Korrelation in Höhe von 0.07, die nicht statistisch signifikant von 0 abweicht, da der p-Wert mit 0.1461 größer als 0.05 ist. Dies stimmt mit den Ergebnissen des t Tests aus Kapitel 9.2.1 überein, indem wir keinen Unterschied zwischen den Geschlechtern in Bezug auf die Leukozytenanzahl finden konnten. Die Produkt-Moment Korrelation quantifiziert den Zusammenhang zwischen zwei intervallskalierten Merkmalen. Falls ordinales Skalenniveau vorliegt, muss stattdessen auf Rangkorrelation zurückgegriffen werden. 9.4.2 Rangkorrelationen Bei Vorliegen zweier ordinaler Merkmale können wir nicht-parametrischen Rangkorrelationen verwenden. Eine weit verbreitete Methode dafür ist die Rangkorrelation nach Spearman. Im Vergleich zum vorherigen Kapitel verändert sich lediglich das method Argument. Die Merkmale müssen auch hier in Form von Vektoren (bzw. Wertereihen) mithilfe des Dollar-Operators aus dem Datensatz extrahiert werden (siehe Kapitel 4.5). Wir möchten an dieser Stelle untersuchen, ob das subjektive Schmerzausmaß der PatientInnen mit der empfundenen Lebensqualität zusammenhängt. Beide Merkmale sind durch einen Fragebogen mit zehn Stufen erhoben worden. Auf der Schmerzskala entspricht eine 1 keinen und eine 10 unerträglichen Schmerzen. Bei der Lebensqualität steht eine 1 für unzumutbare Einschränkungen und eine 10 für uneingeschränktes Glück. Die Nullhypothese ist auch bei der Rangkorrelation ein fehlender Zusammenhang, während die Alternativhypothese eine Korrelation ungleich 0 beschreibt. cor.test(chemo$Schmerzen, chemo$Lebensqualitaet, method = &quot;spearman&quot;) Warning in cor.test.default(chemo$Schmerzen, chemo$Lebensqualitaet, method = &quot;spearman&quot;): Cannot compute exact p-value with ties Spearman&#39;s rank correlation rho data: chemo$Schmerzen and chemo$Lebensqualitaet S = 23595117, p-value &lt; 2.2e-16 alternative hypothesis: true rho is not equal to 0 sample estimates: rho -0.5535956 Die Rangkorrelation nach Spearman wird als \\(\\rho\\) (rho) bezeichnet. In der Ausgabe sehen wir eine statistisch signifikante Abweichung der Korrelation von 0, da ein p-Wert von 2.2e-16 die wissenschaftliche Notation für 0.00000000000000022 darstellt und somit deutlich kleiner als das übliche \\(\\alpha\\)-Niveau von 0.05 ist. Wir finden eine negative Korrelation von -0.55 vor. Je mehr Schmerzen die PatientInnen haben, desto geringer ist ihre Lebensqualität in unserer Stichprobe. Umgekehrt, je höher die Lebensqualität ist, desto niedriger die Schmerzen. Auch hier muss bei kausalen Schlussfolgerungen Vorsicht geboten werden, weil Korrelationskoeffizienten lediglich Zusammenhänge quantifizieren. Zusätzlich erhalten wir eine Warnmeldung, dass sogenannte Ties vorliegen. Ties beschreiben im Kontext von Rangreihungen mehrere Beobachtungen mit demselben Rang. In diesem Fall ist die Berechnung der Kendall-Korrelation angemessener, wofür wir lediglich das method Argument entsprechend anpassen müssen. cor.test(chemo$Schmerzen, chemo$Lebensqualitaet, method = &quot;kendall&quot;) Kendall&#39;s rank correlation tau data: chemo$Schmerzen and chemo$Lebensqualitaet z = -12.197, p-value &lt; 2.2e-16 alternative hypothesis: true tau is not equal to 0 sample estimates: tau -0.4351842 Die Höhe des negativen Zusammenhangs ist hier mit -0.44 etwas kleiner, aber ebenfalls statistisch signifikant von 0 abweichend. 9.5 Regressionsmodelle 9.5.1 Lineare Regression Lineare Regressionmodelle werden zur Untersuchung des Zusammenhangs zwischen einer intervallskalierten abhängigen Variable und einer bis zu mehreren unabhängigen Variablen verwendet. Der Hauptunterschied zur einfachen Korrelation des vorherigen Kapitels besteht in der Möglichkeit, mehr als einen möglichen Einfluss auf das interessierende Merkmal zu prüfen. Die unabhängigen Variablen müssen dabei intervallskaliert (z.B. Alter) oder nominal mit zwei Ausprägungsgraden (z.B. Geschlecht) sein. Bei mehr als zwei Ausprägungsgraden müssen sogenannte Dummy Variablen erstellt werden. Im Vergleich zu einer Referenzgruppe erhalten diese die Information in Form einer 0 oder 1, ob Gruppenzugehörigkeit besteht. Die einfachste Möglichkeit in R besteht darin, einen Faktor aus einem nominalen Merkmal mit mehr als zwei Ausprägungen zu erstellen (siehe Kapitel 6.10). Wir werden dies im Rahmen der Behandlungsarten im weiteren Verlauf dieses Kapitels durchgehen. In R erstellt die Funktion lm() ein lineares Regressionsmodell (Akronym für lineares Modell). Auf der linken Seite der Tilde (~) wird die abhängige Variable und auf der rechten Seite eine oder mehrere unabhängige Variablen der Funktion übergeben. Wir stellen uns an dieser Stelle die Frage eines möglichen Zusammenhangs zwischen der Leukozytenanazahl nach sechs Monaten Behandlung und den unabhängigen Variablen Alter und Geschlecht. Eine Übersicht über die Ergebnisse erhalten wir mit der Funktion summary(). res_reg &lt;- lm(Leukos_t6 ~ Alter + Geschlecht, data = chemo) summary(res_reg) Call: lm(formula = Leukos_t6 ~ Alter + Geschlecht, data = chemo) Residuals: Min 1Q Median 3Q Max -0.92152 -0.23057 0.01753 0.21903 0.87584 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.614790 0.162217 34.613 &lt;2e-16 *** Alter 0.008676 0.003618 2.398 0.0169 * Geschlechtm -0.051062 0.031170 -1.638 0.1021 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.3294 on 447 degrees of freedom Multiple R-squared: 0.01735, Adjusted R-squared: 0.01296 F-statistic: 3.947 on 2 and 447 DF, p-value: 0.01998 In der Ausgabe sehen wir zunächst die Verteilung der Residuen. Darunter sind die \\(\\beta\\)-Koeffizienten als estimates für jede Kovariate aufgelistet. Der Intercept (\\(\\beta_0\\)) ist entsprechend darüber angegeben. In den nächsten Spalten ist der Standardfehler und der t-Wert mit entsprechendem p-Wert angegeben. Die Teststatistik als t-Wert basiert in diesem Falle auf dem Wald Test, der die Nullhypothese prüft, ob der \\(\\beta\\)-Koeffizient der jeweiligen unabhängigen Variable gleich 0 ist (kein Zusammenhang). Die Alternativhypothese ist hier, ein \\(\\beta\\)-Koeffizient ungleich 0. Beachte außerdem, dass es sich hierbei um partielle Korrelationen handelt. Der Einfluss der jeweils anderen unabhängigen Variablen auf die abhängige Variable wird dabei herausgerechnet. Der \\(\\beta\\)-Koeffizient der Kovariate Alter weicht statistisch signifikant von 0 ab, da der p-Wert mit 0.0169 kleiner als das übliche \\(\\alpha\\)-Niveau von 0.05 ist. Da der \\(\\beta\\)-Koeffizient positiv ist, haben wir hier einen positiven Zusammenhang vorliegen: je älter die PatientInnen sind, desto höher ist die Leukozytenanzahl und umgekehrt. Beim Geschlecht finden wir hingegen keinen nennenswerten Einfluss auf die Leukozytenanzahl in unserer Stichprobe (0.1021 ist größer als 0.05). Der \\(\\beta\\)-Koeffizient ist negativ, aber welches der Geschlechter hat eine geringer Leukozytenanzahl? Hinter dem Geschlecht ist ein kleines m angehängt, womit R uns sagt, dass das weibliche Geschlecht innerhalb der Funktion mit 0 und das männliche Geschlecht mit 1 kodiert wird. Wenn die Kovariate Geschlecht einen Wert von 1 hat, reduziert sich die Leukozytenanzahl. Folglich ist die Anzahl weißer Blutkörperchen bei Männern nach sechs Monaten Behandlung tendenziell geringer. Allerdings ist dieser Zusammenhang in unserem Fall nicht statistisch signifikant. Weiter unten in der Ausgabe finden wir noch die Effektstärke der Regressionsanalyse: die Varianzaufklärung in Form des (korrigierten) \\(R^2\\). Darunter ist das Ergebnis des globalen F-Tests ausgegeben, welcher die Alternativhypothese prüft, ob mindestens einer der \\(\\beta\\)-Koeffizienten ungleich 0 ist. Da eine der Kovariaten einen nennenswerten Zusammenhang zeigt mit der Leukozytenanzahl zeigt, finden wir auch hier einen statistisch signifikanten p-Wert vor (0.01998). Die Voraussetzungen der linearen Regression sind Normalverteilung und Varianzhomogenität der Residuen. Diese lassen sich am einfachsten und besten graphisch überprüfen (siehe Kapitel 8.13.2 und 9.7). Auch im Kontext von Regression können wir mithilfe eines Doppelpunktes Interaktionsterme zum Modell hinzufügen. Hier schauen wir uns zusätzlich zu den bisherigen Kovariaten ebenfalls die Interaktion zwischen Alter und Geschlecht an. Hier untersuchen wir also, ob der Alterseffekt möglicherweise je nach Geschlecht anders ausgeprägt ist. res_reg2 &lt;- lm(Leukos_t6 ~ Alter + Geschlecht + Alter:Geschlecht, data = chemo) summary(res_reg2) Call: lm(formula = Leukos_t6 ~ Alter + Geschlecht + Alter:Geschlecht, data = chemo) Residuals: Min 1Q Median 3Q Max -0.9323 -0.2266 0.0190 0.2186 0.8763 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.871429 0.229189 25.618 &lt;2e-16 *** Alter 0.002902 0.005134 0.565 0.5721 Geschlechtm -0.562745 0.324841 -1.732 0.0839 . Alter:Geschlechtm 0.011431 0.007223 1.582 0.1143 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.3288 on 446 degrees of freedom Multiple R-squared: 0.02284, Adjusted R-squared: 0.01627 F-statistic: 3.475 on 3 and 446 DF, p-value: 0.01606 Wir können beobachten, dass der Alterseffekt nach Berücksichtigung einer möglichen Interaktion mit dem biologischen Geschlecht nun nicht mehr statistisch signifikant ist. Auch wenn der Interaktionsterm an sich nicht signifikant ist, könnte dies ein Hinweis auf einen gewissen Zusammenhang zwischen Alter und Geschlecht hindeuten. Falls die nominale unabhängige Variable mehr als zwei Stufen hat, solltest du vor der Analyse einen Faktor daraus machen (siehe Kapitel 6.10). Definiere dabei explizit, welche Gruppe die Referenz darstellen soll (auch Baseline genannt). Die \\(\\beta\\)-Koeffizienten der jeweils anderen Gruppen beziehen sich dann auf Veränderungen im Vergleich zur Referenzgruppe (Stufe 1 des Faktors). An dieser Stelle untersuchen wir mögliche Zusammenhänge zwischen den Behandlungsarten und der Leukozytenanzahl nach sechs Monaten Therapie. Die Referenzgruppe ist in diesem Fall die Behandlung mit Radiochemotherapie. res_reg3 &lt;- lm(Leukos_t6 ~ Behandlung, data = chemo) summary(res_reg3) Call: lm(formula = Leukos_t6 ~ Behandlung, data = chemo) Residuals: Min 1Q Median 3Q Max -0.9727 -0.2224 0.0088 0.2387 0.8588 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.02272 0.02682 224.58 &lt; 2e-16 *** BehandlungChemo -0.03152 0.03799 -0.83 0.40722 BehandlungRadio -0.10124 0.03805 -2.66 0.00808 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.3295 on 447 degrees of freedom Multiple R-squared: 0.01629, Adjusted R-squared: 0.01189 F-statistic: 3.701 on 2 and 447 DF, p-value: 0.02545 Während beide \\(\\beta\\)-Koeffizienten negativ sind, ergibt nur der Wald Tests der Gruppe der Radiotherapie einen statistisch signifikanten p-Wert (0.00808 ist kleiner als 0.05). Die Leukoyztenzahl ist folglich sowohl bei alleiniger Chemotherapie als auch alleiniger Radiotherapie kleiner als bei der Kombinationstherapie. Allerdings scheint der Zusammenhang beim Vergleich zwischen Radiochemotherapie und Chemotherapie kleiner ausgeprägt zu sein. Im linearen Regressionsmodell untersuchen wir den Zusammenhang zwischen einer intervallskalierten abhängigen Variable und einer oder mehrerer intervallskalierter oder nominaler unabhängiger Variable. Falls die abhängige Variable nicht intervallskaliert ist, müssen wir auf ein anderes Regressionsmodell zurückgreifen. 9.5.2 Logistische Regression Logistische Regression verwenden wir zur Untersuchung einer abhängigen Variablen mit nominalem Skalenniveau mit zwei Ausprägungsgraden (0 und 1), die aus einer Binomialverteilung stammen (Synonym: Binomiales Logit Modell). In R trägt die Funktion zur Modellierung einer logistischen Regression den Namen glm() (Akronym für generalisiertes lineares Modell). Wir möchten an dieser Stelle die Frage beantworten, ob das biologische Geschlecht und die Infektionshäufigkeit innerhalb von zwei Jahren einen Zusammenhang mit dem stationären Aufenthalt aufweist. Die abhängige Variable des stationären Aufenthalts (Stationaer) beinhaltet die Information, ob jemand im Beobachtungszeitraum einen Krankenhausaufenthalt hatte (1) oder nicht (0). Das Aufstellen der Regressionsgleichung funktioniert hier äquivalent zu den bisher kennengelernten linearen Modellen. Auf der linken Seite der Tilde schreiben wir die abhängige Variable und auf der rechten Seite die unabhängigen Variablen, getrennt von einem Pluszeichen. Zusätzlich definieren wir über das family Argument die Verteilung als binomial (ohne Anführungszeichen). Beachte, dass wir hier die Funktion glm() anstelle von lm() verwenden. Die Ergebnisse werden von der Funktion summary() zusammengefasst. res_reg4 &lt;- glm(Stationaer ~ Geschlecht + Infektionen, data = chemo, family = binomial) summary(res_reg4) Call: glm(formula = Stationaer ~ Geschlecht + Infektionen, family = binomial, data = chemo) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.99282 0.32229 -9.286 &lt; 2e-16 *** Geschlechtm 0.19232 0.33572 0.573 0.56674 Infektionen 0.21365 0.06936 3.080 0.00207 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 269.96 on 449 degrees of freedom Residual deviance: 260.76 on 447 degrees of freedom AIC: 266.76 Number of Fisher Scoring iterations: 5 Während der \\(\\beta\\)-Koeffizient für das Geschlecht nicht statistisch signifikant von 0 abweicht, erhalten wir für die Infektionshäufigkeit einen nennenswerten Zusammenhang mit der Notwendigkeit eines Krankenhausaufenthalts (0.00207 ist kleiner als 0.05). Der positive \\(\\beta\\)-Koeffizient besagt, dass die Wahrscheinlichkeit eines Krankenhausaufenthalts mit steigender Infektionshäufigkeit ansteigt. Die Odds Ratios erhalten wir durch das Exponieren der \\(\\beta\\)-Koeffizienten, die wir mit dem Dollar-Operator aus der Ergebnisliste herausziehen (siehe Kapitel 4.5). exp(res_reg4$coefficients) (Intercept) Geschlechtm Infektionen 0.05014566 1.21206341 1.23818556 Im Unterschied zu den linearen Modellen wird uns an dieser Stelle zusätzlich ein Informationskriterium (AIC) ausgegeben, mit welchem wir mehrere ähnliche Modelle miteinander vergleichen könnten. Für eine genaue Beschreibung der Ausgabe von Regressionsmodellen in R und deren Voraussetzungen sei auf Kapitel 9.5.1 verwiesen. 9.5.3 Multinomiales Logit Modell Für dieses Unterkapitel muss das Package VGAM installiert und geladen sein. Beachte hier, dass der Name des Packages vollständig in Großbuchstaben geschrieben ist. library(VGAM) Möchten wir eine abhängige Variable mit nominalem Skalenniveau mit mehr als zwei Ausprägungen untersuchen, greifen wir auf ein Logit Modell unter multinomialer Verteilungsannahme zurück. Da wir ein nominales Skalenniveau und somit ungeordnete Kategorien vorliegen, müssen wir uns auch hier für eine Referenzkategorie entscheiden. Falls die erste Kategorie als Referenz gewählt wird, ist das Modell in der Literatur auch als sogenanntes Baseline Logit Modell beschrieben. Wir möchten untersuchen, inwiefern die unabhängigen Variablen Alter und Geschlecht einen Einfluss auf drei mögliche Komorbiditäten der PatientInnen hat. Hierbei handelt es sich um weitere Erkrankungen, die neben der Haupterkrankung auftreten können. In unserem Beispiel haben 271 keine weitere Erkrankung, während bei insgesamt 179 zusätzlich entweder eine Erkrankungen der Lungen, des Herzens oder Gehirns auftritt. table(chemo$Komorb) Keine Lunge Herz Gehirn 271 67 63 49 Umgesetzt ist das multinomiale Logit Modell in R in Form der vglm() Funktion aus dem VGAM Package. Wir übergeben der Funktion die abhängige Variable auf der linken Seite der Tilde und die unabhängigen Variablen auf der rechten Seite. Entscheiden ist hier das family Argument. Innerhalb der Helferfunktion multinomial() legen wir die Referenzkategorie fest. Hier entscheiden wir uns für die erste Faktorstufe (keine Komorbidität). Die drei weiteren Faktorstufen Lunge, Herz und Gehirn werden im Vergleich zur Referenz betrachtet. Wir erhalten folglich einen Intercept und zwei \\(\\beta\\)-Koeffizienten pro Kategorie. Welche Faktorstufe die erste ist, muss explizit im Zuge der Erstellung des Faktors definiert werden (siehe Kapitel 6.10). Die Zusammenfassung der Ergebnisse wird mit der Funktion summary() erstellt. reg_res5 &lt;- vglm( formula = Komorb ~ Geschlecht + Alter, data = chemo, family = multinomial(refLevel = 1) ) summary(reg_res5) Call: vglm(formula = Komorb ~ Geschlecht + Alter, family = multinomial(refLevel = 1), data = chemo) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 0.545334 1.425047 0.383 0.702 (Intercept):2 -0.696183 1.466621 -0.475 0.635 (Intercept):3 0.008255 1.619572 0.005 0.996 Geschlechtm:1 -0.071655 0.274680 -0.261 0.794 Geschlechtm:2 -0.097160 0.281295 -0.345 0.730 Geschlechtm:3 -0.047474 0.312092 -0.152 0.879 Alter:1 -0.042804 0.032045 -1.336 0.182 Alter:2 -0.015986 0.032713 -0.489 0.625 Alter:3 -0.037991 0.036388 -1.044 0.296 Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1]), log(mu[,4]/mu[,1]) Residual deviance: 992.3763 on 1341 degrees of freedom Log-likelihood: -496.1882 on 1341 degrees of freedom Number of Fisher scoring iterations: 5 No Hauck-Donner effect found in any of the estimates Reference group is level 1 of the response Die Referenzkategorie wurde zuvor als keine Komorbiditäten definiert. Wir erhalten drei Intercepts, drei \\(\\beta\\)-Koeffizienten für Geschlecht und drei für das Alter. Dabei wird die Reihenfolge der Faktorenstufen eingehalten. (Intercept):1, Geschlechtm:1 und Alter:1 beschreiben folglich mögliche Geschlechts- oder Alterseffekte auf die Wahrscheinlichkeit, eine Lungenerkrankungen zusätzlich zu der bereits bestehenden Krebserkrankungen zu haben (im Vergleich zu keiner Komorbidität). Äquivalent steht die Zahl 2 für Herzerkrankungen und 3 für Erkrankungen des Gehirns jeweils im Vergleich zu unserer Baseline (keine Komorbidität). Da keiner der \\(\\beta\\)-Koeffizienten signifikant von 0 abweicht, liegt kein Zusammenhang zwischen den unabhängigen Variablen und dem Auftreten einer Komorbidität vor. Wenn die abhängige Variable nominal skaliert mit mehr als zwei Ausprägungen ist, können wir das multinomiale Logit Modell verwenden. Falls nur zwei Ausprägungen vorliegen, ist die logistische Regression angemessener. Bei ordinalem Skalenniveau greifen wir auf das kumulative Logit Modell zurück. 9.5.4 Kumulatives Logit Modell In diesem Kapitel muss das VGAM Package installiert und geladen werden. Der Packagename wird in Großbuchstaben geschrieben. library(VGAM) Bei einem interessierenden Merkmal, welches geordnete Kategorien beinhaltet, haben wir ein ordinales Skalenniveau vorliegen und verwenden daher ein kumulatives Logit Modell. Ein anderer etwas unscharfer Begriff dafür ist Ordinales Regressionsmodell. Grundsätzlich gibt es verschiedene kumulative Logit Modelle wie das Adjacent Category Model oder das Proportional Odds Model. Wir werden uns im weiteren Verlauf auf letzteres konzentrieren. Wir möchten im weiteren Verlauf einen möglichen Zusammenhang zwischen dem subjektiv empfundenen Schmerz während der Behandlung und den unabhängigen Variablen Alter und Geschlecht untersuchen. Die Schmerzen wurden mithilfe eines Fragebogens erfasst, bei dem 1 für keine Schmerzen und 10 für unerträgliche Schmerzen steht. table(chemo$Schmerzen) 1 2 3 4 5 6 7 8 9 10 36 46 37 40 86 62 61 56 17 9 Wie im vorherigen Kapitel greifen wir auch hier auf die Funktion vglm() aus dem VGAM Package zurück. Die abhängige Variable wird auf die linke Seite der Tilde und die unabhängigen, getrennt von einem Pluszeichen, auf die rechte Seite geschrieben. Das family Argument wird beim Proportional Odds Model durch die Helferfunktion propodds() definiert. Die Zusammenfassung der Ergebnisse erhalten wir durch die Funktion summary(). reg_parallel &lt;- vglm( formula = Schmerzen ~ Alter + Geschlecht, data = chemo, family = propodds ) summary(reg_parallel) Call: vglm(formula = Schmerzen ~ Alter + Geschlecht, family = propodds, data = chemo) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 2.90034 0.87618 3.310 0.000932 *** (Intercept):2 1.95733 0.86692 2.258 0.023958 * (Intercept):3 1.47788 0.86452 1.709 0.087361 . (Intercept):4 1.05810 0.86311 1.226 0.220232 (Intercept):5 0.27075 0.86174 0.314 0.753379 (Intercept):6 -0.31889 0.86184 -0.370 0.711376 (Intercept):7 -1.05919 0.86392 -1.226 0.220186 (Intercept):8 -2.34958 0.87822 -2.675 0.007465 ** (Intercept):9 -3.44873 0.91845 -3.755 0.000173 *** Alter -0.01246 0.01920 -0.649 0.516203 Geschlechtm 0.22298 0.16561 1.346 0.178152 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 9 Names of linear predictors: logitlink(P[Y&gt;=2]), logitlink(P[Y&gt;=3]), logitlink(P[Y&gt;=4]), logitlink(P[Y&gt;=5]), logitlink(P[Y&gt;=6]), logitlink(P[Y&gt;=7]), logitlink(P[Y&gt;=8]), logitlink(P[Y&gt;=9]), logitlink(P[Y&gt;=10]) Residual deviance: 1957.469 on 4039 degrees of freedom Log-likelihood: -978.7347 on 4039 degrees of freedom Number of Fisher scoring iterations: 4 No Hauck-Donner effect found in any of the estimates Exponentiated coefficients: Alter Geschlechtm 0.987615 1.249799 Für jeden Logit wird ein Intercept ausgegeben. Wir erhalten über alle Logits einen \\(\\beta\\)-Koeffizienten für Alter und einen für Geschlecht. Beide zeigen in diesem Fall keinen signifikanten Zusammenhang mit den einzelnen Logits. In der Praxis sollte immer die sogenannte Proportional Odds Assumption überprüft werden. Schließlich wissen wir im vornherein nicht, ob die Annahme der gleichen Effekte der Kovariate über alle Kategorien zutrifft. Hierfür stellen wir ein Partial Proportional Odds Model auf, in dem wir für jede Kategorie einen eigenen Effekt schätzen. Hierfür benötigen wir für das family Argument die cumulative() Funktion, in der wir das Argument auf FALSE setzen. Getrennt von einer Tilde wird das partielle Modell definiert. In diesem Fall exkludieren wir den Intercept (-1) und schätzen die Effekte für beide Kovariaten (Alter und Geschlecht). reg_notparallel &lt;- vglm( formula = Schmerzen ~ Alter + Geschlecht, data = chemo, family = cumulative(parallel = FALSE ~ -1 + Alter + Geschlecht, reverse = TRUE) ) Anschließend vergleichen wir beide Modelle mithilfe des Likelihood Ratio Tests, welcher durch die Funktion lrtest() ebenfalls im VGAM Package enthalten ist. Die Argumente sind hierbei lediglich beide Modelle. lrtest(reg_parallel, reg_notparallel) Likelihood ratio test Model 1: Schmerzen ~ Alter + Geschlecht Model 2: Schmerzen ~ Alter + Geschlecht #Df LogLik Df Chisq Pr(&gt;Chisq) 1 4039 -978.73 2 4031 -978.90 -8 0.334 1 Da wir einen p-Wert von 1 erhalten, welcher größer als 0.05 ist, können wir von zutreffender Proportional Odds Annahme ausgehen. Eine andere häufig verwendete Funktion ist polr() aus dem MASS Package, welche Ergebnisse mit umgekehrten Vorzeichen im Vergleich zu der hier kennengelernten Funktion ausgibt. Dies liegt daran, dass in VGAM die Logits als \\(logit[P(Y \\leq j)] = \\alpha_j + \\beta x\\) definiert sind und im MASS Package als \\(logit[P(Y \\leq j)] = \\alpha_j - \\beta x\\) mit \\(j =1, \\dots ,J −1\\). Falls die geordneten Kategorien des interessierenden Merkmals viele Unterstufen haben, ist möglicherweise das Schätzen eines linearen Modell möglich, welches einfacher zu interpretieren ist. In der wissenschaftlichen Praxis werden vor allem im Kontext von Fragebogenstudien häufig Verfahren für intervallskalierte Merkmale angewandt, obwohl Messungen durch einen Fragebogen im Regelfall nur ordinales Skalenniveau vorweisen. 9.5.5 Poisson Regression Für dieses Kapitel sind die Installation und das Laden des MASS Packages notwendig. library(MASS) Von sogenannten Zähldaten spricht man normalerweise im Kontext von Häufigkeiten. Die klassische Verteilung für Häufigkeiten ist die Poisson Verteilung, welche den Mittelwert gleich der Varianz postuliert. Dies ist in der Realität dennoch selten der Fall, weil die Varianz häufig bei höherer Häufigkeit steigt. Wir werden im weiteren Verlauf dieses Kapitels zwei mögliche Lösungen für das Problem kennenlernen. Um den Einfluss der Behandlungsarten auf das Immunsystem zu beurteilen, wurde im chemo Datensatz ebenfalls die Infektionshäufigkeit innerhalb von zwei Jahren erhoben. Da die Behandlungsarten mehr als zwei nominale Ausprägungen haben, müssen wir diese als Faktor übergeben (wurde bereits erstellt) (siehe Kapitel 6.10). Die Referenz ist die Radiochemotherapie. Zusätzlich möchten wir noch etwaige Alterseffekte auf die Infektionshäufigkeit ausschließen. Als family Argument verwenden wir poisson (ohne Anführungszeichen). Mit der Funktion summary() werden die Ergebnisse übersichtlich zusammengefasst. reg_res7 &lt;- glm(Infektionen ~ Behandlung + Alter, data = chemo, family = poisson) summary(reg_res7) Call: glm(formula = Infektionen ~ Behandlung + Alter, family = poisson, data = chemo) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.671640 0.331482 2.026 0.0427 * BehandlungChemo 0.161160 0.078867 2.043 0.0410 * BehandlungRadio 0.229135 0.077726 2.948 0.0032 ** Alter 0.000258 0.007307 0.035 0.9718 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 874.75 on 449 degrees of freedom Residual deviance: 865.62 on 446 degrees of freedom AIC: 1862.6 Number of Fisher Scoring iterations: 5 Im Vergleich zur Radiochemotherapie haben PatientInnen unter alleiniger Chemotherapie oder Radiotherapie mehr Infektionen innerhalb von 2 Jahren. Dies erkennen wir an den positiven \\(\\beta\\)-Koeffizienten, die statistisch signifikant von 0 abweichen, weil die p-Werte mit 0.0410 und 0.0032 kleiner als das übliche \\(\\alpha\\)-Niveau von 0.05 sind. Das Alter scheint hingegen keinen relevanten Zusammenhang mit der Infektionshäufigkeit zu haben. Als Effektstärke berechnen wir in diesem Kontext die sogenannte Incidence Rate Ratio, welche wir durch Exponieren der \\(\\beta\\)-Koeffizienten erhalten. Die \\(\\beta\\)-Koeffizienten extrahieren wir anhand des Dollar-Operators aus dem Modell. exp(reg_res7$coefficients) (Intercept) BehandlungChemo BehandlungRadio Alter 1.957445 1.174873 1.257512 1.000258 Der Dispersionsparameter ist hier als 1 festgelegt, da in der Poisson Verteilung der Mittelwert gleich der Varianz postuliert wird. Wie bereits angesprochen, ist die aus den Daten geschätzte Varianz häufig höher als durch die Poisson Verteilung angenommen. Man redet in diesem Kontext auch von einer sogenannten Überdispersion (engl. over-dispersion). Eine Möglichkeit zur Korrektur ist die Verwendung der Quasipoisson Verteilung. Ein Nachteil hierbei ist, dass Quasi-Likelihood Methoden keine Möglichkeit zur Berechnung von Informationskriterien bieten. reg_res8 &lt;- glm(Infektionen ~ Behandlung + Alter, data = chemo, family = quasipoisson) summary(reg_res8) Call: glm(formula = Infektionen ~ Behandlung + Alter, family = quasipoisson, data = chemo) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.671640 0.453395 1.481 0.1392 BehandlungChemo 0.161160 0.107873 1.494 0.1359 BehandlungRadio 0.229135 0.106312 2.155 0.0317 * Alter 0.000258 0.009995 0.026 0.9794 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 1.870828) Null deviance: 874.75 on 449 degrees of freedom Residual deviance: 865.62 on 446 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 5 Im Vergleich zur vorherigen Poisson Regression stellen wir bei den Ergebnissen hier einen entscheidenden Unterschied fest. Nur die Radiotherapie hat im Vergleich zur Radiochemotherapie noch einen nennenswerten positiven Zusammenhang mit der Infektionshäufigkeit, die Chemotherapie hingegen nicht mehr. In den Daten scheint folglich eine Überdispersion vorzulegen, worauf auch der auf 1.87 geschätzte Dispersionsparameter hindeutet. Eine weitere Alternative ohne die Einschränkungen der Quasipoisson Verteilung bei bestehender Überdispersion ist die sogenannte negative Binomial Verteilung. Diese ist in Form der glm.nb() im MASS Package implementiert. Bei der Verwendung bleibt abgesehen vom Wegfall des family Arguments alles gleich. reg_res9 &lt;- glm.nb(Infektionen ~ Behandlung + Alter, data = chemo) summary(reg_res9) Call: glm.nb(formula = Infektionen ~ Behandlung + Alter, data = chemo, init.theta = 2.592130972, link = log) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.6602195 0.4533734 1.456 0.1453 BehandlungChemo 0.1611161 0.1065635 1.512 0.1306 BehandlungRadio 0.2292369 0.1057676 2.167 0.0302 * Alter 0.0005127 0.0100032 0.051 0.9591 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(2.5921) family taken to be 1) Null deviance: 516.19 on 449 degrees of freedom Residual deviance: 511.28 on 446 degrees of freedom AIC: 1767.4 Number of Fisher Scoring iterations: 1 Theta: 2.592 Std. Err.: 0.408 2 x log-likelihood: -1757.383 Das Ergebnis ist hier sehr ähnlich des Regressionsmodell unter Annahme einer Quasipoisson Verteilung. Hier wird die Überdispersion mit 2.59 sogar noch höher geschätzt. Für eine genaue Beschreibungen der Ausgabe von Regressionsmodellen in R und deren Voraussetzungen sei auch an dieser Stelle auf Kapitel 9.5.1 verwiesen. Beim Laden des MASS Packages für die negative Binomial Regression wird select() aus dem tidyverse maskiert, weswegen die Reihenfolge beim Laden der beiden Packages relevant ist. Andernfalls hat die kennengelernte Funktion select() aus dem tidyverse nicht mehr die gewohnte Funktionalität. Lade also zunächst MASS und anschließend das tidyverse. Für mehr Informationen über den dafür verantwortlichen Namespace, schaue dir Kapitel 2.5.3 erneut an. Führe im Zweifel einen Neustart von RStudio durch und lade die beiden Packages nun in der für uns richtigen Reihenfolge. Die klassische Verteilung für Zähldaten ist die Poisson Verteilung. Man sollte diese aber wegen häufig auftretender Überdispersion immer mit den Ergebnissen der Regression unter Annahme einer Quasipoisson oder negativen Binomial Verteilung vergleichen. Falls Häufigkeit nicht als Spalte sondern in Form von Kontingenztafeln vorliegen, muss man stattdessen auf den Exakten Fisher Test oder den \\(\\chi^2\\) Test zurückgreifen (siehe Kapitel 9.6). 9.5.6 Cox Regression In diesem Kapitel muss das survival Package installiert und geladen werden. library(survival) Wir haben in Kapitel 8.13.1 bereits gelernt, wie Überlebenszeiten in Form von Kaplan-Meier Kurven graphisch dargestellt werden können. In diesem Kapitel beschäftigen wir uns mit der statistischen Modellierung durch die Cox Regression (engl. Cox Proportional-Hazards Model) und den Log Rank Test. Uns interessiert die Frage, ob die drei Behandlungsmethoden und das Geschlecht einen Einfluss auf die Überlebenszeit haben. Im chemo Datensatz beinhaltet die Spalte Beob_zeit die Beobachtungszeit innerhalb eines Zeitraums von 15 Jahren und Status die Information über das Versterben oder sonstige Ausscheiden aus dieser Studie. Eine 0 steht in dieser Spalte für ein sogenanntes zensiertes Ereignis. Der Patient oder die Patientin ist also aus irgendeinem Grund aus der Studie ausgeschieden (z.B. durch Umzug in ein anderes Land). Mit 1 ist der jeweilige Todesfall dokumentiert. Grundsätzlich wäre es außerdem möglich zwei Ereignisse miteinander zu vergleichen, wobei das zweite Ereignis dann entsprechend als 2 kodiert werden muss. Es ist wichtig, dass die Spalte mit den Informationen über die Ereignisse immer genau diese Kodierung hat. Funktionen zur Umkodierung haben wir bereits in Kapitel 6.4.3 kennengelernt. # A tibble: 450 × 4 Beob_zeit Status Behandlung Geschlecht &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; 1 0.833 1 Radio f 2 4.80 1 Radiochemo f 3 1.46 1 Radio m 4 0.922 1 Radio f # ℹ 446 more rows Im Kontext von Überlebenszeitanalysen interessiert uns häufig zunächst das Überleben nach einer bestimmten Zeit. Hierfür übergeben wir die abhängigen Variablen (Beob_zeit und Status) der Helferfunktion Surv() auf der linken Seite der Tilde und eine 1 auf der rechten Seite. Schließlich sind wir zunächst am gesamten Überleben ohne Unterteilung interessiert. zeit &lt;- survfit( formula = Surv(time = Beob_zeit, event = Status) ~ 1, data = chemo ) zeit Call: survfit(formula = Surv(time = Beob_zeit, event = Status) ~ 1, data = chemo) n events median 0.95LCL 0.95UCL [1,] 450 407 1.99 1.82 2.21 Das mediane Überleben liegt bei dieser Erkrankung bei knapp 2 Jahren. Für die Überlebenswahrscheinlichkeit nach 5 Jahren übergeben wir die Variable zeit an die Funktion summary(). Durch das times Argument kann die interessierende Überlebenszeit festgelegt werden (hier 5 Jahre). summary(zeit, times = 5) Call: survfit(formula = Surv(time = Beob_zeit, event = Status) ~ 1, data = chemo) time n.risk n.event survival std.err lower 95% CI upper 95% CI 5 47 361 0.129 0.017 0.0993 0.167 Nur knapp 13% überleben die ersten fünf Jahre nach Diagnosestellung. Als nächsten Schritt beantworten wir die Frage nach möglichem Einfluss der unabhängigen Variablen Behandlungsart und Geschlecht, welche wir ebenfalls auf die rechte Seite der Tilde schreiben. Die Cox Regression ist mit der Funktion coxph() aus dem survival Package in R verfügbar. Falls eine Modellierung in Abhängigkeit der Start- und Endzeit gewünscht ist, kann die Startzeit dem time und die Endzeit dem time2 Argument übergeben werden. Die Zusammenfassung der Ergebnisse erfolgt durch summary(). reg_res10 &lt;- coxph( formula = Surv(time = Beob_zeit, event = Status) ~ Behandlung + Geschlecht, data = chemo ) summary(reg_res10) Call: coxph(formula = Surv(time = Beob_zeit, event = Status) ~ Behandlung + Geschlecht, data = chemo) n= 450, number of events= 407 coef exp(coef) se(coef) z Pr(&gt;|z|) BehandlungChemo 1.1703 3.2230 0.1364 8.577 &lt;2e-16 *** BehandlungRadio 2.2387 9.3816 0.1558 14.372 &lt;2e-16 *** Geschlechtm 0.8596 2.3622 0.1041 8.260 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 exp(coef) exp(-coef) lower .95 upper .95 BehandlungChemo 3.223 0.3103 2.467 4.211 BehandlungRadio 9.382 0.1066 6.913 12.731 Geschlechtm 2.362 0.4233 1.926 2.897 Concordance= 0.732 (se = 0.012 ) Likelihood ratio test= 261.1 on 3 df, p=&lt;2e-16 Wald test = 241.1 on 3 df, p=&lt;2e-16 Score (logrank) test = 275.7 on 3 df, p=&lt;2e-16 Oben in der Ausgabe erfahren wir, dass nach 15 Jahren nur 43 der 450 Behandelten, die bis zum Ende Teil der Studie waren, noch Leben. Anschließend werden die \\(\\beta\\)-Koeffizienten (coef), Hazards Ratios (exp(coef)), Standardfehler (se), z-Wert und p-Wert. Die Kovariate Behandlung ist bereits von vornherein als Faktor definiert worden, da hier drei Gruppen vorliegen. Dabei ist die Referenzgruppe die Radiochemotherapie. Wir erhalten also die Informationen, dass unter der Behandlung mit alleiniger Chemotherapie dreimal mehr PatientInnen sterben, mit alleiniger Radiotherapie sogar neunmal mehr. In der Kovariate Geschlecht sehen wir am Ende des Wortes den Buchstaben m. Die Referenzkategorie ist hier folglich das weibliche Geschlecht (f). Männer haben in unserer Stichprobe mehr als die doppelte Gefahr, an der Erkrankung zu sterben. Alle drei \\(\\beta\\)-Koeffizienten weichen statistisch signifikant von 0 ab, da die p-Werte mit 2.2e-16 (\\(2.2*10^{-16}\\)) deutlich kleiner als das übliche Signifikanzniveau von 5% ist. Anschließend werden die Harzard Ratios diesmal mit 95% Konfidenzintervall gezeigt. Zuletzt werden die Ergebnisse der drei üblichsten Likelihood Tests für die globale Nullhypothese, dass alle \\(\\beta\\)-Koeffizienten gleich 0 sind, angegeben. Die Alternativhypothese stellt hier das Abweichen mindestens eines \\(\\beta\\)-Koeffizienten von 0 dar. Der sogenannte Log-Rank Test stellt eine Alternative dar, falls nur zwei Gruppen in Bezug auf Unterschiede der Überlebenszeit miteinander verglichen werden sollen. Wir könnten bspw. nur an Geschlechtsunterschieden hinsichtlich des Überlebens interessiert sein. In Abbildung 9.1 lassen die Kaplan-Maier Kurven bereits einen Zusammenhang zwischen Überleben und Geschlecht vermuten. Für Männer scheint die Prognose schlechter zu sein. Abbildung 9.1: Kaplan-Meier-Kurve unterteilt nach biologischem Geschlecht. Den Log Rank Test berechnen wir mit der Funktion survdiff() aus dem survival Package. Ansonsten ändert sich im Vergleich zur Cox Regression nichts. survdiff( formula = Surv(time = Beob_zeit, event = Status) ~ Geschlecht, data = chemo ) Call: survdiff(formula = Surv(time = Beob_zeit, event = Status) ~ Geschlecht, data = chemo) N Observed Expected (O-E)^2/E (O-E)^2/V Geschlecht=f 235 205 265 13.5 40.5 Geschlecht=m 215 202 142 25.3 40.5 Chisq= 40.5 on 1 degrees of freedom, p= 2e-10 Auch hier finden wir einen statistisch signifikanten Zusammenhang zwischen dem Geschlecht und der Überlebenszeit. Zusätzlich werden die beobachteten und erwarteten Häufigkeiten ausgegeben, die zur Berechnung miteinander verglichen wurden. 9.6 Kontingenztafeln 9.6.1 Exakter Fisher Test Zur Berechnung der Effektstärken muss das effectsize Package installiert und geladen werden. library(effectsize) Die Erstellung von Kontingenztafeln wurde bereits in Kapitel 7.2 eingeführt (Synonym: Kreuztabellen). Durch Erstellung dieser Kontingenztafeln können wir die Unabhängigkeit zweier kategoreale Merkmale systematisch überprüfen. Wir möchten wissen, ob ein stationärer Aufenthalt aufgrund irgendeiner Erkrankung abhängig vom biologischen Geschlecht ist. Zunächst müssen wir dafür die Kontingenztafel erstellen. tbl1 &lt;- table(chemo$Geschlecht, chemo$Stationaer) tbl1 0 1 f 216 19 m 194 21 Im Kontext von \\(2 \\times 2\\) Tabellen und relativ kleinen Stichproben verwenden wir den exakten Test nach Fisher. Es gibt eine umstrittene Daumenregel, die besagt, dass dieser Test nur bei einer Stichprobengröße kleiner als 1000 mit mindestens einer Zelle mit einer Häufigkeit von weniger als fünf verwendet wird. Ansonsten wird der im folgenden Kapitel erklärte \\(\\chi^2\\) Test nach Pearson empfohlen. Heutige Computer können den exakten Fisher Test zwar auch problemlos bei größeren Stichproben berechnen, allerdings wird im Regelfall schon bei moderater Stichprobengröße eine \\(\\chi^2\\) Verteilung approximiert. Die Funktion innerhalb von R heißt fisher.test(), welcher wir lediglich die Kontingenztafel übergeben müssen. Ein großer Vorteil des exakten Fisher Tests ist die Möglichkeit des gerichteten Hypothesentestens. Anstelle der Abhängigkeit beider Merkmale könnten wir uns also auch fragen, ob der stationäre Aufenthalt häufiger bei Männern oder weniger häufig ist. Dafür müssten wir das Argument alternative von \"two.sided\" auf \"less\" oder \"greater\" ändern. fisher.test(tbl1, alternative = &quot;two.sided&quot;) Fisher&#39;s Exact Test for Count Data data: tbl1 p-value = 0.6195 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.6091001 2.4985032 sample estimates: odds ratio 1.23003 Als Ausgabe erhalten wir die Odds Ratio mit 95% Konfidenzintervall und einen p-Wert. In unserem Beispiel ist dieser mit 0.6195 größer als das übliche \\(\\alpha\\)-Niveau von 0.05 und somit statistisch nicht signifikant. Es findet sich folglich keine Abhängigkeit zwischen dem Krankenhausaufenthalt und dem biologischen Geschlecht wieder. Effektstärken erhalten wir mit Funktionen aus dem effectsize Package. Im Falle einer \\(2 \\times 2\\) Tafel ist \\(\\phi\\) als Zusammenhangsmaß empfohlen, welches durch die Funktion phi() implementiert ist. phi(tbl1, adjust = FALSE) Phi | 95% CI ------------------- 0.03 | [0.00, 1.00] - One-sided CIs: upper bound fixed at [1.00]. Der marginale Effekt nahe Null passt zu unserem nicht-signifikanten Hypothesentest. Alternativ könnten wir bspw. auch Pearson’s C mit der Funktion pearsons_c() berechnen. Das relative Risiko erhalten wir durch riskratio() ebenfalls aus dem effectsize Package. riskratio(tbl1) Risk ratio | 95% CI ------------------------- 1.11 | [0.75, 1.65] 9.6.2 Chi Quadrat Tests Für dieses Kapitel muss das effectsize Package installiert und geladen sein. library(effectsize) Bei größeren Stichproben verwenden wir zum Vergleich zweier kategorealer Merkmale den \\(\\chi^2\\) Test nach Pearson. Auch hier wird eine mögliche Abhängigkeit bzw. ein Zusammenhang zwischen den Merkmalen durch die Alternativhypothese überprüft. Der Vorteil des \\(\\chi^2\\) Tests ist die Anwendung auch auf größeren Tafeln als \\(2 \\times 2\\), der Nachteil ist die fehlende Möglichkeit gerichtete Hypothesen zu überprüfen. Der \\(\\chi^2\\) Test nach Pearson ist in R in Form der Funktion chisq.test() enthalten. Zunächst möchten wir einen möglichen Zusammenhang zwischen dem stationären Aufenthalt im Krankenhaus und dem biologischen Geschlecht untersuchen. Dafür nehmen wir die im vorherigen Kapitel erstelle Kontingenztafel namens tbl1. chisq.test(tbl1) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: tbl1 X-squared = 0.21214, df = 1, p-value = 0.6451 Das Ergebnis ist in diesem Fall fast identisch mit dem des exakten Fisher Tests (siehe Kapitel 9.6.1). Da die Funktion hier nicht direkt die Odds Ratio mit ausgibt, können wir diese mithilfe der Funktion oddsratio() aus dem effectsize Package berechnen. oddsratio(tbl1) Odds ratio | 95% CI ------------------------- 1.23 | [0.64, 2.36] Wir können mit dem \\(\\chi^2\\) Test auch zwei kategoreale Merkmale mit mehr als zwei Ausprägungen untersuchen. Uns interessiert an dieser Stelle, ob ein stationäre Aufenthalt von der Behandlungsart abhängt. Dafür erstellen wir uns zuerst die zugehörige Kontingenztafel (siehe Kapitel 7.2). tbl2 &lt;- table(chemo$Behandlung, chemo$Stationaer) tbl2 0 1 Radiochemo 144 7 Chemo 139 11 Radio 127 22 Anschließend verwenden wir, wie zuvor, die Funktion chisq.test() mit der Kreuztabelle als alleiniges Argument. chisq.test(tbl2) Pearson&#39;s Chi-squared test data: tbl2 X-squared = 10.174, df = 2, p-value = 0.006178 Zurückgegeben wird die Teststatistik in Form des \\(\\chi^2\\)-Wertes, die Freiheitsgrade (df) und der p-Wert. In diesem Fall liegt ein Zusammenhang zwischen dem stationären Aufenthalt und der Behandlungsform vor, da der p-Wert mit 0.006178 kleiner als 0.05 ist. Welche Behandlung häufiger zu einem stationären Aufenthalt führt, vermögen wir aufgrund des \\(\\chi^2\\) Tests nicht zu sagen. Eine geeignete Effektstärke für Tafeln, die größer als \\(2 \\times 2\\) sind, bietet Cramers V (cramers_v) aus dem effectsize Package. cramers_v(tbl2) Cramer&#39;s V (adj.) | 95% CI -------------------------------- 0.13 | [0.00, 1.00] - One-sided CIs: upper bound fixed at [1.00]. Bei abhängigen (gepaarten) Merkmalen ist der \\(\\chi^2\\) Test nach McNemar die Methode der Wahl. Ein häufiger Anwendungsfall ist daher der Kontext einer Messung zu zwei verschiedenen Zeitpunkten oder mittels zwei verschiedenen Verfahren. Hier schauen wir uns an, ob die Qualität zwischen PCR-Test und Antigen-Schnelltest bei der Diagnostik des SARS-CoV-2 Virus nennenswert unterschiedlich ist. Der zugehörige Datensatz aus dem remp heißt covid. covid Schnelltest PCR Positiv Negativ Positiv 35 14 Negativ 8 2421 Anschließend übergeben wir die Kontingenztafel der Funktion mcnemar.test(). mcnemar.test(covid) McNemar&#39;s Chi-squared test with continuity correction data: covid McNemar&#39;s chi-squared = 1.1364, df = 1, p-value = 0.2864 Da der p-Wert mit 0.2864 in diesem Fall größer als das in der Wissenschaft übliche \\(\\alpha\\)-Niveau von 0.05 ist, nehmen wir die Nullhypothese an. Es besteht also kein nennenswerter Unterschied zwischen beiden diagnostischen Verfahren, auch wenn der Antigen-Schnelltest offensichtlich 8 falsch positive und 14 falsch negative Ergebnisse produziert hat. Bei beiden Verfahren können wir mithilfe des correct Arguments die Korrekturen ausschalten. Eine adäquate Effektstärke wäre in diesem Fall Cohens g, welche ebenfalls im effectsize Package implementiert ist. cohens_g(covid) Cohen&#39;s g | 95% CI ------------------------- 0.14 | [-0.07, 0.30] Durch die Quadrierung geht bei \\(\\chi^2\\) Tests jede Richtung verloren. Als Konsequenz können ausschließlich zweiseitige Hypothesen getestet werden. Bei kleinen bis moderaten Stichproben in Form von \\(2 \\times 2\\) Tafeln bietet der exakte Fisher Test eine Alternative für gerichtete Hypothesen. 9.7 Voraussetzungen überprüfen 9.7.1 Normalverteilung In diesem Kapitel wird das Package broom benötigt. library(broom) Eine der gängigsten statistischen Methoden zur Überprüfung des Vorliegens einer Normalverteilung ist der Shapiro-Wilk Test. Dieser ist in Form der Funktion shapiro.test() direkt in R integriert. Als Argument übergeben wir die entsprechende Variable als Vektor (bzw. Wertereihe) mithilfe des Dollar-Operators (siehe Kapitel 4.5). Die Leukozytenanzahl nach sechs Monaten Behandlung wurde in diversen Kapitels dieses Buches als normalverteilt angenommen. Nun können wir überprüfen, ob diese Annahme zutreffen war. shapiro.test(chemo$Leukos_t6) Shapiro-Wilk normality test data: chemo$Leukos_t6 W = 0.99747, p-value = 0.7284 Der p-Wert ist mit 0.7284 größer als das gängige \\(\\alpha\\)-Niveau von 0.05, weswegen wir die Nullhypothese annehmen und von normalverteilten Daten ausgehen können. Ein weiterer Anwendungsfall ist die Überprüfung der Normalverteilung der Residuen eines Regressionsmodells. Die Berechnung der Residuen erfolgt durch Funktion des broom Packages und wird in Kapitel 9.8 eingeführt. An dieser Stelle genügt uns das Wissen, dass diese im tibble namens resid_df abgespeichert werden, welcher unter anderem die Spalte .resid enthält. model1 &lt;- lm(Leukos_t6 ~ Alter + Geschlecht, data = chemo) resid_df &lt;- augment(model1) Anschließend übergeben wir äquivalent zu vorher die Spalte .resid der Funktion des Shapiro Wilk Tests. Auch hier können wir von einer Normalverteilung ausgehen, weil der p-Wert in Höhe von 0.6904 größer als 0.05 ist. shapiro.test(resid_df$.resid) Shapiro-Wilk normality test data: resid_df$.resid W = 0.99734, p-value = 0.6904 Eine Alternative der Hypothesentests zur Überprüfung der Normalverteilung stellen graphische Vergleiche der Verteilungen mithilfe eines Quantil-Quantil Plots dar. In Kapitel 8.8 haben wir bereits die Darstellung eines Q-Q Plots für intervallskalierte Variablen gesehen. Auch Residuen wurden bereits mit der Funktion autoplot() unter anderem auf Normalverteilung überprüft (siehe Kapitel 8.13.2). In Abbildung 9.2 werden die Quantile der standardisierten Residuen (y-Achse) mit den theoretischen Quantilen der Normalverteilung durch autoplot() verglichen. Solange die Werte auf der Diagonalen liegen, kann von einer Normalverteilung ausgegangen werden. autoplot(model1, which = 2, label.repel = TRUE) Abbildung 9.2: Graphische Überprüfungen der Normalverteilung der Residuen Wie man die Residuen der Regressionsanalyse berechnet, wird in Kapitel 9.8 erläutert, sodass du diese Q-Q Plots auch manuell ohne autoplot() erstellen kannst. Die meisten kennengelernten Verfahren sind relativ robust gegenüber leichteren Verletzungen der Annahme der Normalverteilung. Hypothesentests wie der Shapiro-Wilk Test sind in solchen Szenarien sehr sensibel gegenüber Verletzungen und verwerfen daher unter Umständen zu häufig die Nullhypothese des Vorliegen einer Normalverteilung. Daher sollte in jedem Fall eine graphische Überprüfung mittels Q-Q Plots erfolgen. 9.7.2 Varianzhomogenität und Homoskedastizität Für dieses Kapitel muss das car Package installiert und geladen werden. library(car) Viele parametrische statistische Verfahren können nur bei gegebener Varianzhomogenität fehlerfrei geschätzt werden. Zur Überprüfung der Varianzgleichheit zweier Gruppen verwenden wir den F Test, welcher durch die Funktion var.test() in R implementiert ist. An dieser Stelle möchten wir die Varianz der Leukozytenanzahl nach sechs Monaten Therapie zwischen zwei biologischen Geschlechtern vergleichen. Dafür wird die intervallskalierte Variable auf die linke Seite der Tilde und die nominale auf die rechte Seite geschrieben. Beachte, dass nur zwei Gruppen verglichen werden können. Schließlich werden die beiden Varianzen direkt miteinander ins Verhältnis gesetzt. Ein F-Wert von 1 entspricht demnach einer Varianzhomogenität (Nullhypothese). var.test(Leukos_t6 ~ Geschlecht, data = chemo) F test to compare two variances data: Leukos_t6 by Geschlecht F = 0.85053, num df = 234, denom df = 214, p-value = 0.2256 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.6534313 1.1053574 sample estimates: ratio of variances 0.8505318 Als Ergebnis erhalten wir den F Wert, die Freiheitsgrade, p-Wert und das 95% Konfidenzintervall. Da in diesem Fall die kleiner Varianz der Frauen (0.101) durch die der Männer (0.119) geteilt wird, erhalten wir einen F-Wert kleiner 1. Welche Gruppe im Zähler und welche im Nenner steht, kann explizit durch Umwandlung in einen Faktor kontrolliert werden (siehe Kapitel 6.10). Der F-Wert ist zwar kleiner als 1, allerdings nicht statistisch signifikant, da der p-Wert in Höhe von 0.2256 größer als das häufig gewählte \\(\\alpha\\)-Niveau von 0.05 ist. Die Nullhypothese der Varianzhomogenität wird demnach angenommen. Beim Vergleich der Varianz von drei oder mehr Gruppen können wir zwischen dem Levene Test und Bartlett Test wählen. Für den Levene Test verwenden wir die Funktion leveneTest() aus dem car Package. Beachte an dieser Stelle, die abweichende Schreibweise der Funktion mit einem Großbuchstaben beim Wort Test. Wir möchten nun überprüfen, ob die Varianz der Leukozytenanzahl zwischen den Behandlungsarten gleich ist. Dafür übergeben wir auf der linken Seite der Tilde erneut die intervallskalierte Variable und auf der rechten Seite das nominalskalierte Merkmal. leveneTest(Leukos_t6 ~ Behandlung, data = chemo) Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 2 0.3454 0.7081 447 Wir erhalten auch hier die Teststatistik eines F-Wertes, die Freiheitsgrade und den p-Wert. Da der p-Wert mit 0.7081 bei eine \\(\\alpha\\)-Niveau von 5% nicht statistisch signifikant ist, nehmen wir auch hier die Nullhypothese der Varianzhomogenität an. Für die Durchführung des Bartlett Test ändert sich im Vergleich zum Levene Test nur der Funktionsaufruf zu bartlett.test(). bartlett.test(Leukos_t6 ~ Behandlung, data = chemo) Bartlett test of homogeneity of variances data: Leukos_t6 by Behandlung Bartlett&#39;s K-squared = 1.785, df = 2, p-value = 0.4096 Hier kommen wir ebenfalls zur Schlussfolgerung vorliegender Varianzhomogenität, da der p-Wert von 0.4096 ebenfalls nicht statistisch signifikant ist. Eine Voraussetzung für die Regressionsanalysen ist die Homoskedastizität, welche die Gleichheit der Varianz der Residuen über den gesamten Parameterraum beschreibt. Die beste Überprüfung erfolgt graphisch durch das Auftragen der Residuen gegen die vorhergesagten Werte. Homoskedastizität ist gegeben, solange die Werte gleichmäßig um die horizontale Nulllinie der y-Achse streuen. Die dafür verwendete Funktion autoplot() wurde bereits im Kapitel 8.13.2 vorgestellt. autoplot(model1, which = 1, label.repel = TRUE) Abbildung 9.3: Graphische Überprüfungen der Homoskedastizität der Residuen. Bei Überprüfung der Varianzhomogenität von zwei Gruppen greifen wir auf den klassischen F Test zurück. Bei nominalen Merkmalen mit drei oder mehr Ausprägungen sind sowohl der Levene Test als auch der Bartlett Test Methoden der ersten Wahl. Im Kontext von Regressionsanalysen kann die Varianzhomogenität der Residuen (Homoskedastizität) am besten graphisch überprüft werden (siehe Abbildung 9.3). 9.8 Ergebnisse formatieren Für das Umformatieren in Tabellenform muss das broom Package installiert und geladen sein. library(broom) Als erstes Beispiel verwenden wir das lineare Regressionsmodell aus Kapitel 9.5.1 und nennen die Ergebnisliste model1. model1 &lt;- lm(Leukos_t6 ~ Alter + Geschlecht, data = chemo) Die wichtigsten Informationen werden mit der Funktion tidy() ausgegeben, welche als einziges Argument die Variable mit dem Modell benötigt. Die \\(\\beta\\)-Koeffizienten sind in der Spalte namens estimate. Die Spalte statistic bezieht sich auf die t-Werte basierend auf den z-Werten der Wald Tests. Die p-Werte werden leider in tibbles immer in wissenschaftlicher Notation dargestellt. Bei der Kovariate Alter ist der p-Wert 1.69e-2, also \\(1.69 * 10^{-2}\\), und entspricht somit der Zahl 0.0169. Äquivalent dazu ist der p-Wert für die Kovariate Geschlecht 0.102, weil dies eine andere Schreibweise für \\(1.02 * 10^{-1}\\) darstellt, welches in R als 1.02e-1 angezeigt wird. tidy(model1) # A tibble: 3 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 5.61 0.162 34.6 1.49e-128 2 Alter 0.00868 0.00362 2.40 1.69e- 2 3 Geschlechtm -0.0511 0.0312 -1.64 1.02e- 1 Zusätzliche Informationen wie die Varianzaufklärung (\\(R^2\\)) oder Informationskriterien in Form des AIC und BIC erhalten wir durch die Funktion glance(). Außerdem ist hier die Teststatistik (F-Wert) des globalen F-Tests als statistic mit zugehörigem p-Wert und Freiheitsgrad (df) ausgegeben. Auch hier muss nur das Modell als Argument übergeben werden. glance(model1) # A tibble: 1 × 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.0174 0.0130 0.329 3.95 0.0200 2 -137. 283. 299. 48.5 447 # ℹ 1 more variable: nobs &lt;int&gt; Die Funktion augment() gibt unter anderem die geschätzten (.fitted) Werte, Cooks Distanz (.cooksd) und die Residuen (.resid) zurück. augment(model1) # A tibble: 450 × 9 Leukos_t6 Alter Geschlecht .fitted .resid .hat .sigma .cooksd .std.resid &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5.68 49 f 6.04 -0.360 0.00676 0.329 0.00273 -1.10 2 6.64 46 f 6.01 0.626 0.00455 0.328 0.00553 1.91 3 5.97 41 m 5.92 0.0505 0.00667 0.330 0.0000530 0.154 4 5.55 43 f 5.99 -0.438 0.00451 0.329 0.00268 -1.33 # ℹ 446 more rows Wir betrachten an dieser Stelle noch ein weiteres Beispiel anhand der Ergebnisse des t-Tests aus Kapitel 9.2.1. Hier entspricht estimate der Mittelwertsdifferenz, estimate1 die mittlere Leukozytenzahl der Frauen und estimate2 die der Männer. In der Spalte statistic wird der t-Wert abgebildet und als parameter die Freiheitsgrade. Leider sind die Bezeichnungen nicht immer eindeutig, weswegen ein Vergleich mit der originalen Ausgabe empfohlen ist. model2 &lt;- t.test(Leukos_t6 ~ Geschlecht, data = chemo, var.equal = TRUE) tidy(model2) # A tibble: 1 × 10 estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.0455 6.00 5.95 1.46 0.146 448 -0.0159 0.107 Two Sample t-test # ℹ 1 more variable: alternative &lt;chr&gt; Die Funktion glance() gibt hier dasselbe Ergebnis zurück wie tidy() und augment() kann in diesem Fall nicht verwendet werden. Während tidy() mit Ausnahme der Varianzanalyse mit Messwiederholung für alle hier vorgestellten statistischen Modellen funktioniert, können glance() und augment() nicht immer angewendet werden. Die aktuelle Liste der unterstützten Funktionen kann in der Dokumentation des broom Packages eingesehen werden. Beachte, dass die genaue Ausgabe von tidy(), glance() und augment() je nach statistischem Modell unterschiedlich ist. So kann die Spalte estimate bei einem Modell für einen t-Wert stehen, während damit beim Betrachten einer Regression \\(\\beta\\)-Koeffizienten gemeint sind. Da wir die Ergebnisse nun als tibble in Tabellenform vorliegen haben, kannst du diese im nächsten Schritt in das Textverarbeitungsprogramm deiner Wahl exportieren. "],["ergebnisse-exportieren.html", "Kapitel 10 Ergebnisse exportieren 10.1 Wichtige Begriffe 10.2 Berichte erstellen 10.3 Tabellen umwandeln 10.4 Alternative mit Quarto", " Kapitel 10 Ergebnisse exportieren Nachdem alle Berechnungen mit R durchgeführt wurden, möchte man in der Regel die erstellten Tabellen und Abbildungen in das für die wissenschaftliche Arbeit verwendete Textverarbeitungsprogramm einfügen. Die Ergebnisse einfach aus der Konsole zu kopieren und bspw. in Word einzufügen sieht allerdings schlecht aus und benötigt nachträglich einige Formatierungen. Glücklicherweise gibt es zwei Erweiterungen, mit denen man die Ergebnisse aus R direkt in PDFs oder Word-Dokumente, Internetseiten oder LaTeX Code umwandeln kann. Auf diese Weise können nicht nur einzelne Tabellen, sondern ganze Paper oder Bücher erstellt werden. 10.1 Wichtige Begriffe Während der Datenanalyse werden alle Auswertungen zunächst nur innerhalb von RStudio angezeigt. Die Ergebnisse aus R in ein anderes Format zu bringen, hat vor allem zwei Vorteile: Die Auswertungen können auch mit KollegInnen und BetreuerInnen besprochen werden, die kein R installiert haben. Tabellen und Abbildungen müssen nicht erst in Word oder einem anderen Programm manuell erstellt werden. Bevor wir uns in den folgenden Kapiteln anschauen, wie das genau funktioniert, müssen wir zunächst einige Begrifflichkeiten klären. In R existiert eine Erweiterung namens R Markdown, welche die Ergebnisse aus R in Word, PDF oder Internetseiten umwandeln kann. HTML. Das sogenannte Hypertext Markup Language (HTML) Format ist das strukturelle Rückrad des Internets. Es gibt die Form von Internetseiten vor, die von Browsern wie Firefox, Chrome oder Brave dargestellt wird. Aber auch ohne Internet können HTML Dateien im Browser geöffnet werden. Dadurch ist es sehr praktisch zum gegenseitigen Teilen von Inhalten, da jeder und jede einen Browser auf dem Computer installiert hat. Markdown. Um das Erstellen von Inhalten in HTML Dokumenten einfacher zu gestalten, wurde Markdown erfunden. Es bietet eine deutlich einfachere Syntax zum Bearbeiten eines Textes und wird heute von vielen Programmen verwendet (z.B. Notion oder Obsidian). YAML. Damit nicht nur das Schreiben von Texten sondern auch die Konfigurationen leichter werden, gibt es YAML (Akronym für yet another markup language). Dabei werden verschiedene Optionen durch Einrücken voneinander abgegrenzt und sind so für das menschliche Auge gut lesbar. R Markdown. Die Funktionen von Markdown werden von R Markdown erweitert, um diese nicht nur für HTML Dateien sondern auch für Word- und PDF-Dokumente anzuwenden. So ist es möglich, eine R Markdown Datei zu verwenden, die mit derselben Basis in Word, PDF und HTML umgewandelt werden kann. Das rmarkdown Package ist dabei direkt in RStudio integriert. Es handelt sich bei R Markdown nicht um ein R Skript, sondern um einen eigenes Dateiformat, welches mit .Rmd und nicht mit .R endet. Quarto. Eine neuere Alternative zu R Markdown ist Quarto. Auch hier handelt es sich um ein eigenes Dateiformat, welches mit .qmd endet. Die Anwendung der beiden Formate ist grundsätzlich sehr ähnlich. Unterschiede und mögliche Vorteile werden in der folgenden Info-Box diskutiert. Pandoc. Die Umwandlung der Dateien findet hinter den Kulissen mithilfe von Pandoc statt. Da Pandoc seit der RStudio Version 1.3 vorinstalliert ist, muss du dir darum keine Gedanken mehr machen. Wer eine ältere Version auf dem Computer hat, muss Pandoc manuell unter https://pandoc.org/installing.html herunterladen. Für das Umwandeln von R Markdown Dateien in Word durch Pandoc ist außerdem eine Installation von Microsoft Office oder LibreOffice auf dem Computer notwendig. LaTeX. Das zuvor beschriebene Pandoc formiert die Datei in LaTeX um, welches wiederum für das Kreieren des PDFs verantwortlich ist. LaTeX wird in abgeänderter Form bereits seit Ende der 70er Jahre vor allem von NaturwissenschaftlerInnen für die Erstellung wissenschaftlicher Arbeiten verwendet wird. Es ist notwendig, dieses Programm auf dem Computer installiert zu haben, wenn man seine Analyse in ein PDF umwandeln möchte. Für R Nutzer ist die einfachste Installation über das tinytex Package. Dafür muss zuerst das Package installiert werden. Anschließend erfolgt mit install_tinytex() die Installation von LaTeX. install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() Alternativ kann auf Windows MiKTeX, auf macOS MacTeX und auf Linux LiveTeX installiert werden. Dies erfolgt unabhängig von R durch Befolgen der jeweiligen Installationsanweisungen auf der entsprechenden Internetseite. Das Unternehmen Posit (EntwicklerInnen von RStudio) hat eine Alternative zu R Markdown namens Quarto entwickelt. Der Aufbau von R Markdown und Quarto Dateien ist dabei sehr ähnlich, weswegen es kaum einen Unterschied macht, für welches Programm man sich zunächst entscheidet. R Markdown existiert deutlich länger, weswegen es diverse Erweiterungen und Dokumentation dafür gibt. Einige dieser Erweiterungen sind ohne Zusatzpakete direkt in Quarto integriert. Tendenziell sehen die Umwandlungen mit Quarto etwas moderner aus. Außerdem kann Quarto besser mit anderen Programmiersprachen wie Python oder Julia verwendet werden. Für welches dieser Formate man sich letzten Endes entscheidet, hängt von der persönlichen Präferenz und dem eigenen Anwendungsgebiet ab. 10.2 Berichte erstellen Für dieses Kapitel muss das knitr Package installiert und geladen werden. library(knitr) Öffne zum Erstellen von R Markdown Dokumenten (Endung .Rmd) das Dropdown-Menü mit dem Papier und dem grünen Pluszeichen unter dem Reiter File. Abbildung 10.1: Erster Schritt in der Erstellung eines neuen R Markdown Skripts. Wähle in diesem Reiter R Markdown aus. Die Punkte implizieren, dass noch weitere Informationen vor dem Erstellen notwendig sind. Anschließend kann man das gewünschte Ausgabeformat, den Titel und Autor festlegen. Beim Bestätigen auf OK wird eine Vorlage erstellt, die man nach Belieben anpassen kann. Abbildung 10.2: Zweiter Schritt zur Erstellung eines neuen R Markdown Skripts. Die wichtigsten Formatierungsmöglichkeiten in Markdown sehen wie folgt aus: Fett gedruckt: **fett** ergibt fett Kursiv: *kursiv* ergibt kursiv Code Integration im Text: `mean(x)` ergibt mean(x) Links: [Klicke hier](https://cran.r-project.org/) wird zu: Klicke hier (im gebundenen Buch sind Links nicht extra farblich hervorgehoben) Überschriften können mit einer führenden Raute (# Überschrift 1) erstellt werden. Für Abschnitte innerhalb des Hauptkapitels können beliebig viele Rauten hinzugefügt werden (z.B. ## Unterkapitel 1). Um das etwas abstrakte Konzept verständlicher zu machen, soll an dieser Stelle eine ganze R Markdown Datei namens Beispiel.Rmd abgebildet werden. Zu Beginn des Dokumentes befindet sich der YAML-Kopf, in dem verschiedene Optionen angepasst werden können (siehe Kapitel 10.1). Wir beschränken uns hier auf die Auswahl des Titels, des Autors, des Datums sowie einer optisch ansprechenden Abbildung der Tabellen (df_print: paged oder df_print: kable). Beachte im Kontext von YAML immer die richtige Einrückung der Optionen. Nach erfolgreicher Definition der Optionen können wir grau hinterlegte Code Blöcke mithilfe von drei Backticks (```) erstellen. Diese erlauben das Ausführen von R Funktionen. Den Backticks folgen geschweifte Klammern. Die erste Information ist die verwendete Programmiersprache (hier R). Weitere Einstellungen dieses Code Blocks werden in den Zeilen darunter mit einer Hashpipe (#|) festgelegt (z.B. #| label: setup). Die einzelnen Optionen werden mit einem Doppelpunkt und nicht mit einem Gleichheitszeichen übergeben. Im ersten Code Block laden wir alle Packages und Datensätze. Auch Vorberechnungen, die später nicht dargestellt werden sollen, würden an dieser Stelle ihren Platz finden. Durch die Option include: FALSE wird der Inhalt zwar ausgeführt, aber nicht angezeigt. Eine weitere Option zum Ausblenden des Codes ist das Argument echo, welches ebenfalls auf FALSE gesetzt werden kann. So können bspw. ausschließlich die Ergebnisse in Form von Tabellen ohne zugehörigen R Code angezeigt werden. – Beginn der Datei Beispiel.Rmd – --- title: &quot;Vorläufige Ergebnisse&quot; author: &quot;Student A&quot; date: &quot;2023-01-13&quot; output: html_document: df_print: paged pdf_document: df_print: kable word_document: df_print: kable --- ```{r} #| label: setup #| include: FALSE library(tidyverse) library(remp) library(knitr) options(dplyr.summarise.inform = FALSE) data(big5_mod2) ``` ## Deskriptive Statistik Verschiedene Lagemaße wie Minimum, 1. Quartil, Mittelwert, Median, 3. Quartil, Maximum, Standardabweichung und Standardfehler. ```{r} big5_mod2 |&gt; descriptive() ``` ## Visualisierung Ein Streudiagramm zur anschaulichen Darstellung des Zusammenhangs zwischen **Extraversion** und **Neurotizismus**. ```{r} #| echo: FALSE #| fig.cap: &quot;Streudiagramm zur Extraversion und Neurotizismus.&quot; ggplot(big5_mod2, aes(x = Extraversion, y = Neurotizismus)) + geom_point(position = &quot;jitter&quot;) ``` – Ende der Datei Beispiel.Rmd – Die Ausgabe nach Umwandlung in ein HTML Dokument ist in Abbildung 10.3 zu sehen. Abbildung 10.3: Umwandlung von Markdown in HTML. Jedes R Markdown Dokument hat die Endung .Rmd und beginnt mit einem so genannten YAML-Kopf, der durch drei Bindestriche oben und unten vom restlichen Dokument abgegrenzt ist. Fortgeschrittenere Anpassungsmöglichkeiten sind im Buch R Markdown - The Definitive Guide von Xie, Allaire und Grolemund beschrieben. Nützliche Optionen für die Code Abschnitte: Ausschalten von Benachrichtigungen und Warnungen mit message: FALSE respektive warning: FALSE. Ausblenden des zugehörigen R Codes mit echo: FALSE. R Code anzeigen, aber nicht auswerten mit eval: FALSE. Name des Chunks zur späteren Referenz mit label: Name. Einstellen der Dimensionen einer Abbildung mit fig.height: 5 und fig.width: 10. Beschriftung der Abbildung mit fig.cap: \"Dies ist eine Abbildung.\" Zentrieren der Abbildung mit fig.align: \"center\". Umwandeln können wir das R Markdown Dokument durch Klicken auf das Wort Knit (engl. für stricken) in der Leiste unter dem Reiter der geöffneten Dateien oder durch die Tastenkombination Strg / Cmd + Shift + K. Wenn man nur auf das Symbol drückt, wird die Datei in das an erster Stelle im YAML Kopf festgelegten Format umgewandelt. Besser ist es jedoch, das Dropdown-Menü durch einen Klick auf den Pfeil nach unten zu öffnen und den gewünschten Dateityp auszuwählen. Abbildung 10.4: Umwandlung des R Markdown Skripts in HTML, PDF oder Word. R Markdown Dateien können nur umgewandelt werden, wenn alle notwendigen Informationen enthalten sind. Es müssen also innerhalb der Datei alle Packages und Datensätze explizit geladen werden. Dies trifft auch zu, obwohl man die Packages oder Datensätze möglicherweise bereits vorher verwendet hat. Man kann jede R Markdown Datei als isoliert von allem anderen in RStudio geöffneten betrachten. RStudio bietet auch einen sogenannten Visual Editor zum Bearbeiten der R Markdown oder Quarto Datei an, welcher in einer Zeile unter den Dateinamen und unter dem Knit Button als Option per Mausklick ausgewählt werden kann. Dieser übersetzt die Markdown Inhalte direkt im Dokument, um einen besseren und übersichtlicheren Eindruck zu erhalten. Alternativ kannst du auch dedizierte Markdown Editoren wie Zettlr oder Typora verwenden, welche vor allem den Vorteil einer besseren Rechtschreibprüfung bieten. 10.3 Tabellen umwandeln 10.3.1 Exportieren nach Word und PDF Für die Tabellen benötigen wir für beide Formate das knitr Package. library(knitr) Wenn keine ganzen Berichte, sondern nur Tabellen für die eigene Arbeit z.B. in Word erstellt werden sollen, vereinfacht sich die R Markdown Datei aus Kapitel 10.2. Im YAML-Kopf spezifizieren wir an dieser Stelle nur den Titel und den Ausgabetyp als Word und PDF. Alle Tabellen sollen dabei mit der kable() Funktion aus dem knitr Package erstellt werden (df_print: kable). Damit der R Code nicht angezeigt wird, muss in weiterer Folge noch die Option echo: FALSE in den entsprechenden Code Abschnitten hinzugefügt werden. – Beginn der Datei Tabelle.Rmd – --- title: &#39;Beispiel&#39; output: word_document: df_print: kable pdf_document: df_print: kable --- ```{r} #| label: setup #| include: FALSE library(tidyverse) library(knitr) library(remp) data(big5_mod2) ``` ```{r} #| echo: FALSE big5_mod2 |&gt; select(Extraversion, Neurotizismus) |&gt; descriptive() ``` – Ende der Datei Tabelle.Rmd – Abbildung 10.5: Beispielhafter Output einer Tabelle von R Markdown in Word. Das Aussehen der Tabelle kann innerhalb von Word anschließend entsprechend angepasst werden. Allerdings solltest du die Breite einer gewöhnlichen Din A4 Seite beachten. Wenn du eine sehr große Korrelationsmatrix ausgeben möchtest, solltest du entweder eine Word Vorlage in Querformat verwenden (die Vorlage muss denselben Dateinamen wie das R Markdown Dokument haben) oder die Tabelle in Teilen abbilden lassen. Zur Umwandlung in ein Word Dokument klickt man wieder auf Knit und anschließend auf Knit to Word oder betätigt die Tastenkombination Strg / Cmd + Shift + K. Für die Umwandlung in ein PDF wählt man entsprechend Knit to PDF aus. Mit einer Code- und Textbasis können wir mit R Markdown Tabellen in Word, PDF oder HTML erstellen. Beachte dabei, dass du zum Erstellen von PDF Dokumenten LaTeX auf deinem Computer installiert haben musst (siehe Kapitel 10.1). 10.3.2 Exportieren nach LaTeX Für dieses Kapitel muss das xtable Package installiert und geladen sein. library(xtable) Es gibt auch die Möglichkeit, tibbles aus R direkt in LaTeX Code umzuwandeln, um diesen dann in die zugehörige LaTeX Datei einzufügen. In den bisherigen Kapiteln wurde LaTeX nur hinter den Kulissen von Pandoc zum Erstellen von PDFs verwendet. Falls du eine wissenschaftliche Publikation direkt in LaTeX schreibst, benötigst du allerdings den tatsächlichen LaTeX Code. Exemplarisch verwenden wir dieselben deskriptiven Lage- und Streuungsmaße wie im letzten Kapitel. erg &lt;- big5_mod2 |&gt; select(Extraversion, Neurotizismus) |&gt; descriptive() |&gt; select(Variable, Min, Mean, Median, Max, SE) # A tibble: 2 × 10 Variable N Min Q1 Mean Median Q3 Max SD SE &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion 200 2.3 2.8 3.08 3 3.3 4.3 0.35 0.02 2 Neurotizismus 200 1.4 2.7 3.13 3.1 3.6 4.6 0.68 0.05 erg # A tibble: 2 × 6 Variable Min Mean Median Max SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Extraversion 2.3 3.08 3 4.3 0.02 2 Neurotizismus 1.4 3.13 3.1 4.6 0.05 Zum Erstellen des LaTeX Codes greift man auf die Funktion xtable() aus dem gleichnamigen Package zurück. Das erste Argument ist hierbei der auszugebende Datensatz. Das zweite Argument der Funktion ist digits, mit dem man pro Spalte die Anzahl der gerundeten Nachkommastellen festlegt. Dabei können wir entweder eine einzelne Zahl eingeben und somit für alle Spalten dieselbe Rundung anwenden oder für jede Spalte die Rundung einzeln als Vektor mithilfe von c() definieren. Da in der Funktion auch die Zeilennamen berücksichtigt werden, muss der Vektor immer um eins länger sein als die Anzahl der Spalten des eigentlichen Datensatzes. Die Funktion print() mit dem Argument include.rownames verhindert anschließend, dass jede Zeile des Datensatzes nummeriert ausgegeben wird. tabelle &lt;- xtable(erg, digits = c(0, 1, 2, 2, 2, 2, 3)) print(tabelle, include.rownames = FALSE) % latex table generated in R 4.3.0 by xtable 1.8-4 package % Sat May 13 10:00:56 2023 \\begin{table}[ht] \\centering \\begin{tabular}{lrrrrr} \\hline Variable &amp; Min &amp; Mean &amp; Median &amp; Max &amp; SE \\\\ \\hline Extraversion &amp; 2.30 &amp; 3.08 &amp; 3.00 &amp; 4.30 &amp; 0.020 \\\\ Neurotizismus &amp; 1.40 &amp; 3.13 &amp; 3.10 &amp; 4.60 &amp; 0.050 \\\\ \\hline \\end{tabular} \\end{table} 10.4 Alternative mit Quarto Zur Erstellung eines Quarto Dokuments innerhalb von RStudio wähle links das Dropdown-Menü mit dem Papier und dem grünen Pluszeichen unter dem Reiter File aus und klicke auf Quarto Document.... Bevor die Datei erstellt wird, können noch Titel, Autor und das Ausgabeformat ausgewählt werden. Nach Betätigen der Create Taste öffnet sich ein Beispieldokument mit dem typischen Aufbau einer Quarto Datei. Auch hier startet die Datei mit einem YAML-Kopf zum Festlegen der Optionen des Dokuments. An dieser Stelle ergibt sich ein wichtiger Unterschied zu R Markdown Dateien. Während in R Markdown die Ausgabeformate mit output definiert wurden, heißt das Argument hier format. Auch die Bezeichnung der Optionen innerhalb der Formate ist anders. Wichtig ist bei Ausgabe als Internetseite (HTML-Dokument) die zusätzliche Verwendung des Arguments embed-resources, da ansonsten ein extra Ordner mit den zugehörigen Javascript und CSS Dateien erstellt wird, die für das Verhalten und Aussehen des HTML-Dokuments verantwortlich sind. Wir verwenden an dieser Stelle dasselbe Beispiel wie in Kapitel 10.2. Die einzigen Unterschiede sind der abgeänderte YAML-Kopf und die Dateiendung mit .qmd anstelle von .Rmd. – Beginn der Datei Quarto.qmd – --- title: &quot;Vorläufige Ergebnisse&quot; author: &quot;Student A&quot; date: &quot;2023-01-13&quot; format: html: embed-resources: true df-print: paged docx: df-print: kable pdf: df-print: kable --- ```{r} #| label: setup #| include: FALSE library(tidyverse) library(remp) library(knitr) options(dplyr.summarise.inform = FALSE) data(big5_mod2) ``` ## Deskriptive Statistik Verschiedene Lagemaße wie Minimum, 1. Quartil, Mittelwert, Median, 3. Quartil, Maximum, Standardabweichung und Standardfehler. ```{r} big5_mod2 |&gt; descriptive() ``` ## Visualisierung Ein Streudiagramm zur anschaulichen Darstellung des Zusammenhangs zwischen **Extraversion** und **Neurotizismus**. ```{r} #| echo: FALSE #| fig.cap: &quot;Streudiagramm zur Extraversion und Neurotizismus.&quot; ggplot(big5_mod2, aes(x = Extraversion, y = Neurotizismus)) + geom_point(position = &quot;jitter&quot;) ``` – Ende der Datei Quarto.qmd – Umgewandelt wird bei Quarto Dokumenten nicht mit dem Button Knit, sondern durch Render. Du kannst alternativ auch hier die Tastenkombination Strg / Cmd + Shift + K verwenden. Das Ergebnis nach Umwandlung in ein HTML-Dokument ist in Abbildung 10.6 zu sehen. Die Ausgabe unterscheidet sich nur im Aussehen von der durch R Markdown erstellten Datei. Nach Umwandlung in ein Word- oder PDF-Dokument finden sich noch weniger optische Unterschiede. Erst bei komplexeren Anwendungsfällen wirst du mögliche Vor- oder Nachteile von Quarto gegenüber R Markdown bemerken. Abbildung 10.6: Umwandlung der Quarto Datei in HTML, PDF oder Word. "],["datatypes.html", "Kapitel 11 Datenstrukturen 11.1 Vektor 11.2 Matrix 11.3 Data.frame und tibble 11.4 Liste 11.5 Umwandlungen 11.6 Objekte und Objektorientierung", " Kapitel 11 Datenstrukturen Wer tiefer in R eintauchen möchte, sollte neben den bereits verwendeten tibbles ebenfalls die anderen Datenstrukturen kennenlernen. In R gibt es außerdem Vektoren, Matrizen, data.frames und Listen. Ein Vektor enthält einen oder mehrere Werte in einer Reihe desselben Datentyps. Eine Matrix erweitert den Vektor um eine zweite Dimension, kann aber ebenfalls nur denselben Datentyp beinhalten. Data.frames sind wie Matrizen zweidimensional, allerdings kann jede Spalte einen beliebigen Datentyp enthalten, der sich von den anderen Spalten unterscheidet. Eine Liste kann in jedem Element entweder Vektoren, Matrizen oder data.frames enthalten, welche auch miteinander kombiniert werden können. 11.1 Vektor Jede Spalte innerhalb eines tibbles ist für sich genommen ein Vektor, der aus einem einzigen Datentyp besteht. Erstellen kann man einen Vektor auf unterschiedliche Art und Weise. Bereits kennengelernt haben wir die c() Funktion (combine, engl. für kombinieren). c(11, 8, 24, 53) [1] 11 8 24 53 Man kann so neben numerischen Werten auch jeden anderen Datentyp zu einem Vektor kombinieren (z.B. Character). Für eine einfach Sequenz können wir den Doppelpunkt verwenden. Man könnte so bspw. das Erstellen des Vektors c(1, 2, 3, 4) etwas abkürzen. 1:4 [1] 1 2 3 4 Der Doppelpunkt ist dabei ein Shortcut für seq() (sequence, engl. für Sequenz). seq(from = 1, to = 4, by = 1) [1] 1 2 3 4 Zusätzlich können hier auch die Abstände zwischen den Zahlen der Frequenz kleiner oder größer gewählt werden (z.B. by = 0.2 oder by = 2). Auf einzelne Werte innerhalb eines Vektors kann mithilfe eckiger Klammern zugegriffen werden. Exemplarisch wird hier das dritte Element des Vektors c(1, 3, 2, 4), welcher als vec gespeichert ist, ausgewählt. vec &lt;- c(1, 3, 2, 4) vec[3] [1] 2 Die eckigen Klammern können auch mit c() oder dem Doppelpunkt kombiniert werden, um mehrere Elemente ausgeben zu lassen. vec[c(1, 4)] [1] 1 4 vec[1:2] [1] 1 3 Vektoren können immer nur einen Datentyp enthalten. Wenn eine Zahl mit einem Wort kombiniert wird, werden alle im Vektor enthaltenen Werte zum Typ Character umgewandelt. 11.2 Matrix Wenn man mehrere Vektoren eines Datentyps aneinander bindet, erhält man eine Matrix. Für zeilenweises Binden der Vektoren wird rbind() verwendet (row bind, engl. für Zeilen verbinden). Dabei müssen alle Vektoren dieselbe Länge haben. rbind( c(1, 3, 2, 4), 1:4 ) [,1] [,2] [,3] [,4] [1,] 1 3 2 4 [2,] 1 2 3 4 Das Äquivalent zum Verbinden von Spalten ist cbind() (column bind, engl. für Spalten verbinden). cbind( c(1, 3, 2, 4), 1:4 ) [,1] [,2] [1,] 1 1 [2,] 3 2 [3,] 2 3 [4,] 4 4 Seltener in der Datenanalyse benutzt, aber trotzdem manchmal nützlich ist die matrix() Funktion. Als Argumente müssen der Vektor, die Anzahl der Zeilen oder Spalten sowie die Information übergeben werden, ob die Werte zeilenweise (byrow) eingefügt werden sollen. mat &lt;- matrix( 1:9, ncol = 3, byrow = TRUE ) mat [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 Da nun zwei Dimensionen involviert sind, müssen zum Zugreifen auf Elemente innerhalb der Matrix auch zwei Parameter berücksichtigt werden: die Spalten- und Zeilenposition. Dabei werden innerhalb der eckigen Klammern getrennt von einem Komma zuerst die Zeilen und dann die Spalten angegeben. Möchte man den Wert aus Zeile 2 und Spalte 3 erhalten, würde man [2, 3] an den Variablennamen hängen. mat[2, 3] [1] 6 Wenn eine ganze Zeile oder Spalte zurückgeben werden soll, lässt man schlichtweg das auszulassende Argument weg. Für die erste Zeile schreibt man folglich: mat[1, ] [1] 1 2 3 Das Leerzeichen nach dem Komma ist zwar nicht zwingend notwendig, allerdings macht es deutlich, dass dort ein zweiter Wert fehlt. Genau wie Vektoren können auch in Matrizen nur Werte von einem Datentyp gespeichert werden. Bei Vermischung der Datentypen wird automatisch die Umwandlungsregel Character &gt; Integer &gt; Logical angewandt. Außerdem müssen die Vektoren innerhalb der Matrix dieselbe Länge haben. Aufgrund der Limitation, nur einen Datentyp enthalten zu können, findet die Matrix als Datenstruktur in der gewöhnlichen Datenanalyse in der Regel keine Anwendung. 11.3 Data.frame und tibble Ein data.frame ist ein zweidimensionales Datenformat, welches direkt in R integriert ist. Hingegen müssen tibbles aus dem gleichnamigen tibble Package des tidyverse zur Verwendung nach jedem Start von R neu geladen werden. Beide haben eine Anordnung wie die Matrix mit Zeilen und Spalten. Zwischen den Spalten darf der Datentyp variieren, wobei innerhalb einer Spalte noch immer derselbe Datentyp vorhanden sein muss. Einer der größten Vorteile der tibbles ist die übersichtlichere Ausgabe. Es werden nur 10 Zeilen ausgegeben. Auf einen Blick sieht man dabei die Datentypen der Spalten und die Dimensionen des Datensatzes. Außerdem sind die Zahlen zur besseren Übersichtlichkeit entsprechend eingerückt und negative Werte rot hervorgehoben. Das automatische Runden von tibbles bei der Anzeige ist hingegen nicht immer ein Vorteil. Während es beim explorativen Anschauen der Daten praktisch ist, muss beim deskriptiven oder inferenzstatistischen Betrachten der Daten eine bestimmte Anzahl von Kommastellen sichtbar sein, um sie in einer wissenschaftlichen Arbeit zu berichten. Grundsätzlich sind Funktionen, die für data.frames verwendet werden können, bis auf wenige Ausnahmen auch auf tibbles anwendbar. Wie man einen tibble erstellt, wurde bereits in Kapitel 5.3 eingeführt. Bei der Erstellung eines data.frames ändert sich nur die Funktion. data.frame( a = 1:2, b = 3:4 ) a b 1 1 3 2 2 4 Beim Zugriff auf einzelne Spalten wurde im Verlauf des Buches entweder select() aus dem tidyverse oder der Dollar-Operator verwendet. Die Funktion select() gibt dabei die einzelne Spalte als tibble zurück. tb &lt;- tibble( Geschlecht = c(&quot;m&quot;, &quot;f&quot;, &quot;d&quot;), Alter = c(44, 16, 52), ) tb |&gt; select(Alter) # A tibble: 3 × 1 Alter &lt;dbl&gt; 1 44 2 16 3 52 Eine alternative Schreibweise dafür sind einfache eckige Klammern. Da hier alle Zeilen der Spalte Alter zurückgegeben werden sollen, wird keine Zahl vor dem Komma angegeben. tb[ ,&quot;Alter&quot;] # A tibble: 3 × 1 Alter &lt;dbl&gt; 1 44 2 16 3 52 Hier gibt es einen weiteren Unterschied zwischen data.frames und tibbles. Während in data.frames bei Auswahl nur einer Spalte ein Vektor zurückgegeben wird, gibt ein tibble immer einen tibble zurück. Einzelne Spalten als Vektor können entweder mit dem Dollar-Zeichen oder mit doppelten eckigen Klammern mit dem Spaltennamen in Anführungszeichen aus einem data.frame oder tibble extrahiert werden. tb$Alter tb[[&quot;Alter&quot;]] [1] 44 16 52 Einzelne Werte können mit einfachen eckigen Klammern äquivalent zu Matrizen ausgewählt werden. tb[2, 1] # A tibble: 1 × 1 Geschlecht &lt;chr&gt; 1 f 11.4 Liste Listen sind die allgemeinste Datenstruktur. Ein Listenelement kann jede Datenstruktur enthalten – sogar ganze tibbles. Beim Erstellen ändert sich der Befehl zu list(). ls &lt;- list( Vektor = vec, Matrix = mat, Tibble = tb ) ls $Vektor [1] 1 3 2 4 $Matrix [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 $Tibble # A tibble: 3 × 2 Geschlecht Alter &lt;chr&gt; &lt;dbl&gt; 1 m 44 2 f 16 3 d 52 Die Zeichen vor dem Gleichheitszeichen sind dabei die Namen der Listenelemente, die man zum Abrufen verwenden kann. Mit Listen haben wir bis zu drei Dimensionen. Die verschiedenen Elemente innerhalb der Liste, die wiederum zweidimensionale tibbles enthalten können. Es ist sogar möglich, Listen innerhalb von Listen zu speichern. Im Rahmen des Einnistens greifen wir diesen Gedanken in Kapitel 12.4 wieder auf. Das Prinzip beim Zugreifen ändert sich im Vergleich zu tibbles nicht. Allerdings gibt es eine Dimensionen mehr. Würde man also auf den tibble der eben erstellen Liste ls zugreifen wollen, könnte man ls$Tibble oder ls[[3]] verwenden. Möchte man direkt auf Elemente innerhalb des tibbles zugreifen, erreicht man dies wie gewohnt mit einfach eckigen Klammern. ls[[3]][1, 2] # A tibble: 1 × 1 Alter &lt;dbl&gt; 1 44 Eine leere Liste einer bestimmten Länge kann mit vector() erstellt werden (hier z.B. eine Liste der Länge 3). Dies ist vor allem bei for-Schleifen wichtig, da so die Dauer der Berechnungen reduziert werden kann (siehe Kapitel 12.3). vector(&quot;list&quot;, 3) [[1]] NULL [[2]] NULL [[3]] NULL Eine besondere Art der Liste ist der tibble. Daher können wir grundsätzlich in eine Zelle nicht nur Zahlen oder Buchstaben hineinschreiben, sondern sogar ganze andere Datensätze darin abspeichern. df &lt;- tibble( a = c(1, 2, 3), b = list( tibble(a = c(1, 2, 3, 4), b = c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;)), tibble(x = 4:5, y = 6:7), Number = 1 ) ) df # A tibble: 3 × 2 a b &lt;dbl&gt; &lt;named list&gt; 1 1 &lt;tibble [4 × 2]&gt; 2 2 &lt;tibble [2 × 2]&gt; 3 3 &lt;dbl [1]&gt; Wie du siehst, sind in der Spalte b nun in den ersten zwei Zeilen tibbles enthalten. Wenn wir mit df$b oder df[[2]] nur diese Spalte anschauen, sehen wir eine Liste als Ausgabe. df[[2]] [[1]] # A tibble: 4 × 2 a b &lt;dbl&gt; &lt;chr&gt; 1 1 m 2 2 f 3 3 f 4 4 m [[2]] # A tibble: 2 × 2 x y &lt;int&gt; &lt;int&gt; 1 4 6 2 5 7 $Number [1] 1 11.5 Umwandlungen Sofern die Voraussetzungen der Dimensionen und Datentypen erfüllt sind, können Datenstrukturen ineinander überführt werden. Dabei haben die Funktionen immer das Präfix as. mit der Ausnahme bei tibbles mit dem Präfix as_. as.vector() as.matrix() as.data.frame() as_tibble() Besonders nützlich ist hierbei as.data.frame(), um einen tibble in einen data.frame umzuwandeln, falls die angezeigten Rundungen zu ungenau sind oder Funktionen nur mit data.frames fehlerfrei funktionieren. Für as_tibble() muss zuvor das tibble Package oder das tidyverse geladen sein. 11.6 Objekte und Objektorientierung Für dieses Kapitel muss das sloop Package installiert und geladen werden. library(sloop) In R ist grundsätzlich alles ein Objekt. Dazu gehören auch die bereits kennengelernten Vektoren, Matrizen, data.frames, tibbles und Listen. Das ist insofern eine irreführende Aussage, als dass R eine primär funktionelle Programmiersprache ist und keine klassische Objektorientierung bietet, wie es z.B. in Java oder C/C++ umgesetzt ist. Schließlich lösen wir anspruchsvolle Probleme in R durch das Erstellen neuer Funktionen und nicht neuer Objekttypen mit spezifischen Eigenschaften. Die vier wichtigsten Systeme zur Objektorientierung in R sind S3, R6, R7 und S4. Wir haben im Verlauf dieses Buches nur das S3 System kennengelernt. Du hast dich vielleicht bereits gefragt, weshalb ein und dieselbe Funktion mit dem Namen summary() in Abhängigkeit des Kontextes unterschiedliche Ausgaben liefern kann. In Kapitel 7.1 erhalten wir Lage- und Streuungsmaße für Numerics und Häufigkeiten für Faktoren, während wir in Kapitel 9 je nach statistischem Modell ebenfalls ein unterschiedliche Ausgabe ausgegeben bekommen. Die Antwort liegt in den Klassen der S3 Objekte, wodurch sich generische Funktionen wie summary() je nach Klasse anders verhalten. Mit der Funktion otype() aus dem sloop Package kann der Objekttyp herausgefunden werden. Einfache Vektoren gehören keinem der Systeme zur Objektorientierung an. otype(c(1, 3, 2, 4)) [1] &quot;base&quot; Hingegen ist jeder verwendete Datensatz dieses Buches ein S3 Objekt. otype(big5) [1] &quot;S3&quot; Dasselbe gilt auch für die Ergebnisse der Statistikfunktionen. Beim Vergleich deren jeweiliger Klasse mithilfe der Funktion class() kriegen wir einen anderes Ergebnis für einen t Test als für ein lineares Regressionsmodell. class(t.test(Leukos_t6 ~ Geschlecht, data = chemo)) [1] &quot;htest&quot; class(lm(Leukos_t6 ~ Geschlecht, data = chemo)) [1] &quot;lm&quot; Die Funktion s3_methods_generic() des sloop Packages gibt Auskunft über alle von einer Funktion unterstützten Klassen. An dieser Stelle möchten wir summary() untersuchen. s3_methods_generic(&quot;summary&quot;) # A tibble: 137 × 4 generic class visible source &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; 1 summary aareg FALSE registered S3method 2 summary afex_aov FALSE registered S3method 3 summary allFit FALSE registered S3method 4 summary Anova.mlm FALSE registered S3method # ℹ 133 more rows 137 verschiedene Klassen werden nach Laden aller in diesem Buch verwendeten Packages unterstützt. Bereits in Zeile zwei sehen wir die Klasse der Funktion zur Varianzanalyse mit Messwiederholung aus Kapitel 9.3.2. Welche Klassen eine Implementierung für summary() erhalten, hängt maßgeblich von den PackageentwicklerInnen ab, die das entsprechende Verhalten von summary() für die neue Klasse programmatisch festlegen müssen. Ein weiteres Beispiel für eine generische Funktion, die sich je nach Klasse anders verhält, ist autoplot() aus dem ggplot2 Package. Wir haben in Kapitel 8.13.2 durch das Package ggfortify das Verhalten für die Klasse lm eines Regressionsmodells kennengelernt. Tatsächlich können mit der Funktion autoplot() aber diverse Visualisierungen von statistischen Modellen erstellt werden. R6 und S4 Systeme sind deutlich ähnlicher der klassischen Objektorientierung aus Java oder C/C++. Die S4 Objektorientierung ist eine Voraussetzung zum Hochladen eines Packages auf Bioconductor, welches eine Alternative mit biologischen Fokus zu CRAN darstellt. Als Konsequenz können die meisten der in diesem Buch kennengelernten Funktionen nicht mit Packages von Bioconductor kombiniert werden. Jedes S4 Objekt erhält eigene Methoden, um auf Informationen der Objekte zuzugreifen. Ein großer Unterschied in der Syntax ist auch die Verwendung eines at-Zeichens (@) anstelle des uns bekannten Dollar-Operators, um einzelne Spalten zu extrahieren. Hätten wir in einem Objekt namens ls eine Information namens ergebnis gespeichert, würden wir im S4 System über ls@ergebnis und nicht ls$ergebnis Zugriff auf das Ergebnis bekommen. Langfristig wird das neue R7 System, welches planmäßig direkt in R integriert werden soll, voraussichtlich das S4 und R6 System ersetzen. Für ausführlichere Erklärungen zum funktionellen Programmieren und der Objektorientierung sei auf Advanced R von Wickham verwiesen. "],["iterationmain.html", "Kapitel 12 Funktionen wiederholt anwenden 12.1 Das Copy &amp; Paste Problem 12.2 Listenbasierte Berechnungen 12.3 for-Schleifen 12.4 Einnisten", " Kapitel 12 Funktionen wiederholt anwenden Im Zuge der Datenanalyse muss man häufig dieselben Berechnungen für mehrere Szenarien vornehmen. Um sich Zeit zu sparen und die Häufigkeit von Fehlern zu minimieren, existieren in jeder Programmiersprache sogenannte Schleifen. In diesem Kapitel lernst du drei verschiedene Arten kennen, diese sogenannten iterativen Prozesse selbst in R umzusetzen. 12.1 Das Copy &amp; Paste Problem In diesem Kapitel ist das Installieren und Laden des purrr Packages aus dem tidyverse eine Voraussetzung. library(tidyverse) In vielen Szenarien ist es nötig, die Funktion nicht nur einmal, sondern mehrmals anzuwenden. Die naheliegende Lösung dafür ist den Funktionsaufruf zu kopieren und leicht modifiziert für den nächsten Anwendungsfall zu verwenden. Ein Beispiel dafür haben wir bereits in Kapitel 9.6 beim Ausgeben der Häufigkeiten kategorialer Variablen gesehen. Wenn ein oder zwei Häufigkeiten ausgegeben werden sollen, ist das Kopieren der Funktion noch kein Problem. table(big5_mod$Geschlecht) f m 118 82 table(big5_mod$Gruppe) Jung Mittel Weise 147 39 14 Falls jedoch die Häufigkeiten von 20 bis 30 Variablen gewünscht sind, ist das Kopieren der jeweiligen Funktion nicht nur zeitaufwendig, sondern auch fehleranfällig. Im Verlauf der nächsten Kapitel werden wir drei mögliche Automatisierungen kennenlernen: map(), for-Schleifen und Einnisten. Mit der Funktion map() können wir die Funktion table() zur Berechnung der Häufigkeiten auf jede ausgewählte Spalte anwenden. Es werden also zuerst die Häufigkeiten der Geschlechter und anschließend die der Altersgruppen berechnet. big5_mod |&gt; select(Geschlecht, Gruppe) |&gt; map(table) $Geschlecht f m 118 82 $Gruppe Jung Mittel Weise 147 39 14 Durch einen weiteren Aufruf von map() dieses Mal mit der Funktion prop.table() können zusätzlich die Verhältnisse der Merkmale ausgegeben werden. big5_mod |&gt; select(Geschlecht, Gruppe) |&gt; map(table) |&gt; map(prop.table) $Geschlecht f m 0.59 0.41 $Gruppe Jung Mittel Weise 0.735 0.195 0.070 Da Häufigkeiten in der Regel nur bei kategorialen Variablen erwünscht sind, könnten wir auch nur Spalten vom Datentyp Character oder Factor ausrechnen lassen (siehe Kapitel 6.2). big5_mod |&gt; select(where(is.character) | where(is.factor)) |&gt; map(table) Copy &amp; Paste ist fehleranfällig und sollte nur bei weniger als zehn wiederholten Anwendungen von Funktionen verwendet werden. Alternativ können diese Funktionswiederholungen auf verschiedene Arten automatisiert werden. 12.2 Listenbasierte Berechnungen Für dieses Kapitel muss das purrr Package aus dem tidyverse installiert und geladen werden. library(tidyverse) Ein Beispiel zum wiederholten Anwenden einer Funktion mithilfe von map() wurde bereits im vorherigen Kapitel eingeführt. Während dort die Funktion auf einen tibble angewandt wurde (eine Sonderform der Liste), werden wir uns hier den klassischen Anwendungsfall von map() im Kontext von Listen anschauen. Als Beispiel wollen wir an dieser Stelle ein lineares Regressionsmodell zur Erklärung der Variation von Extraversion durch Geschlecht für jede Altersgruppe einzeln berechnen (siehe Kapitel 9.5.1). Dafür trennen wir den Datensatz zunächst mithilfe der Funktion split() in eine Liste mit drei Elementen. Das erste Element enthält einen Datensatz mit den jüngsten Personen, das zweite die mittlere Altersklasse und das dritte Element die Ältesten. mod_ls &lt;- split(big5_mod, ~ Gruppe) Zum Erstellen der Regressionsmodelle kopieren wir den Befehl dreimalig. Dabei wird jedes Mal mithilfe der doppelten eckigen Klammern auf ein anderes Listenelement zugegriffen (siehe Kapitel 11.4). model1 &lt;- lm(Extraversion ~ Geschlecht, data = mod_ls[[1]]) model2 &lt;- lm(Extraversion ~ Geschlecht, data = mod_ls[[2]]) model3 &lt;- lm(Extraversion ~ Geschlecht, data = mod_ls[[3]]) Mit der Funktion map() kann das redundante Kopieren der Funktion vermieden werden. Als einziges Argument wird das Regressionsmodell als anonyme Lambdafunktion übergeben (siehe Kapitel 6.4.4). Als Ergebnis erhalten wir eine Liste mit einem Modell pro Listenelement. mod_ls |&gt; map(\\(teildaten) lm(Extraversion ~ Geschlecht, data = teildaten)) $Jung Call: lm(formula = Extraversion ~ Geschlecht, data = teildaten) Coefficients: (Intercept) Geschlechtm 3.067 0.055 $Mittel Call: lm(formula = Extraversion ~ Geschlecht, data = teildaten) Coefficients: (Intercept) Geschlechtm 3.07000 0.05632 $Weise Call: lm(formula = Extraversion ~ Geschlecht, data = teildaten) Coefficients: (Intercept) Geschlechtm 2.8333 0.1267 Mit einem weiteren Aufruf von map() können wir für jedes der drei Modelle die Zusammenfassungen der Ergebnisse zeigen. mod_ls |&gt; map(\\(teildaten) lm(Extraversion ~ Geschlecht, data = teildaten)) |&gt; map(summary) Alternativ könnte auch die tidy() Funktion zum Ausgeben als tibble (anstelle einer Liste) verwendet werden (siehe Kapitel 9.8). Da die gewünschte Ausgabe hier ein tibble bzw. data.frame (kurz: df) ist, müssen wir stattdessen die Funktion map_df() benutzen. mod_ls |&gt; map(\\(teildaten) lm(Extraversion ~ Geschlecht, data = teildaten)) |&gt; map_df(tidy) # A tibble: 6 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3.07 0.0376 81.6 4.49e-123 2 Geschlechtm 0.0550 0.0598 0.919 3.60e- 1 3 (Intercept) 3.07 0.0649 47.3 1.08e- 34 4 Geschlechtm 0.0563 0.0930 0.605 5.49e- 1 # ℹ 2 more rows Wenn die Ausgabe nicht als Liste erfolgen soll, verändert sich der Funktionsname je nach gewünschter Datenstruktur und gewünschtem Datentyp in andere Varianten von map() (z.B. map_chr() für Character Vektoren oder map_dbl() für numerische Vektoren). Alternativ sind Funktionen mit ähnlicher Funktionsweise direkt in R integriert, die allerdings mitunter umständlicher zu verwenden sind (z.B. lapply(), apply(), tapply() und mapply()). Innerhalb der hier vorgestellten Funktionen werden sogenannte Schleifen verwendet. 12.3 for-Schleifen Eine wiederholte Anwendung von Funktionen für jeden Kontext unabhängig der Datenstruktur und des Datentyps wird durch for-Schleifen ermöglicht. Um das Konzept einzuführen, greifen wir an dieser Stelle auf dieselbe listenbasierte Berechnung mehrerer linearer Regressionmodelle des vorherigen Kapitels zurück. Zuerst wird also erneut der Datensatz als Liste aufgeteilt. mod_ls &lt;- split(big5_mod, ~ Gruppe) Mit Copy &amp; Paste müsste das Modell dreimalig ausgeschrieben werden. model1 &lt;- lm(Extraversion ~ Geschlecht, data = mod_ls[[1]]) model2 &lt;- lm(Extraversion ~ Geschlecht, data = mod_ls[[2]]) model3 &lt;- lm(Extraversion ~ Geschlecht, data = mod_ls[[3]]) Den ersten Schritt beim Berechnen einer for-Schleife stellt das Kreieren einer leeren Ergebnisliste dar. erg_ls &lt;- vector(&quot;list&quot;, 3) erg_ls [[1]] NULL [[2]] NULL [[3]] NULL Falls das Ergebnis für andere Kontexte nicht als Liste, sondern bspw. als numerischer Vektor gespeichert werden soll, könnte man stattdessen vector(\"numeric\", 3) wählen. In der eigentlichen Schleife soll für jedes i von 1 bis 3 das Modell aufgestellt und in erg_ls an entsprechender Stelle gespeichert werden. Dabei greifen wir auf den jeweiligen Datensatz an der Stelle von Index i aufgrund der Liste innerhalb einer doppelten eckigen Klammer zu. for(i in 1:3) { erg_ls[[i]] &lt;- lm(Extraversion ~ Geschlecht, data = mod_ls[[i]]) } erg_ls [[1]] Call: lm(formula = Extraversion ~ Geschlecht, data = mod_ls[[i]]) Coefficients: (Intercept) Geschlechtm 3.067 0.055 [[2]] Call: lm(formula = Extraversion ~ Geschlecht, data = mod_ls[[i]]) Coefficients: (Intercept) Geschlechtm 3.07000 0.05632 [[3]] Call: lm(formula = Extraversion ~ Geschlecht, data = mod_ls[[i]]) Coefficients: (Intercept) Geschlechtm 2.8333 0.1267 Eine sinnvolle Herangehensweise beim Erstellen von for-Schleifen ist es, den zu wiederholenden Funktionsaufruf zwei- bis dreimal zu kopieren, weil so die Automatisierung leichter fällt. In unserem Beispiel konnten wir so sehen, dass sich nur mod_ls[[1]] zu mod_ls[[2]] und mod_ls[[3]] verändert. Effiziente Schleifen zu schreiben ist nicht einfach. Daher sollte nach Möglichkeit auf die map() Funktionen aus dem purrr Package zurückgegriffen werden. Dies macht sich vor allem bei großen Datensätzen bemerkbar (siehe Kapitel 12.2). 12.4 Einnisten Zum Bearbeiten dieses Kapitels muss das tidyr Package aus dem tidyverse geladen werden. library(tidyverse) Durch die Funktion nest() können innerhalb von Zellen eines tibbles Datenstrukturen jeder Art verschachtelt werden. Dies wurde bereits in Kapitel 11.4 anhand eines einfachen Beispiels eingeführt. Dieses Konzept wenden wir beim big5_mod Datensatz an und gruppieren dafür innerhalb von nest() mithilfe des Arguments .by. big5_mod |&gt; nest(.by = Gruppe) # A tibble: 3 × 2 Gruppe data &lt;fct&gt; &lt;list&gt; 1 Mittel &lt;tibble [39 × 5]&gt; 2 Jung &lt;tibble [147 × 5]&gt; 3 Weise &lt;tibble [14 × 5]&gt; Als Ergebnis erhalten wir einen tibble, welcher in der ersten Spalte die Altersgruppen abbildet. Daneben steht eine neue zweite Spalte namens data, in der wiederum drei tibbles mit den Dimensionen 39x5, 147x5 und 14x5 gespeichert sind. Ähnlich wie bei den listenbasierten Berechnungen in Kapitel 12.2 können wir auch auf diese verschachtelten tibbles die map() Funktionen anwenden. Der Unterschied besteht darin, dass wir den Befehl innerhalb von mutate() ausführen müssen (siehe Kapitel 6.4). Schließlich soll mit den Inhalten einer Spalte eines tibbles eine neue Spalte erstellt werden. Im zweiten Schritt nehmen wir die Modelle und geben diese in einem aufgeräumten Format mit der Funktion tidy() aus (siehe Kapitel 9.8). big5_mod |&gt; nest(.by = Gruppe) |&gt; mutate( Modelle = map(data, \\(teildaten) lm(Extraversion ~ Geschlecht, data = teildaten)), Ergebnisse = map(Modelle, tidy) ) # A tibble: 3 × 4 Gruppe data Modelle Ergebnisse &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 Mittel &lt;tibble [39 × 5]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; 2 Jung &lt;tibble [147 × 5]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; 3 Weise &lt;tibble [14 × 5]&gt; &lt;lm&gt; &lt;tibble [2 × 5]&gt; Die neue Spalte namens Modelle hat Einträge des Datentyps &lt;lm&gt; (die linearen Modelle). Daneben sind die Ergebnisse von tidy() verschachtelt (u.a. mit Teststatistiken und p-Werten). Damit wir an diese Ergebnisse herankommen, müssen wir diese abschließend mit der Funktion unnest() aus der eingenisteten Struktur herausholen. big5_mod |&gt; nest(.by = Gruppe) |&gt; mutate( Modelle = map(data, \\(teildaten) lm(Extraversion ~ Geschlecht, data = teildaten)), Ergebnisse = map(Modelle, tidy) ) |&gt; unnest(Ergebnisse) # A tibble: 6 × 8 Gruppe data Modelle term estimate std.error statistic p.value &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Mittel &lt;tibble [39 × 5]&gt; &lt;lm&gt; (Intercept) 3.07 0.0649 47.3 1.08e- 34 2 Mittel &lt;tibble [39 × 5]&gt; &lt;lm&gt; Geschlechtm 0.0563 0.0930 0.605 5.49e- 1 3 Jung &lt;tibble [147 × 5]&gt; &lt;lm&gt; (Intercept) 3.07 0.0376 81.6 4.49e-123 4 Jung &lt;tibble [147 × 5]&gt; &lt;lm&gt; Geschlechtm 0.0550 0.0598 0.919 3.60e- 1 # ℹ 2 more rows "],["appendix.html", "Appendix Literaturverzeichnis Verwendete Softwareversionen", " Appendix Literaturverzeichnis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
